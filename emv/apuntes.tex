\section{Fundamentación probabilística de vectores aleatorios}

En esta sección definimos algunos conceptos bien conocidos de la teoría de la probabilidad,
a modo de breve repaso y para establecer la notación que utilizaremos en todo el libro.

% TODO: insertar referencia a apuntes de Análisis Matemático II y Probabilidad.

\begin{ndef}[Vector y variable aleatorios]
    Sea $(\Omega, \mathscr{A}, P)$ un espacio de probabilidad. Un \emph{vector aleatorio}, $\boldsymbol X = (X_1, \dots, X_p)$ es una función medible
    \begin{align*}
      \boldsymbol X:(\Omega, \mathscr{A}) &\rightarrow (\mathbb{R}^p, \mathscr{B}^p)\,.
    \end{align*}
     Cuando $p=1$, lo notamos $X$ y decimos que es una \emph{variable aleatoria}.
    La condición de medibilidad es: \(\boldsymbol X^{-1}(B) \in \mathscr{A}\) para cada \( B \in \mathscr{B}^p\).
\end{ndef}



\begin{ndef}[Probabilidad inducida]
    La \emph{probabilidad inducida por} o \emph{distribución de} un vector aleatorio $\vectX = (X_1, \dots, X_p)$ se define como la función
    \[
    P_{\vectX}[B] := P[\boldsymbol X^{-1}(B)]\,.\] para todo \(B \in \mathscr{B}^p\). Es decir, $P_{\vectX} = P \circ \restr{\boldsymbol X^{-1}}{\mathscr{B}}\,.$
\end{ndef}

\begin{ndef}
  Cuando, para dos vectores aleatorios $\vectX$ y $\vectY$ se tenga $P_{\vectX} = P_{\vectY}$, diremos que
  son \emph{iguales en distribución} y lo notaremos $\vectX \eqd \vectY$.
\end{ndef}

\begin{notacion}
  Cuando escribamos probabilidades de sucesos asociados a un vector aleatorio $\boldsymbol X$, usaremos la siguiente notación:
  \begin{itemize}
  \item $P[\boldsymbol X \in A] = P_X[A]$,
  \item $P[X_1 \le x_1, \dots, X_p \le x_p] = P_X[(-\infty, x_1]\times \dots \times (-\infty, x_p]]$,
  \end{itemize}
  y algunas formas análogas.
\end{notacion}

\begin{ndef}[Función de distribución]
    Se define la \emph{función de distribución} asociada a $P_{\boldsymbol X}$ como
    \begin{align*}
    F_{\boldsymbol X}:\mathbb{R}^p &\rightarrow [0,1] \\
    (x_1,\dots,x_p) &\mapsto P[X_1 \leq x_1, \dots, X_p \leq x_p]\,.
    \end{align*}
\end{ndef}

\begin{ndef}[Función de densidad]
    Si existe una función $f_{\boldsymbol X} : \R^p \to \R$ que sea integrable en el sentido de Lebesgue y tal que
    \[
    F_{\boldsymbol X}(\boldsymbol x) = \int^{x_1}_{-\infty} \dots \int^{x_p}_{-\infty} f_{\boldsymbol X}(u_1, \dots,  u_p) \mathop{}\!\mathrm{d}u_1 \dots \mathop{}\!\mathrm{d}u_p\quad \text{para todo } \boldsymbol  x \in \mathbb{R}^p
    ,\]
    diremos que $f_{\boldsymbol X}$ es una \emph{función de densidad} asociada a  $F_{\boldsymbol X}$. En los puntos de continuidad de $f_{\boldsymbol X}$, la podemos escribir como:
    \[
    f_{\boldsymbol X}(\boldsymbol x) = \frac{\partial^p}{\partial x_1 \dots \partial x_p} F_{\boldsymbol X}(\boldsymbol x)
    .\]
\end{ndef}

\subsection{Independencia}

Veamos ahora algunas nociones relativas a la independencia de variables aleatorias.

\begin{ndef}[Conjunto rectangular]
  Se dice que $B\in \mathscr{B}^p$ es un \emph{conjunto rectangular} si es un producto de $p$ conjuntos de $\mathscr{B}$. Es decir, si
\[
    B = B_1 \times \dots \times B_p,\quad\text{con }B_j \in \mathscr{B} \quad\text{para } j = 1, \dots, p
.\]
\end{ndef}

Observamos que en general, dado un conjunto rectangular $B \in \mathscr{B}^p$ y un vector aleatorio $\boldsymbol X$ se tiene que
\[
P_{\boldsymbol X}[B] \neq P_{X_1}[B_1] \cdots  P_{X_p}[B_p],
\]
lo que motiva la siguiente definición.

\begin{ndef}[Independencia]
    Sea $\boldsymbol X = (X_1, \dots, X_p)$ un vector aleatorio. Si para todo conjunto rectangular $B \in \mathscr{B}^p$ se verifica que \[
      P_{\boldsymbol X}[B] = P_{X_1}[B_1] \cdots  P_{X_p}[B_p],
      \] se dice que las variables aleatorias \(X_1, \dots, X_p\) son \emph{independientes}.
\end{ndef}

\begin{nprop}[Caracterizaciones de independencia]
  Dado un vector aleatorio $\boldsymbol X = (X_1, \dots, X_p)$ sus componentes son independientes si, y solo si se da alguna de las siguientes condiciones:
  \begin{nlist}
  \item La función de distribución verifica $F_{\boldsymbol X} (\boldsymbol x) = F_{X_1} (x_1) \cdots F_{X_p}(x_p)\,$ para todo \(\boldsymbol x \in \mathbb{R}^p\).
   \item $\boldsymbol X$ tiene una función de densidad y \(f_{\boldsymbol X} (\boldsymbol x) = f_{X_1}(x_1) \cdots f_{X_p}(x_p)\,\) para todo \(\boldsymbol x \in \mathbb{R}^p\), salvo, a lo sumo, en un conjunto de medida de Lebesgue nula.
  \end{nlist}
\end{nprop}

\subsection{Función característica}

\begin{ndef}[Función característica] \label{funcioncaracteristica}
    La \emph{función característica} de un vector aleatorio \(\boldsymbol X = (X_1,\dots,X_p)\) se define como la función \[\psi_{\boldsymbol X}(\boldsymbol t)=E\left[e^{i\boldsymbol t^T\boldsymbol X}\right], \quad\text{para } \boldsymbol t\in \mathbb{R}^p.\]
\end{ndef}

\begin{nth}[Unicidad]
  La función característica de un vector aleatorio determina de forma única su distribución.
\end{nth}

El siguiente resultado nos proporciona otra caracterización de la independencia de las componentes de un vector aleatorio, análoga a las descritas anteriormente pero ahora utilizando la función característica recién definida.

\begin{nprop}
  Las componentes de \(\boldsymbol X=(X_1,\dots,X_p)^T\) son independientes si, y solo si

  \[
    \psi_{\boldsymbol X}(\boldsymbol t) = \prod_{k=1}^p\psi_{X_k}(t_k)\,
  .\]
\end{nprop}

\begin{nprop}
  Si las componentes de \(\boldsymbol X=(X_1,\dots, X_p)\) son independientes, entonces la función característica de la variable $Y=\sum_{k=1}^p X_k$ es

  \[
    \psi_{Y}(t) = \prod_{k=1}^p\psi_{X_k}(t)
  .\]
\end{nprop}

\subsubsection{Transformaciones lineales}

A continuación estudiaremos qué forma tiene la función característica de un vector aleatorio cuando este es el resultado de someter otro vector aleatorio a transformaciones lineales.

Sea $\boldsymbol X = (X_1, \dots X_p)^T$ un vector aleatorio $p-$dimensional y sea $\boldsymbol Y = (Y_1,\dots,Y_q)^T$ otro vector aleatorio $q-$dimensional definido como $\boldsymbol Y = B\boldsymbol X + \boldsymbol b$, con $B \in \mathscr{M}_{q\times p}(\R)$ una matriz constante y $\boldsymbol b\in \mathbb R ^n$ un vector también constante.

\begin{nprop} \label{fc:translineal}
  La función característica de $\boldsymbol Y$ se obtiene como:
  
  \[
  \psi_{\boldsymbol Y}(\boldsymbol t) = e^{i\boldsymbol t^T \boldsymbol b} \psi_{\boldsymbol X}(B^T \boldsymbol t)\,, \quad \text{para }\boldsymbol t = (t_1,\dots,t_q)^T \in \mathbb R^q\,.
  \]
\end{nprop}

  En particular, si \[ \vectX= \left( X_{(1)} | X_{(2)}\right)^T := \left(X_{(1),1}, \dots, X_{(1),k}, X_{(2),1}, \dots, X_{(2),p-k}\right),\] con $X_{(1)}$ de dimensión $(k\times1)$ y $X_{(2)}$ de dimensión $(p-k) \times 1$, se tendría que
  \[
X_{(1)} = \left( I_k | 0_{p-k} \right)X + 0_{k\times 1}\,,
\]
por lo que, para $\boldsymbol t_{(1)} = (t_1,\dots,t_k)^T \in \mathbb R^k$ la función característica viene determinada por
\[
\psi_{X_{(1)}}\left(\boldsymbol t_{(1)}\right) = \underbrace{e^{i\boldsymbol t_{(1)}^T 0}}_{=1} \psi_X\left(\left( I_k | 0_{p-k} \right)\boldsymbol t_{(1)}\right) = \psi_X\left(\frac{\boldsymbol t_{(1)}}{0_{(p-k)}}\right)\,.
\]

\subsubsection{Relación con la función de densidad}

La función de densidad (en el caso continuo) de un vector aleatorio $\boldsymbol X = (X_1,\dots,X_p)^T$ y la correspondiente función característica constituyen un par de \emph{transformadas de Fourier}, de forma que
\[
\psi_{\boldsymbol X}(\boldsymbol t) = \int_{\mathbb R ^p} e^{i \boldsymbol t^T \boldsymbol x} f_X(\boldsymbol x) \mathop{}\!\mathrm{d}\boldsymbol x\,,
\]
y por otro lado
\[
f_{\boldsymbol X}(\boldsymbol x) =  \dfrac{1}{(2\pi)^p} \int_{\mathbb R^p} e^{- i \boldsymbol t^T x} \psi_{\boldsymbol X}(t) \mathop{}\!\mathrm{d}\boldsymbol t\,.
\]


\subsection{Aspectos generales sobre vectores aleatorios}

En este apartado definiremos y estudiaremos algunas de las propiedades de los vectores aleatorios con las que trabajaremos habitualmente.

\subsubsection{Esperanza y covarianza}

Para definir la esperanza de un vector aleatorio partimos de la definición de esperanza de una variable aleatoria y la extendemos de forma natural al caso multivariante.

\begin{ndef}
  Sea $\boldsymbol X=(X_1,\dots,X_n)^T$ un vector aleatorio. Se define el \emph{vector de medias} de $\boldsymbol X$ como:
  \[
  \boldsymbol \mu_{\boldsymbol X} = E[\boldsymbol X] = \begin{pmatrix}  E[X_1] \\ \dots \\ E[X_p] \end{pmatrix} = \begin{bmatrix} \mu_1 \\ \dots \\ \mu_p \end{bmatrix}
  \]
  siempre que existan \emph{todas} las esperanzas unidimensionales.
  \end{ndef}

  Observamos a continuación que la propiedad de linealidad de la esperanza en las variables aleatorias se traslada trivialmente al caso multivariante.

\begin{nprop}[Propiedad de linealidad]
  Sea $\boldsymbol Y = B\boldsymbol X + \boldsymbol b$ con $B \in M_{q\times p}$ constante y $\boldsymbol b \in \mathbb R^q$ constante. Entonces,
  \[
    \boldsymbol \mu_{\boldsymbol Y} = E[\boldsymbol Y] = BE[\boldsymbol X] + \boldsymbol b = B\boldsymbol \mu_{\boldsymbol X} + \boldsymbol b\,.
\]
\end{nprop}
%#TODO: Demostración(ejercicio)

Podemos incluso seguir iterando este proceso y extender esta propiedad al caso de matrices aleatorias. Sea $X\in \mathcal M_{p\times q}$ es una matriz aleatoria, y $B\in \mathcal M_{m\times p}$ , $C \in \mathcal M_{q\times n}$ , $D \in \mathcal M_{m \times n}$ matrices constantes. Entonces, para $W = BXC + D$, una matriz aleatoria de dimensión $m\times n$, la matriz de medias viene dada por
\[
E[W] = \left(E[W_{kl}]\right)_{(kl)} = B \cdot E[X] \cdot C + D\,, \quad \text{con } E[X] = \left(E[X_{ij}]\right)_{(ij)}\,.
\]

Como comprobaremos a continuación, la definición de la matriz de covarianzas no es más que una extensión al caso multivariante de la definición de covarianza de dos variables aleatorias.

\begin{ndef}
  Se define la \emph{matriz de covarianzas} del vector $\boldsymbol X = (X_1,\dots,X_p)^T$ como:
  \[
\Sigma = \operatorname{Cov}(\boldsymbol X) = E\left[(\boldsymbol X-\boldsymbol \mu)(\boldsymbol X-\boldsymbol \mu)^T\right] = \begin{bmatrix} \sigma_{11} & \dots & \sigma_{1p} \\ \vdots& \ddots & \vdots \\ \sigma_{p1} &  \dots & \sigma_{pp}\end{bmatrix}\,,
\]
donde $\sigma_{ij} = E\left[(X_i - \mu_i)(X_j - \mu_j)\right] = \sigma_{ji}$ es la covarianza de las variables aleatorias $X_i$ y $X_j$. Solo puede definirse si existen todas las covarianzas. 
\end{ndef}

\begin{nota}
  Los elementos de la diagonal de la matriz de covarianzas se pueden calcular como \[\sigma_{ii}=E\left[(X_i - \mu_i)^2\right] = \operatorname{Var}(X_i) = \sigma_i^2\,,\]siendo $\sigma_i$ la \emph{desviación típica}.
\end{nota}

\begin{nota}
  La desigualdad de Cauchy-Schwarz nos dice que existen todas las covarianzas si, y solo si existen las varianzas de cada componente.
\end{nota}

\begin{nprop}[Propiedades elementales]
  La matriz de covarianzas verifica las siguientes propiedades elementales:
  \begin{enumerate}
    \item Es simétrica.
    \item Los elementos de su diagonal son no negativos.
    \item La clase de matrices de covarianzas en dimensión $p\times p$ coincide con la clase de matrices simétricas definidas no negativas.
    \end{enumerate}
  \end{nprop}

\begin{proof}
  La comprobación de las dos primeras propiedades es inmediata, por lo que vamos a ver únicamente la demostración de la propiedad 3.
  \begin{nlist}
    \item[3.] $\boxed{\implies}\,$ Supongamos que $\Sigma$ es la matriz de covarianza de un vector aleatorio $\boldsymbol X$, y $\boldsymbol \mu_{\boldsymbol X}$ es su vector de medias. Sea $\boldsymbol \alpha \in \mathbb R^p$. Tenemos que probar que $\boldsymbol \alpha^T \Sigma \boldsymbol \alpha \geq 0$. Consideramos la variable aleatoria $\boldsymbol \alpha^T \boldsymbol X$. \begin{align*}
      0 \leq \operatorname{Var}(\alpha^T X) = E\left[\|\alpha^T X - E[\alpha^T X]\|^2\right] &= E\left[\|\alpha^T X - \alpha^T \mu_X\|^2\right] \\ 
        &= E\left[\|\alpha^T(X-\mu)\|^2\right]\\ 
        &=^{(1)} E\left[\alpha^T(X-\mu)(X-\mu)^T \alpha\right] \\
        &= \alpha^T E\left[(X-\mu)(X-\mu)^T\right] \alpha \\
        &= \alpha^T \Sigma \alpha,
    \end{align*} donde en $(1)$ hemos separado el cuadrado y hemos traspuesto uno de los términos (el resultado es el mismo, un escalar).

    $\boxed{\impliedby}\,$ Sea $A$ una matriz simétrica, definida no negativa. Entonces, se puede encontrar una factorización $A = B B^T$. donde $C \in \mathscr{M}_{p}(\R)$.
    Consideramos un vector aleatorio $\vectX$ cuyas entradas son $p$ variables independientes e idénticamente distribuidas con distribución normal estándar. Entonces, $\Sigma_{\vectX} = I_p$. Esto se puede comprobar usando que $E[XY] = E[X]E[Y]$ si $X$ e $Y$ son variables aleatorias independientes. Consideramos también $\vectY = B\vectX$, entonces:

    \[
       \Sigma_{\vectY} = B\Sigma_{\vectX} B^T = B B^T = A
    .\]

  \end{nlist}
\end{proof}

%#TODO: revisar el estilo de esta parte, quizá añadir una subsección?
Vamos a ver ahora diferentes \textbf{casos} que se pueden dar respecto a la matriz $\Sigma$.

\paragraph{Si $\Sigma$ es definida positiva} Se nota como $\Sigma > 0$. En este caso $\Sigma$ es \emph{no singular} pues $(|\Sigma| > 0)$, y por tanto existe su inversa, $\Sigma^{-1}$. Esto es interesante pues nos permite normalizar el vector aleatorio de manera que tenga un vector de medias cero y una matriz de covarianzas que sea la identidad.

\begin{nota}
  Usaremos la notación $X\sim(\mu,\Sigma)$ para denotar que la media y la matriz de covarianzas están bien definidas.
\end{nota}

\begin{ndef}[Normalización]
    Dado un vector aleatorio $\vectX\sim(\mu,\Sigma)$, con $\Sigma>0$, se define su \emph{normalización} en «origen» y «escala» como
    \[
  \vectZ = C^{-1}(\vectX - \mu)\,,
\]
para cualquier matriz $C\in M_{p\times p}$ tal que $\Sigma = CC^T$. Esta matriz $C$ no es única, pues existen infinitas descomposiciones de $\Sigma$ de esta forma.

  \end{ndef}

Veamos a continuación algunas de las propiedades de esta variable $Z$.
\begin{nprop}
  Una normalización de un vector aleatorio tiene media $0$ y matriz de covarianzas identidad.
\end{nprop}

\begin{proof} \hfill\\
  Para la esperanza tenemos, \[
  E[Z] = E[C^{-1}(X-\mu)] = C^{-1}E[(X-\mu)] = C^{-1}(E[X] - \mu) = C^{-1}(\mu - \mu ) = 0,
  \] y para la matriz de covarianzas, \begin{align*}
    \operatorname{Cov}(Z) &= E[ZZ ^T] = E[C^{-1}(X-\mu)(X-\mu)^T(C^{-1})^T]\\
    \intertext{y por la propiedad de las matrices que nos dice que $(C^{-1})^T = (C^T)^{-1}$,}
      &= C^{-1}E[(X-\mu)(X-\mu)^T](C^T)^{-1} = C^{-1} \Sigma (C^T)^{-1} = C^{-1}CC^T(C^T)^{-1} = I \cdot I\\ 
      &= I.\qedhere
  \end{align*}
\end{proof}

\begin{ndef}
    Se define la \emph{distancia de Mahalanobis} de $\vectX\sim(\mu,\Sigma)$ con $\Sigma > 0$ con respecto a su vector de medias como
    \[
    \Delta(\vectX,\mu) = \left\{ (\vectX-\mu)^T \Sigma (\vectX-\mu) \right\} ^{\frac{1}{2}},
    \]
    que es una variable aleatoria.
  \end{ndef}

  Vamos a interpretar esta definición. Sea $\vectZ$ una normalización de $\vectX$: \begin{align*}
    \Delta(\vectX,\mu) &= \left\{ (\vectX-\mu)^T (CC^T)^{-1}(\vectX-\mu)\right\}^{\frac{1}{2}} = \left\{ (\vectX-\mu)^T (C^T)^{-1}C^{-1}(\vectX-\mu)\right\}^{\frac{1}{2}}\\
      &= \left\{\vectZ^T \vectZ\right\}^{\frac{1}{2}} = \Vert \vectZ\Vert\,,
  \end{align*} con norma la euclídea en $\mathbb R^p$. Así, esta distancia es la norma de cualquier normalización de $\vectX$. Recordemos que $\Vert \vectZ\Vert$ es una variable aleatoria, pues es una aplicación de una función medible a un vector aleatorio.

  \begin{nprop} Sea $\vectX = (X_1, \dots, X_p)^T \sim (\boldsymbol \mu, \Sigma)$ un vector aleatorio con $\Sigma > 0$. Entonces,
    \begin{nlist}
    \item $E[\Delta^2 (X,\mu)] = p$,
    \item la ecuación $\Delta(X,\mu) = k$ —con $k\geq 0$ constante— define la \textit{hipervariedad de contorno} correspondiente a aquellos puntos $x \in \mathbb R^p$ tales que, en el espacio transformado $\mathbb R^p$ por
      \[
      x \mapsto z = C^{-1}(x-\mu)
      \]
      se sitúan en la esfera euclídea $p-$dimensional de radio $k$ y centro en el origen.
    \end{nlist}

  \end{nprop}

\paragraph{$\Sigma$ es \textbf{semidefinida positiva}} (esto es, $\Sigma \geq 0$ con $|\Sigma| = 0$).  En este caso $\Sigma$ es singular, por lo que no existe $\Sigma ^{-1}$. Por tanto, se tiene que $\operatorname{rango}(\Sigma) = r < p$ y $\Sigma = C C^T$ con $C \in \mathscr{M}_{p\times r}(\R)$ y cuyo rango es $r$.\\

  \begin{ncor}
    Con probabilidad $1$, las componentes del vector $\vectX = (X_1, \dots, X_n)$ cumplirán una relación de dependencia lineal del tipo:
    \[
    \alpha^T \vectX = k, \quad \quad \alpha \in \mathbb R^p, \quad \alpha \ne 0 , \quad k \in \R
    \]
    
  Por tanto, toda la variabilidad de $\vectX$ se sitúa en un hiperplano afín (casi seguramente con respecto a la medida $P$).
\end{ncor}

  \begin{proof}
    Como $\det(\Sigma) = 0$, $\exists \alpha \ne 0$ tal que $\alpha^T \Sigma \alpha = 0$. Definimos la variable aleatoria $\vectY = \alpha^T \vectX$, y se tiene que:
    
    \[
      E[\vectY] = E[\alpha^T \vectX] = \alpha^T E[\vectX] = \alpha^T \mu =: k
    ,\] y
    
     \[
        \begin{split}
          \operatorname{Var}(\vectY) & = E\left[\|\vectY-k\|^2\right]  = E\left[\|\alpha^T\vectX - \alpha^T \mu\|^2\right] \\
          & = E\left[\|\alpha^T(\vectX-\mu)\|^2\right] = E\left[\alpha^T(\vectX-\mu)(\vectX-\mu)^T\alpha\right] \\
          & = \alpha^T E\left[(\vectX-\mu)(\vectX-\mu)^T\right]\alpha = \alpha^T \Sigma \alpha = 0
        \end{split}  
     \]
      con lo que $\vectY$ es una variable aleatoria degenerada: $P[\vectY = k] = 1 \iff \alpha^T \vectX = k$ c.s.
  \end{proof}

Dicho esto, existen varias \textbf{medidas de variación global}. Podemos distinguir, dado $\Sigma$, y $\lambda_j$ con $j = 1,\dots,p$ los autovalores de $\Sigma$:
\begin{nlist}
\item $\det(\Sigma) =  \prod_{j=1}^p \lambda_j$,
  \item $\traza(\Sigma) = \sum_{j = 1}^p \sigma_j^2 = \sum_{j=1}^p \lambda_j$.
\end{nlist}

\subsection{Caracterización de la distribución de un v.a. en términos de las distribuciones univariantes de c.l. de sus componentes}

\begin{nth} \label{dist_univ}
  Sea $\vectX = (X_1,\dots, X_n)^T$ un vector aleatorio. La distribución de $\vectX$ queda unívocamente determinada por las distribuciones univariantes de la forma
  \[
\alpha ^T \vectX, \quad \alpha \in \mathbb R ^p\,.
  \]
\end{nth}
\begin{proof}
  Sea $Y_\alpha = \alpha^T \vectX$ (variable aleatoria unidimensional), para cada $\alpha \in \mathbb R^p$. La función característica de $Y_\alpha$ viene dada por
  \[
  \psi_{Y_\alpha}(t) = E\left[e^{itY_\alpha}\right] = E \left[e^{it(\alpha^T \vectX)}\right] \quad \text{para todo } t \in \mathbb R\,.
  \]
  En particular, si $t=1$
  \[
  \psi_{Y_\alpha}(1) = E\left[e^{i\alpha^T \vectX}\right] = \psi_{\vectX} (\alpha)
  \]
  Es decir:
  \[
  \psi_{\vectX}(t) = \psi_{Y_t}(1), \quad \forall t \in \mathbb R ^p
  \]
\end{proof}

\subsection{Momentos y cumulantes}

Sea $\vectX= (X_1,\dots,X_p)^T$ un vector aleatorio con función característica $\phi_{\vectX}$.
\begin{ndef}
  Se define el \textbf{momento} (no centrado) $p-$dimensional de orden $(r_1, \dots, r_p)$ de $X$ como:
  \[
\mu_{r_1,\dots,r_p}^{1,\dots,p} = E \left[ X_1^{r_1} \dots X_p^{r_p}\right]\,.
  \]
\end{ndef}

Los momentos se pueden obtener a partir de la función característica derivándose de su expansión de Taylor(respecto al origen).

\[
\phi_X(t) = E\left[e^{it^T X}\right] = E\left[\sum_{r=0}^\infty \frac{1}{r!}(i t^T X)^r\right] = \sum_{r= 0}^\infty \sum _{r_1+\dots + r_p = r} \mu_{r_1 \dots r_p}^{1 \dots p} \frac{(it_1)^{r_1} \dots (it_p)^{r_p}}{r_1 ! \dots r_p !}
\]
En particular, podemos enunciar este teorema:
\begin{nth}
  Si $E=\left[|X_1|^{m_1} \dots |X_p|^{m_p}\right] < \infty$, entonces la función característica de $X$ es $(m_1, \dots, m_p)$ veces diferenciable y , si $m = m_1 + \dots + m_p$,
  \[
\frac{\partial ^m}{\partial t_1^{m_1} \dots \partial t_p^{m_p}} \phi_X(t)| _{t = 0} = i^m \mu_{m_1}^1 \dots \mu_{m_p}^p
  .\]
\end{nth}

Consideremos el logaritmo de la función característica:
\[
\log \phi(t).
\]
\begin{ndef}
  Se definen los \emph{cumulantes} ($p-$dimensionales) de orden $(r_1 , \dots, r_p)$ como los coeficientes de la correspondiente expansión:
  \[
  \log \phi_X(t) =  \sum_{r= 0} \sum_{r_1+\dots + r_p = r} \mathcal K_{r_1 \dots r_P}^{1\dots p} \frac{(it_1)^{r_1} \dots (it_p)^{r_p}}{r_1 ! \dots r_p !}
  .\]
\end{ndef}


\subsubsection{Cambio de variables}

\begin{nth} \label{cambiovariablealeatoria}
  Sea $\vectX = (X_1,\dots,X_n)^T$ un vector aleatorio con función de densidad $f_X(x)$ positiva sobre $S \subseteq \mathbb R^d$ y continua. Sea $Y = (Y_1,\dots,Y_p)^T$ un vector aleatorio con:
  \[
  Y = g(X) = (g_1(X),\dots ,g_p(X))^T
  \]
  con $g = (g_1,\dots,g_p): S \to \mathcal T$ biyectiva, por lo que $\exists g^{-1}$, y denotamos $g^{-1} = h = (h_1,\dots, h_p)^T : \mathcal T \to S$.

  Supongamos que existen las derivadas parciales:
  \[
    \frac{\partial h_i(y)}{\partial y_j} \quad (i,j = 1,\dots,p)
  \]
  y que son continuas sobre $\mathcal T$.

  Entonces, la función de densidad $f_Y(y)$ del vector $Y=(Y_1,\dots,Y_p)^T = g(X)$ viene dada por:
  \[
  f_Y(y) = f_X(g^{-1}(y))\left|\det(J_{g^{-1}}(y))\right|
  .\]
\end{nth}

\begin{ncor}[Caso lineal] \label{cambiocasolineal}
  Sea $Y = BX + b$ con $B\in \mathscr{M}_{p}(\R)$ y no singular. En este caso,
  \[
  J_{g^{-1}}(g(x)) = \left[ J_{g}(x) \right]^{-1} = \det(B)^{-1} = \det\left(B^{-1}\right)
  \]
  y se tiene:
  \[
  f_Y(y) = f_X\left(B^{-1}(y-b)\right)\left|\det(B)^{-1}\right|
  .\]

\end{ncor}
\begin{nota}
  Al factorizar una matriz definida positiva, siempre existe una factorización $CC^T$ con $\det(C) > 0$.
  \end{nota}

\section{Distribución normal multivariante (en el caso $\Sigma > 0$)}
  En esta sección estudiamos la distribución normal multivariante, refiriéndonos siempre al caso en el que tiene matriz de covarianzas definida positiva.
Algunos aspectos que justifican el estudio de esta distribución en particular:
\begin{itemize}
\item es la distribución multivariante más estudiada, y existen infinidad de resultados acerca de la misma,
\item DNM es la base de muchas técnicas del análisis multivariante confirmatorio o inferencial,
\item DNM es una extensión \emph{natural} de la DNM univariante al caso multivariante,
\item las distribuciones marginales de cualquier orden en una DNM son también normales,
\item las distribuciones condicionadas (internamente) en una DNM también son normales y se obtienen de forma sencilla,
\item la familia de DNM es cerrada bajo transformaciones lineales,
\item la familia de DNM es cerrada bajo combinaciones lineales de vectores (mutuamente) independientes,
\item la DNM viene determinada por los momentos de primer y segundo orden (media y matriz de covarianzas),
  \item si dos subvectores de un vector aleatorio con DNM tienen correlaciones cruzadas nulas, entonces dichos subvectores son (mutuamente) independientes.

\end{itemize}


\begin{ndef} \label{posvar:defnormal}
  Sea $\vectX = (X_1,\dots, X_p)^T$ un vector aleatorio. Se dice que $\vectX$ tiene una distribución normal $p-$variante si su densidad es de la forma:
  
  \[
    f_{\vectX}(x) = \frac{1}{(2\pi)^{p/2}\det(\Sigma)^{\frac{1}{2}}} \exp\left\{- \dfrac{1}{2}(x-\mu)^T \Sigma^{-1}(x-\mu)\right\}
  \]

Siendo $\mu = (\mu_1, \dots, \mu_p)^T \in \mathbb R^p$ y $\Sigma$ una matriz escalar $p\times p$ simétrica y definida positiva.
\end{ndef}

Veamos que, en efecto, $f_{\vectX}$ es una función de densidad
\begin{enumerate}
\item $f_{\vectX}(x) \geq 0, \ \forall x \in \mathbb R$,
\item $\int_{\mathbb R^p} f_{\vectX}(x) \mathrm{d}x = 1$. En efecto, como $\Sigma$ es una matriz simétrica y definida positiva, podemos descomponerla de la forma $\Sigma = C C^T$ con $C \in \mathcal \mathscr{M}_{p}(\R), \det(C) > 0$. La forma de la densidad sugiere el cambio de variable $Z = C^{-1}(x-\mu)$.  El determinante del jacobiano de la transformación es $\det\left(C^{-1}\right)$. Por otro lado:

\[
  (x-\mu)^T \Sigma^{-1}(x-\mu) = zz^T
\]

Por tanto, podemos escribir:

\begin{DispWithArrows*}[fleqn, mathindent = 0cm, wrap-lines]
      \int_{\R^p} f_{\vectX}&(x) \mathrm{d}x \\
      &= \int_{\mathbb R^p} \frac{1}{(2\pi)^{p/2}\det(\Sigma)^{\frac{1}{2}}} \exp\left\{- \dfrac{1}{2}(x-\mu)^T \Sigma^{-1}(x-\mu)\right\} \mathrm{d}x \Arrow{cambio de variable} \\
      & = \int _{\mathbb R^p} \frac{1}{(2\pi)^{\frac{p}{2}}\det(C)^{\frac{1}{2}} \det(C^T)^{\frac{1}{2}}} \exp\left\{ -\frac{1}{2}zz^T\right\} \det(C) \mathrm{d}z\\
      & = \int_{\mathbb R^p} \frac{1}{\left(\sqrt{2\pi}\right)^p}\exp\left\{-\frac{1}{2}\sum_{j=1}^p z_j^2\right\} \mathrm{d}z_1 \dots \mathrm{d}z_p\\
      & = \prod_{j=1}^p \int_{\mathbb R} \underbrace{ \frac{1}{\sqrt{2\pi}} \exp\left\{- \frac{1}{2} z^2\right\} }_{\text{densidad de una $N(0,1)$}} \mathrm{d}z_j \\
      &= \prod_{j=1}^p 1 = 1.
\end{DispWithArrows*}
\end{enumerate}


\subsection{Vector de medias y matriz de covarianzas}
Del cálculo anterior se desprende que el vector aleatorio \[ \vectZ = C^{-1}(\vectX-\mu) \] se distribuye con función de densidad
\[
f_{\vectZ}(z) = \prod_{j = 1}^p \frac{1}{\sqrt{2\pi}}\exp\left\{-\frac{1}{2}z_j^2\right\} = \prod_{j=1}^p f_{Z_j}(z_j)
\]
Es decir, para cada $j = 1,\dots, p$
\[
Z_j \sim N(0,1)
\]
siendo $Z_1,\dots,Z_p$ independientes (en particular, incorreladas), luego $\vectZ \sim N_p(0,I_p)$.

A partir de esto, usando \ref{cambiovariablealeatoria}, se obtiene que $\mu$ y $\Sigma$ son, respectivamente, el vector de medias y la matriz de covarianzas del vector aleatorio $\vectX \sim N_p(\mu,\Sigma)$.


\begin{nprop}[Transformación afín sobre una normal] \label{afinnormal}
  Sea $\vectX \sim N_p(\mu,\Sigma)$, con $\Sigma > 0$ e $\vectY = B\vectX+b$ con $B$ matriz escalar $p\times p$ no singular y $b\in\R^p$. Entonces, se tiene que:
  \[
     \vectY \sim N_p(B\mu+b, B \Sigma B^T)
  .\]
\end{nprop}

\begin{proof}
  De \ref{cambiocasolineal} se sigue que

  \[
  \begin{split}
    f_{\vectY}(y) & = f_{\vectX}(B^{-1}(y-b))\left|\det(B)^{-1}\right| \\
    & = \frac{1}{(2\pi)^{p/2} \det(\Sigma)^{1/2} \left|\det(B)\right|}
    \exp\left\{ -\frac{1}{2} \left(B^{-1}(y-b) - \mu\right)^T \Sigma^{-1} \left(B^{-1}(y-b) - \mu \right) \right\} \\
    & = \frac{1}{(2\pi)^{p/2} \det(B^T\Sigma B)}
    \exp\left\{ -\frac{1}{2} \left(y-b - B\mu\right)^T (B^T\Sigma B)^{-1} \left(y - b - B\mu \right) \right\}.
  \end{split}
  \]
\end{proof}

\begin{nth}[Caracterización de DNM] \label{posvar:car-i}
  Sea $\vectX$ un vector aleatorio $p-$dimensional, entonces $\vectX$ tiene distribución normal multivariante, de vector de medias $\mu$ y matriz de covarianzas $\Sigma > 0$, si y solo si
  
  \[
    \vectX = A\vectZ + \mu
  \]
    
con $A\in \mathscr{M}_{p}(\R)$ no singular, $AA^T = \Sigma$, $\vectZ \sim N_p(0,I_p)$.
\end{nth}

\begin{proof}
  Supuesto que $\vectX = A\vectZ + \mu$, de \ref{afinnormal} se deduce que $\vectX \sim N_p(\mu, \Sigma)$. Supuesto que $\vectX \sim N_p(\mu, \Sigma)$,
  sin más que definir $\vectZ := A^{-1}(\vectX - \mu)$ se tiene la igualdad, y claramente $\vectZ \sim N_p(0, I_p)$.
\end{proof}

\subsubsection{Sobre independencia y condicionamiento}

  \begin{nprop}
    Sea $\vectX = (X_1,\dots,X_p)^T$ un vector aleatorio con DNM $\vectX \sim N_p(\mu,\Sigma)$, $\Sigma > 0$. Si la matriz $\Sigma$ es diagonal,
    entonces las variables aleatorias componentes del vector son independientes y tienen distribución normal univariante $X_i \sim N\left(\mu_i, \sigma_i^2\right)$.
  \end{nprop}
  \begin{proof}
    Por hipótesis, sabemos que:
    \[
    f_X(x) = \frac{1}{(2\pi)^{\frac{p}{2}} \det(\Sigma)^{\frac{1}{2}}}\exp\left\{- \frac{1}{2}(x-\mu)^T \Sigma^{-1}(x-\mu)\right\}
    \]
    Y además:
    \begin{itemize}
    \item $\det(\Sigma)^{\frac{1}{2}} = \left(\prod_{i = 1}^p \sigma_i^2\right)^{\frac{1}{2}} = \prod_{i=1}^p \sigma_i$,
    \item $\Sigma^{-1} = \diag\left(\left(\sigma_1^2\right)^{-1}, \dots , \left(\sigma_p^2\right)^{-1}\right)$,
      \item $(x-\mu)^T \diag\left(\left(\sigma_1^2\right)^{-1}, \dots , \left(\sigma_p^2\right)^{-1}\right) (x-\mu) = \sum_{i=1}^p (x_i-\mu_i)^2 \sigma_i^{-2} = \sum_{i = 1}^p \left(\frac{x_i-\mu_i}{\sigma_i}\right)^2$.
    \end{itemize}
    Por tanto, reescribiendo:
    \[
      \begin{split}
        f_{\vectX}(x) & = \frac{1}{(2\pi)^{\frac{p}{2}} \prod \sigma_i} \exp \left\{ -\frac{1}{2}(x-\mu)^T \diag\left(\left(\sigma_1^2\right)^{-1}, \dots , \left(\sigma_p^2\right)^{-1}\right) (x-\mu)  \right\} \\
        & = \frac{1}{(2\pi)^{\frac{p}{2}} \prod \sigma_i} \exp \left\{ -\frac{1}{2} \sum_{i = 1}^p \left(\frac{x_i-\mu_i}{\sigma_i}\right)^2  \right\} \\
        & = \prod_{i = 1}^p  \frac{1}{(2\pi)^{\frac{p}{2}} \prod \sigma_i} \exp \left\{ -\frac{1}{2} \left(\frac{x_i-\mu_i}{\sigma_i}\right)^2  \right\} \\
        & = \prod_{i = 1}^p f_{X_i}(x_i)
    \end{split}
    \]

  \end{proof}


  El recíproco de este último resultado dado es cierto:
  \begin{nprop}
    Si $\vectX = (X_1,\dots,X_p)^T$ es un vector aleatorio con componentes \emph{mutuamente} independientes que tienen DNM, $X_i \sim N(\mu_i, \sigma_i)$, con $\sigma > 0$. Entonces, $\vectX$ tiene DNM
    \[
    \vectX \sim N_p(\mu, \Sigma)
    \]
    con $\mu = (\mu_1,\dots,\mu_p)^T$ y $\Sigma = \diag(\sigma_1^2 , \dots , \sigma_p^2) > 0$.
  \end{nprop}

  \begin{nota}
Exigimos $\sigma_i > 0, \ \forall i = 1,\dots, p$, y entonces $\Sigma > 0$.
  \end{nota}

  \begin{nprop}
    Sea $\vectX$ una v.a. $N(0,1)$ y sea $W$ una v.a. con distribución $U(\{-1,1\})$, independiente de $\vectX$. Consideramos $\vectY = W\vectX$. Entonces, se verifica:
    \begin{itemize}
    \item $\vectY \sim N(0,1)$,
    \item $\vectX$ e $\vectY$ son incorreladas,
      \item $\vectX$ e $\vectY$ no son independientes. 
    \end{itemize}

  \end{nprop}
  Esta proposición es un contraejemplo para ver que, en general, incorrelación no implica independencia (aunque alguna de las marginales sea una normal).

  \begin{nth} \label{independenciacovbloques}
    Sea $\vectX = (X_1, \dots ,X_p)^T \sim N_p(\mu,\Sigma)$ con $\Sigma > 0$. Supongamos que las componentes de $\vectX$ están ordenadas de tal modo que, por la partición del vector $\vectX = \left(\vectX_{(1)}^T | \vectX_{(2)}^T\right)^T$ con $\vectX_{(1)} = (X_1,\dots,X_q)^T$ y $\vectX_{(2)} = (X_{q+1},\dots,X_p)^T$, se tiene
    
    \[
    \mu = \left(\mu_{(1)}^T | \mu_{(2)}^T\right)^T, \quad \Sigma = \begin{pmatrix} \Sigma_{(11)} & 0 \\ 0 & \Sigma_{(22)} \end{pmatrix}
    .\]
    
    Entonces, $\vectX_{(1)}$ y $\vectX_{(2)}$ son mutuamente independientes y además, $\vectX_{(1)} \sim N_q(\mu_{(1)}, \Sigma_{(11)})$ y $\vectX_{(2)} \sim N_{p-q}(\mu_{(2)}, \Sigma_{(22)})$.
  \end{nth}

  \begin{proof}
    Comenzamos notando que, como $\Sigma = \diag(\Sigma_{(11)},\Sigma_{(22)})$, entonces:
    \begin{itemize}
    \item $\det(\Sigma)^{\frac{1}{2}} = \det(\Sigma_{(11)})^{\frac{1}{2}} \det(\Sigma_{(22)})^{\frac{1}{2}}$
      \item $ \Sigma^{-1} = \diag(\Sigma_{(11)}^{-1}, \Sigma_{(22)}^{-1})$
    \end{itemize}

    De esta forma, tenemos:
    
    \[
    \begin{split}
      (x-\mu)^T \Sigma^{-1}(x-\mu) & = \left( (x_{(1)}- \mu_{(1)})^T | (x_{(2)}-\mu_{(2)})^T \right) \begin{bmatrix} \Sigma_{(11)} & 0 \\ 0 & \Sigma_{(22)} \end{bmatrix}
      \left(\begin{array}{c} x_{(1)} - \mu_{(1)} \\ \hline
        x_{(2)} - \mu_{(2)} \end{array}\right) \\
       & = (x_{(1)}- \mu_{(1)})^T \Sigma_{(11)}^{-1}  (x_{(1)}- \mu_{(1)}) +  (x_{(2)}- \mu_{(2)})^T \Sigma_{(22)}^{-1}  (x_{(2)}- \mu_{(2)})
      \end{split}
    \]
    
    Podemos factorizar entonces la función de densidad de $\vectX$:
    
    \[
    \begin{split}
    f_{\vectX}(x) =  \frac{1}{(2\Pi)^{\frac{q}{2}} \det(\Sigma_{(11)})^{\frac{1}{2}} } \exp \left\{ - \frac{1}{2} (x_{(1)}- \mu_{(1)})^T \Sigma_{(11)}^{-1}  (x_{(1)}- \mu_{(1)})\right\} \\
    \cdot \frac{1}{(2\Pi)^{\frac{p-q}{2}} \det(\Sigma_{(22)})^{\frac{1}{2}} } \exp \left\{ - \frac{1}{2} (x_{(2)}- \mu_{(2)})^T \Sigma_{(22)}^{-1}  (x_{(2)}- \mu_{(2)})\right\}
    \end{split}
    \]
    
  \end{proof}

  El recíproco también es cierto, en el sentido en el que lo era el recíproco del resultado anterior.

  \begin{nth}
    Sea $\vectX = (X_1, \dots, X_p)^T \sim N_p(\mu,\Sigma)$ con $\Sigma > 0$. Supongamos el particionamiento de $\vectX$ como en el resultado anterior, pero ahora con
    
    \[
\Sigma = \begin{pmatrix}  \Sigma_{(11)} & \Sigma_{(12)} \\ \Sigma_{(21)} & \Sigma_{(22)} \end{pmatrix}
\]

Entonces, se tiene:
\begin{enumerate}
\item $\vectX_{(1)}$ y $\vectX_{(2)} - \Sigma_{(21)} \Sigma_{(11)}^{-1}\vectX_{(1)}$ son independientes, \label{indepitem}
\item \[
  \begin{cases}
    \vectX_{(1)} \sim N_q(\mu_{(1)}, \Sigma_{(11)}) \\
    \vectX_{(2)} - \Sigma_{(21)} \Sigma_{(11)}^{-1}X_{(1)} \sim N_{p-q}\left(\mu_{(2)} - \Sigma_{(21)}\Sigma_{(11)}^{-1}\mu_{(1)} \ , \ \Sigma_{(22)} - \Sigma_{(21)} \Sigma_{(11)}^{-1} \Sigma_{(12)}\right)
  ,\end{cases}
  \] \label{distitem}
  
\item La distribución condicionada de $\vectX_{(2)}$ dado $\vectX_{(1)} = x_{(1)}$ es una DNM
  
  \[
    N_{p-q}\left(\mu_{(2)} + \Sigma_{(21)}\Sigma_{(11)}^{-1}(x_{(1)} - \mu_{(1)}) \ , \ \Sigma_{(22)} - \Sigma_{(21)} \Sigma_{(11)}^{-1} \Sigma_{(12)}\right)
  .\] \label{conditem}
  
\end{enumerate}
  \end{nth}
  \begin{proof}
    Sea $C = \begin{pmatrix} I_1 & 0 \\ -\Sigma_{(21)}\Sigma_{(11)}^{-1} & I_2\end{pmatrix}$. $C$ es no singular, pues $\det(C) = 1$. Consideramos el cambio lineal $\vectY = C \vectX$:
      
   \[
      \vectY = \begin{pmatrix} \vectX_{(1)} \\ \vectX_{(2)} - \Sigma_{(21)}\Sigma_{(11)}^{-1}\vectX_{(1)}\end{pmatrix} := \begin{pmatrix} \vectY_{(1)} \\ \vectY_{(2)} \end{pmatrix}.
   \]

   Por \ref{afinnormal}, $\vectY \sim N_p\left( C\mu, C\Sigma C^T \right)$. Se tiene:

   \[
      C\mu = \left(
      \begin{array}{c}
        \mu_{(1)} \\ \hline
        \mu_{(2)} - \Sigma_{(21)}\Sigma_{(11)}^{-1}\mu_{(1)}
      \end{array}
      \right),
      \quad
      C\Sigma C^T = \left(
      \begin{array}{c | c}
        \Sigma_{(11)} & 0 \\ \hline
        0            & \Sigma_{(22\cdot 1)}
      \end{array}
      \right),
  \]

  donde $\Sigma_{(22\cdot 1)} := \Sigma_{(22)} - \Sigma_{(21)}\Sigma_{(11)}^{-1}\Sigma_{(12)}$. Así, \ref{indepitem} y \ref{distitem} se siguen de \ref{independenciacovbloques}.

  Para probar \ref{conditem} escribimos, usando el teorema de Bayes, la expresión de la de función de densidad de la distribución condicionada:

  \begin{DispWithArrows*}[fleqn, wrap-lines]
    f_{\vectX_{(2)} | \vectX_{(1)} = x_{(1)}}(x_{(2)}) & = \frac{f_{\vectX_{(1)}, \vectX_{(2)}} (x_{(1)}, x_{(2)})} {f_{\vectX_{(1)}}(x_{(1)})} \Arrow{usando el teorema de cambio de variable \ref{cambiocasolineal} con $\vectX = C^{-1}\vectY$, $\det(C) = 1$} \\
    & = \frac{f_{\vectY_{(1)}, \vectY_{(2)}}\left(x_{(1)}, x_{(2)} - \Sigma_{(21)}\Sigma_{(11)}^{-1}x_{(1)}\right)} {f_{\vectX_{(1)}}(x_{(1)})}
  \end{DispWithArrows*}


  Veamos el valor de la forma cuadrática que aparecerá en $f_{\vectY_{(1)}, \vectY_{(2)}}$:

  \[
     \left(
     \begin{array}{c}
       x_{(1)} \\ \hline
       x_{(2)} - \Sigma_{(21)}\Sigma_{(11)}^{-1}x_{(1)}
     \end{array}
     \right) - C\mu = \left(
     \begin{array}{c}
       x_{(1)} - \mu_{(1)} \\ \hline
       x_{(2)} - \mu_{(2)} - \Sigma_{(21)}\Sigma_{(11)}^{-1}(x_{(1)} - \mu_{(1)})
     \end{array}
     \right) := D(x_{(2)}),
   \]

   \[
   \begin{split}
     D(x_{(2)})^T (C\Sigma C^T) D(x_{(2)}) = \overbrace{(x_{(1)} - \mu_{(1)})^T \Sigma_{(11)}^{-1} (x_{(1)} - \mu_{(1)})}^{:= A} \\
     + \underbrace{(x_{(2)} - \mu_{(2)} - \Sigma_{(21)}\Sigma_{(11)}^{-1}(x_{(1)} - \mu_{(1)}))^T \Sigma_{(22\cdot 1)}^{-1}
       (x_{(2)} - \mu_{(2)} - \Sigma_{(21)}\Sigma_{(11)}^{-1}(x_{(1)} - \mu_{(1)}))}_{:= B(x_{(2)})}
   \end{split}
   \]  

   Resultando:

   \[
   \begin{split}
     f_{\vectX_{(2)} | \vectX_{(1)} = x_{(1)}}(x_{(2)}) & = \frac{ \frac{1}{(2\pi)^{p/2} (\det(\Sigma_{(11)})\det(\Sigma_{(22\cdot 1)}))^{1/2}} \exp\left\{ -\frac{1}{2} (A + B(x_{(2)}))  \right\} }
     { \frac{1}{(2\pi)^{q/2} \det(\Sigma_{(11)})^{1/2}} \exp\left\{ -\frac{1}{2} A \right\} } \\
     & = \frac{1}{(2\pi)^{\frac{p-1}{2}} \det(\Sigma_{(22\cdot 1)})^{1/2}} \exp\left\{ -\frac{1}{2} B(x_{(2)}) \right\}.
   \end{split}
   \]
  \end{proof}



  \subsection{Función característica de la DNM}

  Cabe recordar en esta sección la definición \ref{funcioncaracteristica}.

  \begin{nprop}[Función característica de la DNM] \label{posvar:car-ii}
    Dado un vector aleatorio $\vectX \sim N_p(\mu, \Sigma)$, su función característica viene dada por
    
    \[
      \psi_{\vectX}(t) = \exp\left\{ it^T\mu - \frac{1}{2} t^T \Sigma t \right\}
    .\]
  \end{nprop}

  
  \begin{proof}
    Tomamos una descomposición $\Sigma = CC^T$, con $C\in \mathscr{M}_{p}(\R)$ no singular, $\det(C) > 0$, y aplicamos el cambio de variable $\vectY = C^{-1}(\vectX - \mu)$.
    \[
    \begin{split}
      \Psi_{\vectX}(t) & = \int_{\mathbb R^p} e^{it^T x} f_{\vectX} (x) \mathrm{d}x \\
      & = \int_{\R^p} \frac{1}{(2\Pi)^{\frac{p}{2}}\det(\Sigma)^{\frac{1}{2}}} \exp \left\{ it^Tx - \frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu) \right\}\mathrm{d}x \\
      & =  \int_{\R^p} \frac{1}{(2\Pi)^{\frac{p}{2}}\det(\Sigma)^{\frac{1}{2}}} \exp \left\{ it^T(Cy + \mu)- \frac{1}{2}y^Ty \right\} \det(C)\mathrm{d}y \\
      & = e^{it^T\mu} \int_{\mathbb R^p} \frac{1}{(2\Pi)^{\frac{p}{2}}} \exp \left\{ \sum_{j=1}^p i\alpha_j y_j -  \frac{1}{2}y_j^2 \right\} \\
      & = \prod_{j=1}^p e^{it_j\mu_j}\Psi_{Y_j}(\alpha_j) \\
      & =^{(1)} \exp\left\{it^T \mu- \frac{1}{2}t^T \Sigma t\right\},
    \end{split}
    \]
    con $\alpha_j = t^T c_{.j}$, y $c_{.j}$ la columna $j-$ésima de la matriz $C$, recordando que:
    \[
    \Psi_{N(\mu,\sigma^2)}(t) = e^{it\mu - \frac{1}{2}\sigma^2 t^2}
    \]
    y en $(1)$ teniendo en cuenta:
    \begin{itemize}
    \item $ \sum_{j=1}^p t_j \mu_j = t^T \mu$,
    \item $\sum_{j = 1}^p \alpha_j^2 = \alpha^T \alpha = t^T CC^T t = t^T \Sigma t$.
    \end{itemize}

  \end{proof}

 
    \begin{nprop} \label{linealesrangomaximo}
      Sea $\vectX = (X_1,\dots,X_p)^T \sim N_p(\mu,\Sigma)$ con $\Sigma > 0$. Sea $\vectY = (Y_1,\dots,Y_q)^T$ definido como $\vectY = B\vectX + b$, donde $B\in \mathscr{M}_{q\times p}(\R)$ de rango $q \leq p$, y $b \in \mathbb R^q$. Entonces, se tiene que
      \[
        \vectY \sim N_q\left(B\mu+b,\, B\Sigma B^T\right),
      \]
      con $B\Sigma B^T > 0$.
    \end{nprop}
    \begin{proof}
      Para $t\in \R^q$, se tiene:

      \[
      \begin{split}
        \psi_{\vectY}(t) & = E\left[ e^{it^T(B\vectX + b)} \right] \\
        & = e^{it^Tb} E\left[ e^{i\left(B^Tt\right)^T\vectX} \right] \\
        & = e^{it^Tb} \psi_{\vectX}\left(B^Tt\right) \\
        & = e^{it^Tb} \exp\left\{i\left(B^Tt\right)^T \mu- \frac{1}{2}\left(B^Tt\right)^T \Sigma B^Tt\right\} \\
        & = \exp\left\{it^T(B\mu + b) - \frac{1}{2}t^T B\Sigma B^Tt\right\},
      \end{split}
      \]
      y la correspondencia biunívoca entre funciones características y distribuciones concluye la prueba de la primera afirmación.
      Además, se verifica que $B\Sigma B^T > 0$ por ser tanto $\Sigma > 0$ como $B$ de rango máximo.
    \end{proof}

    \begin{ncor}[Marginalización]
      Sea $\vectX = (X_1,\dots,X_p)^T \sim N_p(\mu,\Sigma)$ un vector aleatorio con $\Sigma > 0$. Entonces para todo subvector $\vectX_r = (X_{r_1},\dots, X_{r_q})^T$ —donde $r = (r_1,\dots,r_q)^T$\, con $r_1,\dots,r_q \in \{1,\dots,p\}$ y $q \leq p$— se tiene que $\vectX_r \sim N_q(\mu_r, \Sigma_r)$, siendo
      \begin{itemize}
      \item $\mu_r$ el subvector de $\mu$ correspondiente a $r$,\, y
        \item $\Sigma_r$ la submatriz de $\Sigma$ definida por las filas y columnas correspondientes a $r$.
      \end{itemize}
    \end{ncor}

\subsection{Algunas descomposiciones matriciales. Diagonalización.}
    
\begin{nprop}
  Sea $A$ una matriz simétrica y definida positiva. Entonces, $\|x\|_A := (x^TAx)^{1/2}$ es una norma en $\R^p$.
\end{nprop}

\subsubsection{Descomposición $U^TDU$ de matrices simétricas definidas no negativas.}

Consideramos $A\in \mathscr{M}_p(\R)$ simétrica y definida no negativa.

\begin{nth}[Descomposición $U^TDU$]
  Existe una matriz $U$ triangular superior, con los elementos de la diagonal todos 1,
  tal que

  \[ A = U^TDU, \]

  con $D$ una matriz diagonal con elementos de la diagonal no negativos, única.
\end{nth}

\begin{ncor}[Descomposición de Cholesky]
  Sea $r$ el rango de $A$, entonces existe una única matriz triangular superior $T$
  con $r$ elementos diagonales positivos y $p-r$ filas nulas, tal que

  \[ A = T^TT, \]

  y, de hecho, se tiene

  \[ T = D^{1/2}U \]

  si $D$ y $U$ son las de la descomposición $U^TDU$.
\end{ncor}

\subsubsection{Descomposición espectral de matrices simétricas definidas no negativas.}

En esta sección no exigimos que $A$ sea simétrica ni definida no negativa.

\begin{ndef}[Polinomio característico]
  Se llama \emph{polinomio característico} de $A$ al definido por:

  \[
    p(\lambda) = \det(A - \lambda I)
  .\] 
\end{ndef}

\begin{nprop}
  Las raíces reales del polinomio característico de $A$ son los valores propios de $A$.
\end{nprop}

\begin{ndef}[Multiplicidades algebraica y geométrica]
  Sea $\lambda \in \R$ un valor propio de $A$. Se llama \emph{multiplicidad algebraica de $\lambda$} a la multiplicidad
  de $\lambda$ como raíz del polinomio característico. Se llama \emph{multiplicidad geométrica de $\lambda$} a la dimensión
  del subespacio que generan los vectores propios asociados a $\lambda$.
\end{ndef}

\paragraph{Diagonalización.}\hfill

Vamos a recordar algunos resultados relativos a la diagonalización de matrices, que nos llevarán a la descomposición espectral.

    \begin{ndef}
      Se dice que $A\in \mathscr{M}_{p}(\R)$ es diagonalizable si existe una matriz $Q\in \mathscr{M}_{p}(\R)$ no singular tal que:
      
      \[
      Q^{-1}AQ = D,
      \]
      
      con $D$ una matriz diagonal.
    \end{ndef}

    \begin{nth}
      Sea $A\in \mathscr{M}_{p}(\R)$. Supongamos que $A$ es diagonalizable por otra matriz $Q\in \mathscr{M}_{p}(\R)$ no singular, esto es
      
      \[
      Q^{-1}AQ = D,
      \]
      
      con $D$ una matriz diagonal. Denotemos $Q = (q_1,\dots,q_p)$, y $D= \operatorname{diag}(d_1,\dots,d_p)$. Entonces:
      
      \begin{nlist}
      \item $\operatorname{rango}(A) = $ número de elementos de la diagonal de $D$ que son distintos de 0,
      \item $\det(A) = \prod_{i = 1}^p d_i$,
      \item $\operatorname{tr}(A) = \sum_{i = 1}^p d_i$,
      \item el polinomio característico de $A$ es:
        \[
        p(\lambda) = (-1)^p (\lambda-d_1)\dots(\lambda-d_p),
        \]
      \item el espectro de $A$ comprende los escalares distintos incluidos en la diagonal de $D$,
      \item las multiplicidades algebraicas y geométricas de un autovalor $\lambda$ de $A$ coinciden y son iguales al número de elementos diagonales de $D$ que igualan $\lambda$,
      \item las columnas de $Q$ son autovectores linealmente independientes de $A$ (la columna $q_i$ es un autovector correspondiente al autovalor $d_i$),
    \end{nlist}
\end{nth}

\begin{ndef}
      Se dice que $A$ es \emph{diagonalizable ortogonalmente} si es diagonalizable por una matriz ortogonal, esto es, $A = QDQ^T$, con $Q^T = Q^{-1}$, $QQ^T = Q^TQ = I$.
\end{ndef}
    
\begin{ncor} \label{diag-ort-sim}
              Si $A$ es diagonalizable ortogonalmente, debe ser simétrica, pues $A^T = (QDQ^T)^T = (Q^T)^T D^T Q^T = QDQ^T = A$.
\end{ncor}

\begin{nprop}\hfill
  \begin{itemize}
  \item $A$ es definida no negativa $\iff$ los autovalores de $A$ son no negativos,
  \item $A$ es definida positiva $\iff $ los autovalores de $A$ son positivos.
  \end{itemize}

  Y, si $A$ es \textbf{simétrica}:
  \begin{itemize}
  \item $A$ es definida no negativa $\iff$ los autovalores de $A$ son no negativos,
  \item $A$ es definida positiva $\iff$ los autovalores de $A$ son positivos,
  \item $A$ es semidefinida positiva $\iff$ los autovalores de $A$ son no negativos y al menos uno es nulo.
  \end{itemize}
\end{nprop}


El concepto de raíz cuadrada de una matriz nos será útil:
\begin{nprop}
  Sea una matriz $A\in \mathscr{M}_{p}(\R)$ simétrica y definida no negativa. Entonces, existe una matriz $R\in \mathscr{M}_{p}(\R)$ simétrica y definida no negativa tal que
  
  \[
  A = R^2 = RR
  \]
  
  Además, $R$ es única y puede expresarse como:

  \[
  R = Q \operatorname{diag}\left(\sqrt{d_1}, \dots, \sqrt{d_p}\right) Q^T
  \]
  
  A $R$  se le llama raíz cuadrada de una matriz simétrica y definida no negativa. La notamos $A^{1/2}$.
\end{nprop}

\paragraph{Descomposición espectral de una matriz simétrica definida no negativa.}\hfill

Dada $A\in \mathscr{M}_{p}(\R)$ simétrica y definida no negativa, vamos a definir su \emph{descomposición espectral}.

\begin{ndef}
  Se llama \emph{descomposición espectral} de $A$ a la descomposición en la forma:
  \[
  A = H\Lambda H^T,
  \]
  toda vez que se verifique que:
  \begin{nlist}
  \item $H\in \mathscr{M}_{p}(\R)$ es ortogonal,
  \item $\Lambda\in \mathscr{M}_{p}(\R)$ es diagonal,
  \end{nlist}
  es decir, que $A$ sea diagonalizable ortogonalmente por $H$ y $\Lambda$.
\end{ndef}

Y, en efecto, el recíproco del corolario \ref{diag-ort-sim} es cierto y siempre existe tal descomposición:

\begin{nth}
  A es diagonalizable ortogonalmente si y solo si $A$ es simétrica.
\end{nth}

Sea $r \le p$ el rango de $A$. Si elegimos $H$ y $\Lambda$ de forma que $\Lambda$ tiene las filas distintas de cero
al principio:

\[
  \Lambda = 
  \begin{pmatrix}
    D & 0 \\
    0 & 0
  \end{pmatrix},
\]

con $D \in \mathscr{M}_r(\R)$, y escribimos $H = (H_1 | H_2)$, con $H_1\in \mathscr{M}_{p\times r}(\R), H_2\in \mathscr{M}_{p\times (p-r)}(\R)$,
entonces:

\[
  A = (H_1 | H_2)^T \begin{pmatrix}
    D & 0 \\
    0 & 0
  \end{pmatrix} (H_1 | H_2) = H_1 D H_1^T.
\]

\subsubsection{Matrices de intercambio de filas/columnas.}

Sea $I_{(r,\,s)}$ la matriz obtenida intercambiando en la identidad $I_p$ las filas $r$ y $s$. Dada $A\in \mathscr{M}_{p}(\R)$,

\begin{itemize}
\item $I_{(r,\,s)}\,A$ es la matriz obtenida intercambiando en $A$ las filas $r$ y $s$,
\item $AI_{(r,\,s)}$ es la matriz obtenida intercambiando en $A$ las columnas $r$ y $s$.
\end{itemize}

\begin{nprop}[Propiedades] \hfill
  \begin{nlist}
  \item Si $r\ne s$, $\det(I_{(r,\,s)}) = -1$,
  \item $I_{(r,\,s)}$ es simétrica y ortogonal.
  \end{nlist}
\end{nprop}

En la descomposición espectral de una matriz simétrica y definida no negativa, tenemos unicidad de $D$
salvo reordenación de los elementos de la diagonal y la correspondiente reordenación de las columnas de $Q$:

\[
  A = QDQ^T = (QI_{(r,\,s)})(I_{(r,\,s)}DI_{(r,\,s)})(I_{(r,\,s)}Q^T)
\]


\paragraph{Inciso - interpretación geométrica de la distancia de Mahalanobis y la densidad de la DNM.}\hfill

Ahora estamos en disposición de acometer este ejercicio:

\begin{ejer}
  Sea $\vectX = (X_1, \dots, X_p)^T \sim (\boldsymbol \mu, \Sigma)$ un vector aleatorio con $\Sigma > 0$. \begin{enumerate}
    \item Estudiar el lugar geométrico de los puntos $\Delta(\boldsymbol x, \boldsymbol \mu) = k$, e interpretarlo en funcón de los autovalores de $\Sigma$.
    \item Si además $\vectX \sim N_p(\boldsymbol \mu, \Sigma)$, estudiar el lugar geométrico de los puntos definidos por $f_{\boldsymbol X}(\boldsymbol x) = h$ —con $h \geq 0$— y dar una interpretación geométrica espectral.
  \end{enumerate}
\end{ejer}

\begin{sol}
  Comenzamos dando la solución del apartado (1). Para $k \geq 0$ consideramos \[
      \Delta(\boldsymbol x, \boldsymbol \mu) = k \iff (\boldsymbol x - \boldsymbol \mu)^T\Sigma^{-1}(\boldsymbol x - \boldsymbol \mu) = k^2 \iff (\boldsymbol x - \boldsymbol \mu)^T(k^2\Sigma)^{-1}(\boldsymbol x - \boldsymbol \mu) = 1\,.
      \] Visto así se tiene que los puntos $\boldsymbol x$ solución de esta ecuación definen el \textit{elipsoide} de centro $\boldsymbol \mu$ con ejes principales definidos por los autovectores de $(k^2\Sigma)^{-1}$, siendo la longitud al cuadrado de los semiejes igual a los inversos de los autovalores.
      Descomponiendo $\Sigma = H\Lambda H^T$, podemos escribir \begin{align*}
        (k^2\Sigma)^{-1} 
        &= (kH\Lambda H^T k)^{-1} 
        = k^{-1}(H\Lambda H^T)^{-1} 
        = k^{-1}(H^T)^{-1}\Lambda^{-1}H^{-1}k^{-1}\\
        &= (k^{-1}H)\Lambda^{-1}(k^{-1}H)^T
        % #TODO: añadir anotación HH^T = 1
        = H(k^2\Lambda)^{-1}H^T\,.
      \end{align*}

      Los autovectores vienen definidos por la matriz $H$ —columnas—, dando las direcciones de los ejes del elipsoide. Por otro lado, los autovalores de $(k^2\Sigma)^{-1}$ vienen dados por los elementos diagonales de la matriz diagonal $(k^2\Lambda)^{-1}$, es decir, los inversos de la forma $(k^2\lambda_j)^{-1}$ de los autovalores de la matriz $k^2\Sigma$.
      Finalmente, las longitudes al cuadrado de los semiejes coinciden con los propios autovalores $k^2\lambda_j$, por lo que las longitudes de los semiejes son los valores $k\lambda_j^{\sfrac{1}{2}}$.

      Procedemos ahora a ver la solución del apartado (2). La ecuación es, por definición, \[
        f_{\vectX}(\boldsymbol X) 
        = h \iff \frac{1}{(2\pi)^{\sfrac{p}{2}} |\Sigma|^{\sfrac{1}{2}}}\exp\left\{-\frac{1}{2}(\boldsymbol x - \boldsymbol \mu)^T\Sigma^{-1}(\boldsymbol x - \boldsymbol \mu)\right\} 
        = h\,.\] 
      Escribimos entonces \[
        (\boldsymbol x - \boldsymbol \mu)^T \Sigma^{-1}(\boldsymbol x - \boldsymbol \mu) 
        = - 2\ln\left(2\pi^{\sfrac{p}{2}}|\Sigma|^{\sfrac{1}{2}}h\right)\,.
          \]
      El máximo de $f_{\vectX}(\boldsymbol x)$ se alcanza en $\boldsymbol x = \boldsymbol \mu$, que llamaremos \[
        M 
        \vcentcolon= f_{\vectX}(\boldsymbol \mu) 
        = \frac{1}{(2\pi)^{\sfrac{p}{2}} |\Sigma|^{\sfrac{1}{2}}}\,.
      \] Así, la ecuación tendrá solución si $0 < h \leq M$. En ese caso, \[(2\pi)^{\sfrac{p}{2}}|\Sigma|^{\sfrac{1}{2}}\,h \leq 1\,,\] luego \[\log\left((2\pi)^{\sfrac{p}{2}}|\Sigma|^{\sfrac{1}{2}}\,h\right) \leq 0\,.\]
\end{sol}

\subsection{Distribución normal multivariante en el caso $\Sigma \ge 0$.}

Dado un vector aleatorio $\vectX = (X_1,\dots,X_p)^T$, \ref{afinnormal} y \ref{dist_univ} nos permiten deducir la siguiente caracterización de la DNM:

\begin{nprop} \label{posvar:car-iii}
  $\vectX$ tiene DNM si y solo si toda combinación lineal de las componentes $X_1,\dots,X_P$ de la forma

  \[
     \alpha^T \vectX
  \]

  con $\alpha\in \mathbb R^P -\{0\}$, tiene DN univariante no degenerada, necesariamente de media $\alpha^T \mu_{\vectX}$
  y varianza $\alpha^T \Sigma_{\vectX} \alpha$.
\end{nprop}


Hasta ahora hemos descrito tenemos las siguientes formulaciones equivalentes de DNM:
\begin{itemize}
\item \hyperref[posvar:defnormal]{[D]}: definición en términos de la densidad $f_{\vectX}$,
\item \hyperref[posvar:car-i]{[C-I]}: $\vectX = AZ + \mu$ con $A$ no singular, $\Sigma = AA^T$,
\item \hyperref[posvar:car-ii]{[C-II]}: $\Psi_{\vectX} = \exp\left\{it^T\mu - \frac{1}{2}t^T \Sigma t\right\}$, y
\item \hyperref[posvar:car-iii]{[C-III]}: $\alpha^T \vectX \sim N(\alpha^T \mu, \alpha^T \Sigma \alpha), \forall \alpha \in \mathbb R^p - \{0\}$.
\end{itemize}


Podemos usar \hyperref[posvar:car-ii]{[C-II]} para definir la DNM en el caso general de la siguiente manera:

\begin{ndef} \label{general:car-ii}
  Dado $\vectX = (X_1,\dots,X_p)^T \sim N_p(\mu,\Sigma)$ un vector aleatorio con $\Sigma \geq 0$, se dice que tiene una DNM si su función característica viene dada por:
  \[
    \Phi_{\vectX}(t) = E\left[e^{it^T \vectX}\right] = \exp\left\{it^T \mu - \frac{1}{2}t^T \Sigma t\right\} \text{ para todo } t \in \mathbb R^p.
  \]

\end{ndef}

\begin{nprop}[Transformaciones lineales de rango no necesariamente máximo]
  Dado un vector aleatorio $\vectX  = (X_1,\dots,X_p)^T \sim N_p(\mu,\Sigma)$ con $\Sigma \geq 0$. Sea $\vectY = (Y_1,\dots,Y_p)^T$ definido como $\vectY = B\vectX + b$, con $B\in \mathscr{M}_{q\times p}(\R)$ una matriz de rango $r \leq \min \{q,p\}$, y $b \in \R^q$.

  Entonces, se tiene que
  \[
    \vectY \sim N_q(B\mu+b, B\Sigma B^T).
  \]
\end{nprop}

\begin{nota}
  La única novedad que nos aporta esta nueva proposición respecto a lo que ya teníamos (\ref{linealesrangomaximo}), es que ahora $B\Sigma B^T \geq 0$ (antes teníamos que tenía que se estrictamente mayor).
La demostración es análoga a la del caso $\Sigma > 0$.
\end{nota}


Vamos a extender la \hyperref[posvar:car-i]{[C-I]} a partir de la definición que acabamos de dar para el caso general:
sea $\vectX = (X_1,\dots,X_p)^T\sim N_p(\mu,\Sigma)$, con $\Sigma \ge 0$, es decir,

\[
\Psi_{\vectX}(t) = \exp\left\{it^T\mu - \frac{1}{2}t^T\Sigma t\right\} \quad \forall t\in \mathbb R^p
\]

Descomponiendo $\Sigma = H^T\Lambda H = H_1 D H_1^T$, podemos escribir:

\[
\begin{split}
  \Psi_{\vectX}(t) = & \exp\left\{it^T\mu - \frac{1}{2}t^T\Sigma t\right\} \\
  = & \exp\left\{i^T HH^T \mu - \frac{1}{2}t^T H_1 D H_1^T t\right\} \\
  = & \exp\left\{it^T H_1 H_1^T\mu - \frac{1}{2}t^T H_1DH_1^Tt\right\} \cdot \exp\left\{itH_2H_2^T \mu\right\}
  .
\end{split}
\]

La forma de la función característica obtenida sugiere considerar el cambio de variable $\vectY = H^T\vectX$:

\[
Y = \begin{pmatrix} H_1^T \\ H_2^T \end{pmatrix} \vectX=\begin{pmatrix} H_1^T \vectX \\ H_2^T\vectX \end{pmatrix} = \begin{pmatrix} Y_1 \\ Y_2 \end{pmatrix}
\]

Con esto, se tiene:
\begin{nlist}
\item $\mu_{\vectY} = H^T \mu = \begin{pmatrix} H_1^T \mu \\ H_2^T\mu \end{pmatrix} = \begin{pmatrix} \mu_{\vectY_1} \\ \mu_{\vectY_2} \end{pmatrix}$
\item $\Sigma_{\vectY} =  H^T \Sigma H = H^T H \Delta H^T H = \Delta = \begin{pmatrix} D & 0 \\ 0 & 0 \end{pmatrix} \implies \begin{cases} \Sigma_{\vectY_1} = D \\ \Sigma_{\vectY_2} = 0\end{cases}$,
\end{nlist}

Y, por tanto:

\[
\begin{cases} \vectY_1 \sim N_r(H_1\mu,D),\, D> 0, \\ \vectY_2 = H_2\mu \quad c.s. \end{cases}
\]

Observamos, por \ref{fc:translineal}:

\[
\Psi_{\vectX}(t) = \Psi_{\vectY}(H^T t)
\]

Denotamos

\[
\nu = H^T t = \begin{pmatrix} H_1^T \\ H_2^T\end{pmatrix}t = \begin{pmatrix} H_1^Tt \\ H_2^Tt\end{pmatrix} = \begin{pmatrix} \nu_1 \\ \nu_2\end{pmatrix}
\]

Con esto:

\[
\Psi_{\vectY}(\nu) = \Psi_{\vectX}(t) = \exp\left\{ i\nu_1\mu_{\vectY_1} - \frac{1}{2} \nu_1^T \Sigma_{\vectY_1}\nu_1 \right\} \cdot \exp\left\{i\nu_2^T \mu_{\vectY_2}\right\}
\]

y en la última igualdad, el primer término es $\Psi_{\vectY_1}(\nu_1)$ y el segundo es $\Psi_{\vectY_2}(\nu_2)$, luego $\vectY_1,\vectY_2$ son independientes.

Aplicamos a $\vectY_1$ la \hyperref[posvar:car-i]{[C-I]}: tenemos que $\vectY_1$ puede representarse como

\[
\vectY_1 = D^{\frac{1}{2}}\vectZ + H_1^T \mu, \quad \vectZ \sim N_r(0,I) 
\]

Multiplicando por $H$:

\[
\vectY = \begin{pmatrix} \vectY_1 \\ \vectY_2 \end{pmatrix} \equiv \begin{pmatrix} D^{\frac{1}{2}} \vectZ + H_1\mu \\ 0\vectZ+ H_2^T \mu\end{pmatrix} = \begin{pmatrix} D^{\frac{1}{2}} \\ 0 \end{pmatrix}\vectZ + H^T \mu
\]

Vemos que:

\[
\vectX = H\vectY = H\begin{pmatrix} D^{\frac{1}{2}} \\ 0\end{pmatrix} \vectZ + HH^T\mu = H_1D^{\frac{1}{2}}\vectZ + \mu
\]

Denotando $A= H_1D^{\frac{1}{2}}$ se tiene finalmente:

\[
\vectX = A\vectZ + \mu
\]
con $A$ una matriz $p\times r$, con $r\leq p$, de rango $r$ y $\vectZ\sim N_r(0,I)$.

Recíprocamente, supongamos que se cumple lo anterior. Usando \ref{fc:translineal},

\[
\Psi_{\vectX}(t) = \Psi_{A\vectZ + \mu}(t)  = \exp\left\{it^T \mu\right\}\Psi_Z(A^Tt)
\]
como $\Psi_{\vectZ}(t) = \exp\left\{-\frac{1}{2}t^T t\right\}$, entonces:
\[
 \Psi_{\vectX}(t) = \exp\left\{-\frac{1}{2}(A^T t)^T(A^T t)\right\} \exp\left\{it^T \mu\right\} = \exp\left\{-\frac{1}{2}t^T A A^T t\right\} \exp\left\{it^T \mu\right\} 
\]
y, como tenemos:
\begin{itemize}
  \item $\mu_{\vectX} = E[\vectX]=A*0 +\mu = \mu$
  \item $\Sigma_{\vectX} = \operatorname{Cov}(\vectX) = AIA^T = AA^T$
\end{itemize}
Nos queda:
\[
 \Psi_{\vectX}(t)= e^{it\mu - \frac{1}{2}t^T(AA^T)t} \implies \vectX \sim N_p(\mu, AA^T), \quad AA^T \geq 0
\]

Con todo este procedimiento, podemos enunciar la siguiente caracterización, que es una extensión de \hyperref[posvar:car-i]{[C-I]}.
\begin{nprop}[Caracterización I]
  Un vector aleatorio $\vectX = (X_1,\dots,X_p)^T$ tiene DNM de vector de medias $\mu$ y matriz de covarianzas $\Sigma$ (con $\Sigma \geq 0$), si y solo si se puede representar:
  \[
  \vectX = A\vectZ + \mu
  \] con $A\in \mathscr{M}_{p\times r}(\R)$, de rango $r$, $AA^T = \Sigma,\, \vectZ\sim N_r(0,I)$.
\end{nprop}

Enunciamos también una nueva versión de \hyperref[posvar:car-iii]{C-III}:
\begin{nprop}[Caracterización III]
  Un vector aleatorio $\vectX = (X_1,\dots,X_p)^T$ tiene DNM (con $\Sigma \geq 0$) si y solo si toda combinación lineal de las componentes $X_1,\dots,X_p$ de la forma:
\[
  \alpha^T X, \quad \alpha^T \in \mathbb R^p
\]
tiene distribución normal (univariante, posiblemente degenerada)
\end{nprop}
\begin{proof}
Es análoga al caso anterior.
\end{proof}

\begin{ndef}
  En el subespacio afín generado por los autovectores correspondientes a autovalores no nulos, se tiene que se puede formular la función de densidad normal multivariante.
\end{ndef}

\begin{nprop}[Normalidad de C.L. de vectores con DNM independientes]
  Sean $\vectX_k$ con $k = 1,\dots,m$ v.a. $p-$dimensionales independientes con distribución $N_p\left(\mu_k, \Sigma_k\right)$ respectivamente. Entonces, para cualquier conjunto de matrices constantes $A_k$ con $k = 1,\dots, m$ de dimensión $q\times p$, se verifica que:
  \[
Y:= \sum_{k=1}^{m} A_k X_k \sim N_q\left(\sum_{k=1}^m\left(A_k \mu_k\right), \sum_{k=1}^m\left(A_k \Sigma_k A_k^T\right)\right)
  .\]
\end{nprop}
%#TODO: Demostración, es seguir de la definición

\begin{nth}[Cramer]
  Sean $\vectX_1$ y $\vectX_2$ vectores aleatorios y $p$-dimensionales independientes, con $\vectX_1 \sim N_p(\mu_1, \Sigma_1)$ y $\vectX_2 \sim N_p(\mu_2, \Sigma_2)$. Entonces, se tiene que $\vectX_1 + \vectX_2 \sim N_p(\mu_1 + \mu_2, \Sigma_1 + \Sigma_2)$.
\end{nth}

La conjetura de Paul Lévy preguntaba si el recíproco de esta afirmación es cierta. Cramer dio una solución para $p=1$ en el año 1936, de la que es sencillo inducir la solución para cualquier $p$. 

\begin{nth}[Recíproco del teorema de Cramer]
  Sean $\vectX_1$ y $\vectX_2$ vectores aleatorios y $p$-dimensionales independientes, tales que $\vectX_1 + \vectX_2$ tiene DNM. Entonces, $\vectX_1$ y $\vectX_2$ también tienen DNM.
\end{nth}

\begin{ejer}
  Suponiendo que este último resultado es cierto para $p=1$, probar que es cierto para cualquier $p$.
\end{ejer}

% ¿Es esto una sección nueva?
\subsection{Distribuciones esféricas y elípticas}

\subsubsection{Caracterización en términos de la densidad esférica estándar (C-IV)}

Vamos a comenzar razonando cómo podemos trabajar con coordenadas polares. Sea $\vectX \sim N_p(\boldsymbol \mu, \Sigma)$, con $\Sigma > 0$. Para cada $k > 0$ la ecuación \[(\boldsymbol x - \boldsymbol \mu)^T \Sigma^{-1}(\boldsymbol x - \boldsymbol \mu) = k^2\] define un elipsoide en $\mathbb R^p$ sobre el que la densidad $f_{\vectX}$ es constante. 

Sea $\Sigma = CC^T$, con $C_{p\times p}$ no singular. Definimos $\boldsymbol Z = C^{-1}(\boldsymbol X - \boldsymbol \mu)$, con $\boldsymbol Z \sim N_p(\boldsymbol 0, I_p)$, a partir del cual vamos a definir el vector aleatorio —excepto para el caso $\boldsymbol Z = 0$— \[\boldsymbol U = \frac{\boldsymbol Z}{\Vert \boldsymbol Z \Vert} = \frac{\boldsymbol Z}{(\boldsymbol Z^T \boldsymbol Z)^{1/2}}\,,\] que tiene norma 1. Observamos que $\boldsymbol U$ se distribuye sobre la esfera unidad en $\mathbb R^p$.

Definimos a continuación $H \boldsymbol U$, con $H$ matriz ortogonal. Entonces, \[
  H \boldsymbol U = H \frac{\boldsymbol Z}{\left(\boldsymbol Z^T \boldsymbol Z\right)^{1/2}} = \frac{H\boldsymbol Z}{\left(\boldsymbol Z^T H^T H\boldsymbol Z\right)^{1/2}} = \frac{H\boldsymbol Z}{\Big((H\boldsymbol Z)^T (H\boldsymbol Z)\Big)^{1/2}}\,.
\] Observamos que $H \boldsymbol Z \sim N_p(H \boldsymbol 0,\, H I_pH^T) = N_p(\boldsymbol 0,\, I_p)$. Por tanto tenemos que $\boldsymbol U$ y $H \boldsymbol U$ se distribuyen de igual forma. Ahora, como la distribución de $\boldsymbol U$ no se ve afectada bajo ninguna isometría —multiplicando por una matriz ortogonal—, concluimos que $\boldsymbol U$ se distribuye uniformemente sobre la esfera unidad en $\mathbb R^p$, lo que llamamos \textit{densidad esférica estándar}. Para recuperar la información de $\boldsymbol Z$ a partir de $\boldsymbol U$ necesitamos conocer el «radio» $R = (\boldsymbol Z^T \boldsymbol Z)^{1/2} = \Vert Z \Vert$.

\begin{nprop}
  El vector aleatorio $\boldsymbol U = \frac{\boldsymbol Z}{(\boldsymbol Z^T \boldsymbol Z)^{1/2}}$ y la variable aleatoria $R = (\boldsymbol Z^T \boldsymbol Z)^{1/2}$ son independientes.
\end{nprop}

% 23-10-19

\begin{nprop}
  Un vector aleatorio $\vectX = (X_1, \dots, X_n)$ tiene DNM $N_p(\boldsymbol \mu, \Sigma)$ con $\Sigma > 0$ si, y solo si, $\vectX$ es un vector aleatorio correspondiente al siguiente «experimento»:

  \begin{enumerate}
    \item Obtener el valor de una variable aleatoria $R^2 \vcentcolon= V \sim \chi_p^2$. Se identifica $V = v$.
    \item Elegir un punto aleatoriamente —es decir, según la distribución uniforme— en la esfera $p$-dimensional de radio $r = \sqrt{v}$. \[\vectZ = (Z_1, \dots, Z_p)^T = (z_1, \dots, z_p)^T = \boldsymbol z\,.\]
    \item Dado el vector de medias $\boldsymbol \mu$ y la matriz de covarianzas $\Sigma > 0$, elegir $C$ (no singular) tal que $\Sigma = CC^T$ y obtener el valor de $\vectX$ mediante la expresión $X\vcentcolon= C\vectZ + \boldsymbol \mu$. De nuevo se identifica $\vectX = \boldsymbol x = C\boldsymbol z + \boldsymbol \mu$.
  \end{enumerate}
\end{nprop}

\begin{proof}
  Veamos la justificación de la implicación ($\implies$). Por la caracterización (C-I) sabemos que podemos escribir $\vectX = C\vectZ + \boldsymbol \mu$, con $\vectZ \sim N_p(\boldsymbol 0, I_p)$.

  Escribimos, para $\vectZ \neq 0$, \[\vectZ = \left(\vectZ^T\vectZ\right)^{\sfrac{1}{2}} \cdot \frac{\vectZ}{\left(\vectZ^T\vectZ\right)^{\sfrac{1}{2}}} = R \cdot \boldsymbol U\,,\] siendo $R$ y $\boldsymbol U$ independientes.

  Ahora tenemos que: \begin{itemize}
    \item \[R^2 = \vectZ^T\vectZ = Z_1^2 + \cdots + Z_p^2 \sim \chi_p^2\]al ser $Z_1, \dots, Z_p$ independientes con $Z_i \sim N_p(0, 1)$ para $i = 1, \dots, p$,
    \item $\boldsymbol U$ tiene distribución uniforme sobre la esfera unidad.
  \end{itemize}

  A partir de la expresión $\vectX = C \cdot R \cdot \boldsymbol U + \boldsymbol \mu$ se tiene que para generar los valores de $\vectX$ se puede seguir el procedimiento del enunciado.
\end{proof}

\subsection{Distribuciones esféricas y elípticas}

\subsubsection{Distribuciones esféricas}

\begin{ndef}
  Se dice que un vector aleatorio $\vectX = (X_1, \dots, X_p)^T$ tiene una distribución \textit{esférica} si para toda matriz ortogonal $H$ los vectores aleatorios $\vectX$ y $H\vectX$ tienen la misma distribución.
\end{ndef}

Veamos a continuación dos caracterizaciones de las distribuciones esféricas.

\begin{nprop}[Caracterización en términos de la densidad]
  Dado un $\vectX$ un vector aleatorio $p$-dimensional con función de densidad $f_{\vectX}$ tiene distribución esférica si y solo si $f_{\vectX}$ puede expresarse como $g(\boldsymbol x^T \boldsymbol x)$ para alguna función escalar $g : \mathbb R_0^+ \to \mathbb R_0^+$.
\end{nprop}

\begin{nprop}[Caracterización en términos de la función característica]
  Dado un $\vectX$ un vector aleatorio $p$-dimensional con función de densidad $f_{\vectX}$ tiene distribución esférica si y solo si su función característica tiene la forma $\xi(\boldsymbol t^T \boldsymbol t)$ para alguna función escalar $\xi: \mathbb R_0^+ \to \mathbb R_0^+$.
\end{nprop}

\subsubsection{Distribuciones elípticas}

\begin{ndef}[Distribución elíptica]
  Se dice que un vector aleatorio $\vectX = (X_1, \dots, X_p)^T$ tiene \textit{distribución elíptica} de parámetros $\boldsymbol \mu\in\R^p$ y $V\in\mathscr{M}_p(\R)$ —simétrica y definida positiva— si se puede expresar de la forma $\vectX = A\vectZ + \boldsymbol \mu$, con $V = AA^T$ y $\vectZ$ un vector aleatorio con distribución esférica. Se denota como $\vectX \sim E_p(\boldsymbol \mu, V)$.
\end{ndef}

\begin{nprop}[Caracterización en función de la densidad]
  Dado un $\vectX$ un vector aleatorio $p$-dimensional con función de densidad $f_{\vectX}$ tiene distribución elítica si y solo si $f_{\vectX}$ puede expresarse como \[|V|^{-1/2}g\left((\boldsymbol x - \boldsymbol \mu)V^{-1}(\boldsymbol x - \boldsymbol \mu)\right)\] para alguna función escalar $g : \mathbb R_0^+ \to \mathbb R_0^+$.
\end{nprop}

\begin{nprop}[Caracterización en términos de la función característica]
  Dado un $\vectX$ un vector aleatorio $p$-dimensional con función de densidad $f_{\vectX}$ tiene distribución esférica si y solo si su función característica tiene la forma \[e^{i \boldsymbol t^T \boldsymbol \mu}\xi(\boldsymbol t^T V \boldsymbol t)\] para alguna función escalar $\xi: \mathbb R_0^+ \to \mathbb R_0^+$.
\end{nprop}

% TODO: Falta observaciones sobre la distribución elíptica
% TODO: Falta lo siguiente a esto

\begin{nprop}
  Sea $X \sim E_p(\boldsymbol \mu, V)$, con $V$ diagonal. Si $X_1, \dots, X_p$ son mutuamente independientes entonces $X$ tiene distribución normal multivariante.
\end{nprop}

\begin{nprop}
  Sea $\vectZ$ un vector aleatorio con distribución esférica tal que $P[\vectZ = 0] = 0$. Entonces $U = \vectZ/\Vert\vectZ\Vert$ y $R = \Vert\vectZ\Vert$ son independientes.
\end{nprop}

\begin{proof}
  Comenzamos viendo que dada una matriz $H$ ortogonal cualquiera,
  \begin{align*}
    H\boldsymbol U = \frac{H\vectZ}{(\vectZ^T\vectZ)^{1/2}}
      = \frac{H\vectZ}{(\vectZ^TH^TH\vectZ)^{1/2}}
      = \frac{H\vectZ}{((H\vectZ)^T(H\vectZ))^{1/2}}
      = \frac{\vectZ}{(\vectZ^T\vectZ)^{1/2}}
      = \boldsymbol U,
  \end{align*} luego $\boldsymbol U$ se distribuye uniformemente en la esfera unidad en $\mathbb R^p$.

  A continuación probamos la independencia de $\boldsymbol U$ y $R$. Para cada $C \in \mathcal B(\mathbb R^+)$ tal que $P[R \in C] \neq 0$ queremos probar que $\mu = P[\boldsymbol U \in B \mid R \in C] = P[\boldsymbol U \in B]$ para todo $B \in \mathcal B(S_p)$.
  Por definición, \begin{align*}
    P[\boldsymbol U \in B \mid R \in C] 
    = \frac{P[(\boldsymbol U \in B) \land (R \in C)]}{P[R \in C]} 
    = \frac{P[\vectZ \in B \cdot C]}{P[R \in C]},
  \end{align*} 
  donde $B \cdot C = \left\{\boldsymbol z \in \mathbb R^p - \{\boldsymbol 0\} : \boldsymbol z/\Vert \boldsymbol z \Vert \in B, \Vert \boldsymbol z \Vert \in C\right\}$. 
  Luego, \begin{align*}
    \frac{P[\vectZ \in B \cdot C]}{P[R \in C]} = \frac{P[H^T\vectZ \in B \cdot C]}{P[R \in C]}
  \end{align*} para cualquier matriz $H$ ortogonal por ser la distribución de $\vectZ$ esférica. 
  Vemos que $H^T\vectZ \in B \cdot C$ si y solo si $\vectZ \in (H(B)) \cdot C$. En efecto, $H^T\vectZ \in B \cdot C$ si y solo si $H(H^T\vectZ) \in H(B \cdot C)$, siendo \begin{align*}
    H(B\cdot C) &= \left\{H \boldsymbol z : \boldsymbol z \in B \cdot C\right\}\\
      &= \left\{H \boldsymbol z : \frac{\boldsymbol z}{\Vert \boldsymbol z \Vert} \in B, \Vert \boldsymbol z \Vert \in C\right\} \\
      &= \left\{H \boldsymbol z : \frac{H \boldsymbol z}{\Vert H \boldsymbol z\Vert} \in HB, \Vert H\boldsymbol z\Vert \in C\right\}\\
      \shortintertext{y llamando $\boldsymbol w = H\boldsymbol z$,}
      &= \left\{\boldsymbol w : \frac{\boldsymbol w}{\Vert \boldsymbol w\Vert} \in HB, \Vert \boldsymbol w\Vert \in C\right\}\\
      &= H(B) \cdot C,
  \end{align*} probando entonces esta última afirmación.
  Concluimos entonces que \begin{align*}
    \frac{P[H^T\vectZ \in B \cdot C]}{P[R \in C]} &= \frac{P[\vectZ \in (H(B)) \cdot C]}{P[R \in C]}\\
      &= \frac{P[(\boldsymbol U \in H(B)) \land (R \in C)]}{P[R \in C]} \\
      &= P[\boldsymbol U \in H(B) \mid R \in C] \\
      &= \mu(H(B)).
  \end{align*}
\end{proof}

\subsection{Formas cuadráticas basadas en vectores aleatorios normales}

Los resultados presentados en esta subsección nos serán de utilidad cuando nos adentremos en la inferencia en la sección siguiente.

El resultado que motiva esta subsección es el siguiente. Sea $\vectX = (X_1, \dots, X_p)^T$ un vector aleatorio y sea $A$ una matriz constante $p \times p$. Consideremos la forma cuadrática $\vectX^T A \vectX$ —que es un vector aleatorio—. En particular, si $\vectX \sim N_p(\boldsymbol \mu, \Sigma)$ bajo ciertas condiciones sobre $A$ se tendrá que la distribución de $\vectX^TA\vectX$ es una distribución $\chi^2$ no centrada.


\subsection{Distribuciones $\chi^2$ y $F$ no centradas}
\subsubsection{Distribución $\chi^2$ no centrada}

\begin{ndef}[Distribución $\chi^2$ centrada]
  La distribución $\chi^2$ centrada se define como la distribución de la suma de cuadrados de variables aleatorias independientes con distribución normal estándar $N(0,1)$:
  \begin{align}
    Z = (Z_1, \dots, Z_n)^T \sim N_n(0,I_n)
    \mapsto Y = \sum_{i = 1}^n Z_i^2 \sim \chi^2_n, \quad ||Z||^2 = Z Z.
\end{align}
  Tenemos entonces que su función de densidad es:
  \[
    f_{\vectY}(y) = \frac{1}{2^{n/2} \Gamma(n/2)} y^{(n/2) - 2} e^{-y/2}, \quad y > 0
  \]
  y su función de distribución es:
  \[
    F_{\vectY}(y) = \frac{\gamma(n/2, y/2)}{\Gamma(n/2)}, \qquad y > 0.
  \]
  La función gamma está definida como:
  \[
    \Gamma(z) = \int_0^\infty t^{z-1} e^{-t} \mathop{}\!\mathrm{d}t, \qquad \text{para todo } z \in \mathbb C \text{ tal que } \operatorname{Re}(z)>0.
  \]
  Las funciones gamma incompletas son:
  \begin{itemize}
    \item $\Gamma(z,v) = \int_v ^\infty t^{z-1} e^{-t} \mathop{}\!\mathrm{d}t$.
    \item $\gamma(0,v) = \int_0^v t^{z-1} e^{-t} \mathop{}\!\mathrm{d}t$ con $v>0$.
  \end{itemize} 
\end{ndef}

\begin{ndef}[Distribución $\chi^2$ no centrada]
  Sea $\vectX = (X_1,\dots,X_n)^T\sim N_n(\mu,I_n)$. Entonces, la v.a. $\vectY = \vectX \vectX^T$ tiene la función de densidad:
  \[
f_{\vectY}(y) = e^{\frac{\delta}{2}} {}_0 F_1(\frac{1}{2}n; \frac{1}{4}\delta y) \frac{1}{2^{\frac{n}{2}} \Gamma(\frac{n}{2})} e^{-\frac{y}{2}} y^{\frac{n}{2} - 1}, \quad y > 0
\]
siendo $\delta = \mu^T \mu$. Se dice que $\vectY$ tiene distribución $\chi^2$ no centrada con $n$ grados de libertad y parámetro de no centralidad $\delta$, denotándose $\chi_n^2(\delta)$
\end{ndef}

\begin{nota}
${}_0F_1$ es la función hipergeométrica generalizada de órdenes 0 y 1, también llamada función hipergeométrica confluente límite.
\end{nota}

\begin{nprop}[Momentos de primer y segundo orden]
  \begin{itemize}
  \item $E[\vectY] = n+\delta$
  \item $Var(\vectY) = 2n + 4\delta$
    \end{itemize}
\end{nprop}
\begin{nota}
$\Psi_{\vectY} ^{(k)}(0) = \frac{d^k \Psi_{\vectY}(t)}{dt^k}|_{t= 0} = i^k E[\vectY^k]$
\end{nota}

\begin{nprop}
  Si $Z_1$ y $Z_2$ son dos v.a. independientes con
  \begin{align}Z_1 \sim \chi_{n_1}^2(\delta_1) \\
    Z_2 \sim \chi_{n_2}^2 (\delta_2)
  \end{align}
  entonces, se tiene que:
  \[
 Z_1 + Z_2 \sim \chi_{n_1+n_2}^2 (\delta_1 + \delta_2)
  \]
\end{nprop}

\subsubsection{Funciones hipergométricas generalizadas}

\begin{ndef}[Función hipergeométrica generalizada]
 
  Se denomina función (serie) hipergeométrica generalizada de órdenes p y q a
  \[
{}_pF_q(a_1,\dots,a_p;b_1,\dots,b_q:z) = \sum_{k = 0}^\infty \frac{(a_1)_k \dots (a_p)_k}{(b_1)_k \dots (b_q)_k} \frac{z^k}{k!}
\]
donde $(a)_k = a(a+1)\dots(a+k-1)$ con $a_1,\dots,b_1,\dots,b_q$ parámetros (posiblemente complejos) y $z\in \mathbb C$ el argumento
\end{ndef}

\begin{nota}
  \begin{itemize}
  \item Ningún parámetro $b_j$ puede ser cero o entero negativo
  \item Si algún parámetro en el numerador es cero o un entero negativo, los términos de la serie se anulan a partir de un $k$ y queda un polinomio en $z$
  \item La serie converge para todo $z$ si $p \leq q$, converge para $|z| < 1$ y diverte para $|z|> $ si $p = q+1$ y diverge para todo $z\ne 0$ si $p>q+1$

  \item Caso particular: ${}_2F_1$ es la función hipergeométrica clásica o gaussiana
    \item Caso particular: ${}_1F_1$ es la función hipergeométrica confluente
  \end{itemize}
  \item ${}_0F_1$ se define como ${}_0F_1(;b;z) = \lim_{a\to \infty} {}_1F_1(a;b;z)$
\end{nota}


Función característica de la distribución $\chi_n^2(\delta)$:
\[
\Psi_{\chi_n^2(\delta)}(t) = (1-2it)^{-\frac{n}{2}} e^{\frac{it\delta}{1-2it}} \quad \forall t \in \mathbb R
\]
$\vectX \sim N(\mu,\sigma^2) \to \Psi_{\vectX^2}(t) = \frac{1}{(1-2i\sigma^2 t)^{\frac{1}{2}}} e^{\frac{it\mu^2}{1-2i\sigma^2 t}}$




\subsubsection{Distribución \texorpdfstring{$\boldsymbol F$}{F} de Snedecor no centrada}

\begin{ndef}[Distribución $F$ de Snedecor centrada]
  Si $\vectY_1\sim \chi_{n_1}$,$\vectY_2 \sim \chi_{n_2}$ centradas e independientes,la distribución $F$ centrada se define como la distribución del cociente:
  \[
  F = \frac{\vectY_1 / n_1}{\vectY_2 / n_2}
  \]
  Se denota $F_{n_1,n_2}$ o $F(n_1,n_2)$
\end{ndef}
También puede verse como:
\[
F = \frac{\sum_{k=1}^{n_1} Z_{1k}^2}{\sum_{l=1}^{n_2} Z_{2l}^2}
\]
con
\begin{itemize}
\item $Z_1 = (Z_{11},\dots, Z_{1n})^T \sim N_{n_1}(0,I_{n_1})$
\item $Z_2 = (Z_{21},\dots, Z_{2n})^T \sim N_{n_2}(0,I_{n_2})$
  \item $Z_1,Z_2$ independientes
\end{itemize}
o , equivalentemente:
\[
Z = \begin{bmatrix}Z_1 \\ Z_2 \end{bmatrix} \sim N_{n_1+n_2}(0,I_{n_1+n_2})
\]

  Su función de densidad viene dada por
  \[
  g_F(f) = \frac{1}{f \ B(\frac{n_1}{2}, \frac{n_2}{2})} (\frac{n_1 f}{n_1 f + n_2})^{\frac{n_1}{2}} (1- \frac{n_1f}{n_1f + n_2})^{\frac{n_2}{2}}
  \]
  para $f \geq 0$ con $n_1,n_2 \in \mathbb N -\{0\}$.\\

  La función de distribución viene dada por:
  \[
G_F(f) = I_{\frac{n_1f}{n_1f + n2}}(\frac{n_1}{2},\frac{n_2}{2})
\]

Donde esta $I$ es la función beta incompleta regularizada, que definimos ahora.

\begin{ndef}[Función Beta]
  La función beta viene dada por:
  \[
B(x,y) = \frac{\Gamma(x) \Gamma(y)}{\Gamma(x+y)} = \int_0^1 t^{x-1} (1-t)^{y-1} dt
  \]
  donde $x,y \in \mathbb C$, $Re(x),Re(y) > 0$.
\end{ndef}
\begin{ndef}[Función Beta Regularizada]
  Esta función viene dada por:
  \[
B(x;a,b) = \int_0^x t^{a-1}(a-t)^{b-1} dt
\]
con $x \in [0,1]$, $a,b\in \mathbb C$, $Re(a),Re(b) > 0$.
\end{ndef}
Con estas dos funciones, definimos:
\begin{ndef}[Función beta incompleta regularizada]
  \[
I_X(a,b) = \frac{B(x;a,b)}{B(a,b)}
  \]
\end{ndef}

\begin{nprop}[Función característica de $F$]
  La función característica viene dada por:

  \[
\Psi_{F_{n_1,n_2}}(t) = \frac{\Gamma(\frac{n_1+n_2}{2})}{\Gamma(\frac{n_2}{2})} U(\frac{n_1}{2},1-\frac{n_2}{2}; -\frac{n_2}{n_2}it)
\]
donde $U$ es la función hipergeométrica confluente de segunda especie:
\[
U(a,b;z) = \frac{\Gamma(1-b)}{\Gamma(a+1-b)}{}_1F_1(a,b;z) + \frac{\Gamma(b-1)}{\Gamma(a)}z^{1-b}{}_1F_1(a+1-b,2-b;z)
\]
\end{nprop}

\begin{nprop}[Momentos de primer y segundo orden]
  \begin{itemize}
  \item $E[F] = \frac{n_2}{n_2-2}$ para $n_2 > 2$
  \item $\operatorname{Var}(F) = \frac{2 n_2^2(n_1+n_2 - 2)}{n_1(n_2-2)^2 (n_2-4)}$ para $n_2 > 4$.

    Si $n_2\in [0,2]$ no está definida. Para $n_2 \in (2,4]$, la varianza es infinita.
  \end{itemize}
\end{nprop}


\begin{ndef}
  Sea $\vectY_1 \sim \chi_{n_1}^2(\delta)$ e $\vectY_2 \sim \chi_{n_2}^2$ independientes.Entonces, la variable
  \[
F = \frac{Y_1 / n_1}{Y_2 / n_2}
\]
tiene función de densidad:
\[
g_F(t) = e^{-\frac{\delta}{2}} {}_1F_1(\frac{1}{2}(n_1+n_2); \frac{1}{2}n_1; \frac{\frac{1}{2} \frac{n_1}{n_2}}{}
\]
\end{ndef}


\subsubsection{Resultados sobre formas cuadráticas}

Sea $\vectX^T A \vectX$ una forma cuadrática con $\vectX \sim N_p(\mu,\Sigma)$. Podemos hacer un estudio directo sobre $E[\vectX^T A \vectX]$ y $Var(\vectX^T A \vectX)$.
\begin{nprop}
  Sea $\vectX \sim N(\mu,\Sigma)$, con $\Sigma > 0$ y $A$ una matriz $p\times p$ simétrica. Entonces:
  \begin{enumerate}
  \item $E[\vectX^T A \vectX] = tr(A\Sigma) + \mu^T A \mu$
    \item $Var(\vectX^T A \vectX) = 2tr((A\Sigma)^2) + 4\mu^T A \Sigma A \mu$
  \end{enumerate}

\end{nprop}
\begin{proof}
  Vamos a probar el primer elemento de los dos, pues puede que su demostración nos sera útil más adelante.\\

  \[
    E[\vectX^T A \vectX] = E[tr(\vectX^T A \vectX)] = E[tr(A\vectX \vectX^T)] = tr(E[A \vectX \vectX^T]) = tr(A \ E[\vectX \vectX^T]) = (*)
  \]
  Donde en la primera igualdad hemso usado que $\vectX^T A \vectX$ es $D\times 1$ y en la segunda hemos usado una propiedad de las matrices que dice que si $C_{m\times n}$, $B_{n\times m}$, entonces $tr(BC) = tr(CB)$. En las siguientes, hemos usado la linealidad de la esperanza. Ahora como sabemos que:
  \[
  \Sigma = E[(\vectX- \mu)(\vectX - \mu)^T] = \dots = E[\vectX \vectX^T] - \mu \mu^T
  \]
  Y usando esto en la igualdad anterior, tenemos:

  \begin{align}
    (*) = tr(A [ E \mu\mu^T]) = tr(A\Sigma + A \mu\mu^T) = tr(A\Sigma) + tr(A\mu\mu^T) = \\
    = tr(A\Sigma) + tr(\mu^T A \mu) = tr(A\Sigma) + \mu^T A \mu
  \end{align}
\end{proof}

Llegamos pues al siguiente teorema:
\begin{nth}
  Sea $\vectX \sim N_p (\mu,\Sigma)$ con $\Sigma > 0$. Entonces:
  \begin{enumerate}
  \item $(\vectX - \mu)^T \Sigma^{-1} (\vectX-\mu) \sim \chi^2_p$
    \item $\vectX^T \Sigma^{-1}\vectX \sim \chi^2_P (\delta)$ con $\delta = \mu^T \Sigma^{-1} \mu$
  \end{enumerate}
\end{nth}
\begin{proof}
  \begin{enumerate}
  \item Hacemos el cambio $\vectY = C^{-1}(\vectX - \mu)$ con $\Sigma = CC^T$. Tenemos entonces que $\vectY \sim N_p(0,I_p)$. Por tanto, podemos escribir:
    \[
    (\vectX - \mu)^T \Sigma^{-1} (\vectX - \mu ) = (\vectX - \mu)^T(CC^T)^{-1} (\vectX - \mu) = \dots = (C^{-1}(\vectX - \mu))^T(C^{-1}(\vectX - \mu)) = \vectY \vectY^T \sim \chi^p_p
    \]

  \item En este caso, tomamos el cambio $V  =C^{-1}\vectX$ con $\Sigma = CC^T$. Entonces, se tiene que
    \[
    V \sim N_p(C^{-1}\mu, C^{-1}\Sigma(C^{-1})^T) \equiv N_p(C^{-1}\mu, C^{-1}CC^T(C^{-1})^T) \equiv N_p(C^{-1}\mu, I_p)
    \]
    Y por tanto, escribimos:
    \[
    \vectX \Sigma^{1} \vectX = \vectX^T (CC^T)^{-1} = \dots = (C^{-1} \vectX)^T(C^{-1}\vectX) = V^T V \sim \chi_p^2(\mu^T(C^{-1})^T(C^{-1}\mu)) \equiv \chi_p^t(\mu^T \Sigma \mu)
    \]

  \end{enumerate}
\end{proof}

\begin{nth}
  Sea $\vectX \sim N_p(\mu, I_p)$ y sea $B_{p\times p}$ simétrica. Entonces, $\vectX^T B \vectX$ tiene una distribución $\chi^2$ no centrada si y solo si $B$ es idempotente ( i.e. , $B^2 = B$), en cuyo caso los grados de libertad y el parámetro de no centralidad son, respectivamente
  \begin{align}
    k = \operatorname{rango}(B) = \operatorname{tr}(B)\\
    \delta = \mu^T B \mu
  \end{align}
\end{nth}
\begin{proof}
  \boxed{\implies} Supongamos que $\vectX^T B \vectX \sim X_k(\delta)$. Sea $r=\operatorname{rango}(B)$. Como $B$ es simétrica, se puede descomponer como
  \[
  H^T B H = D \mapsto B = HDH^T
  \]
  con $H_{p\times p}$ ortogonal y $D_{p\times p}$ diagonal, que tiene en las $r$ rpimeras filas, los $\lambda_i \ne 0$ con $i = 1\dots r$. Hacemos entonces el cambio $V = H^T \vectX$. Entonces:
  \[
  \nu \sim N_p(H^T \mu,H^TI_p H) \equiv N_p(H^T \mu,I_p) \equiv N_p(D,I_p)
  \]
  donde hemos denotado $\nu = H^T\mu$. Podemos escribir:
  \[
  \vectX^T B \vectX = \vectX^T H D H^T \vectX = (H^T \vectX)^T D (H^T \vectX) = V^T D V =  \sum_{j = 0}^r \lambda_j \nu_j^2
  \]
  donde $\nu_j^2\sim \chi_1^2(\nu_j^2)$ y siendo $\nu_1^2,\dots,\nu_r^2$ independientes. La función característica de $\vectX^T B \vectX$ se obtiene como
  \[
\Psi_{\vectX^T B \vectX}(t) = \Psi_{\sum_{j=1}^r \lambda_j \nu_j^2}(t) = \prod_{j=1}\Psi_{\lambda_j \nu_j^2}(t) = \prod_j \Psi_{\nu_j^2}(\lambda_j t)
\]
y, como cada función característica individual $\Psi_{\nu_j^2}(\cdot)$ tiene la forma
\[
\Psi_{\chi^2_1}(s) = (1-2is)^{-\frac{1}{2}} \exp \left\{ \frac{is\delta}{1-2is}\right\}, \quad \delta = \nu_j^2
\]
luego, conjuntamente, se tiene: \begin{align*}
  \Psi_{\vectX^T B \vectX}(t) &= \prod_{j = 1}^r (1-2i \lambda_jt)^{-\frac{1}{2}} \exp\left\{ \frac{i\lambda_j t \nu^2_j}{1-2i\lambda_jt}\right\}\\
    &= \exp\left\{it \sum_{j = 1}^r \frac{\lambda_j \nu_j^2}{1-2i\lambda_j t}\right\}\prod_{j=1}^r(1-2i\lambda_j t)^{\frac{1}{2}}, \quad \text{para todo } t \in \mathbb R.
\end{align*}

Por otra parte, por la hipótesis $\vectX^T B \vectX \sim \chi_k^2(\delta)$, con función característica:
\[
\Psi_{\vectX ^T B \vectX}(t) =  \exp \left\{ \frac{it\delta}{1-2it}\right\}(1-2it)^{-\frac{k}{2}}, \quad \text{para todo } t \in \mathbb R.
\]

Así que comparando ambas expresiones, puesto que deben ser iguales para todo $t \in \mathbb R$, debe tenerse que:
\begin{itemize}
\item $\lambda_j = 1$ con $ j = 1,\dots,r$
\item $\delta = \sum_{j = 1}^r \lambda_j \nu_j^2 = \sum_{j = 1}^r \nu_j^2 = \nu^T D \nu = (\mu^TH)DH^T\mu = \mu^THDH^T\mu$
  \item $r = k$
\end{itemize}
En consecuencia, podemos escribir:
\[
H^T B H = D = \begin{pmatrix} I_k & 0 \\ 0 & 0 \end{pmatrix}
\]
pues es una matriz idempotente, es decir:
\[
(H^T B H) (H^T B H) = H^T B H,
\]
pero multiplicando a la izquierda por $H$ y a la derecha por $H^T$ (con $H^TH = HH^T = I$), se tiene que la identidad anterior es:
\[
H^T B^2 H = H^T B H \implies B^2  = B.
\]

\boxed{\impliedby} Supongamos ahora que $B$ es idempotente. Se puede escribir:
\[
H^T B H = \begin{pmatrix} I_k & 0 \\ 0 & 0\end{pmatrix}
\]
para cierta matriz ortogonal $H^T$. Recordamos que una matriz simétrica es idempotente si y solo si sus autovalores son ceros o unos. Ahora, definimos  $V = H^T \vectX$. Se tiene que:
\[
V \sim N_p(H^T \mu, H^T I_p H) \equiv N_p(H^T \mu, I_p)
\]
por lo que
\[
\vectX^T B \vectX = \vectX^T H  \begin{pmatrix} I_k & 0 \\ 0 & 0\end{pmatrix} H^T \vectX = V^T  \begin{pmatrix} I_k & 0 \\ 0 & 0\end{pmatrix} V = \sum_{j = 1}^k V_j^2
\]
    y , puesto que $V_j$, con $j = 1,\dots,k$ son independientes, se tiene que
    \[
    \vectX^T B \vectX \sim \chi_k^2(\delta)
    \]
    con
    \begin{align}
      \delta = \sum_{j = 1} (E[V_j])^2 = E[V^T] \begin{pmatrix} I_k & 0 \\ 0 & 0\end{pmatrix} E[V] = \\
  = E[\vectX^T H]\begin{pmatrix} I_k & 0 \\ 0 & 0\end{pmatrix} E[H^T \vectX] = \\
          = E[\vectX^T] H \begin{pmatrix} I_k & 0 \\ 0 & 0\end{pmatrix} H^T E[\vectX] = \mu^T B \mu
     \end{align}
    

\end{proof}

\begin{nth}
  Sea $\vectX \sim N_p(\mu,\Sigma)$ con $\Sigma > 0$. Supongamos el particionamiento:
  
 \[
   \vectX = \begin{pmatrix} \vectX_1 \\ \vectX_2 \end{pmatrix} \ \ ; \ \ \mu = \begin{pmatrix} \mu_1 \\ \mu_2 \end{pmatrix} \ \ ; \ \ \Sigma = \begin{pmatrix} \Sigma_{11} & \Sigma_{12} \\ \Sigma_{21} & \Sigma_{22} \end{pmatrix}
  \]
  
  con $\vectX_1$ y $\mu_1$ son $q\times 1$, y $\Sigma_{11}$ es $q\times q$. Entonces:
  \[
  Q = (\vectX- \mu)^T\Sigma^{-1}(\vectX - \mu) - (\vectX_1-\mu_1)^T \Sigma_{11}^{-1}(\vectX_1-\mu_1) \sim \chi^2_{p-q}
  \]
  \end{nth}
\begin{nota}
Como vemos, no se está exigiendo ninguna independencia entre $\vectX_1$ y $\vectX_2$.
\end{nota}
\begin{proof}
  Sea $C_{p\times p}$ no singular tal que $\Sigma = CC^T$. Podemos escribir $C = \begin{pmatrix}C_1 \\ C_2 \end{pmatrix}$ con $C_1 q\times p$, $C_2 (p-q)\times p$. Dado que
  \[
  CC^T = \begin{pmatrix} C_1 \\ C_2 \end{pmatrix}(C_1^T C_2^T) = \begin{pmatrix} C_1C_1^T & C_1C_2^T \\ C_2C_1^T & C_2C_2^T\end{pmatrix}
    \]
    identificamos $\Sigma_{11} = C_1C_1^T$ y hacemos el cambio $U = C^{-1}(\vectX - \mu)$, que es lo mismo que $\vectX = CU + \mu$. Así: $X_1 = C_1U + \mu_1$.

    Sabemos también que $U \sim N_p(0,I_p)$, por lo que:
    \[
    Q = U^T U - U^TC_1^T(C_1C_1^T)^{-1}C_1U = U^T [I - C_1^T(C_1C_1^T)^{-1}C_1]U
    \]
    Ahora, aplicando el teorema anterior al vector $U$ y la matriz $B = I - C_1^t(C_1C_1^T)^{-1}C_1$, tenemos que $U^TBU$ tiene una distribución $\chi^2$ no centrada ( con parámetro de no centralidad $\delta = 0^TB0=0$, es decir, centrado), si y solo si $B$ es idempotente.
    \[
    BB = [I - C_1^T(C_1 C_1^T)^{-1}C_1][I - C_1^T(C_1 C_1^T)^{-1}C_1] = I - 2C_1^T(C_1C_1^T)^{-1}C_1 + C_1^T(C_1C_1^T)^{-1}(C_1C_1^T)(C_1C_1^T)^{-1}C_1 =  I - 2C_1^T(C_1C_1^T)^{-1}C_1 + C_1^T( C_1C_1^T)^{-1}C_1 =  I - C_1^T(C_1C_1^T)^{-1} = B
    \]
    Por lo que $B$ es idempotente. Ahora, $rango(B) = tr(B) = tr(I_p - C_1^T(C_1C_1^T)^{-1}C_1)$ y vamos que:
    \[
    tr(I_p) - tr(C_1^T(C_1C_1^T)C_1) = tr(I_p) - tr((C_1C_1^T)^{-1}C_1C_1^T) = tr(I_p) - tr(I_q) = p-q
    \]
    
    Por lo que $Q \sim \chi_{p - q}^2$.
\end{proof}

\begin{nth}
  Sea $\vectX \sim N_p(\mu,\Sigma)$, con $\Sigma > 0$ y $B_{p\times p}$ simétrica. Entonces, $\vectX^T B \vectX$ tiene una distribución $\chi_k^2(\delta)$ donde $k = rango(B)$ y $\delta = \mu^T B \mu$ si y solo si $B\Sigma$ es idempotente.
  \begin{nota}
    $B\Sigma B\Sigma = B\Sigma \iff B\Sigma B = B$
  \end{nota}
\end{nth}
\begin{proof}
  Sea $C_{p\times p }$ no singular tal que $\Sigma = C C^T$ y sea $Y = C^T \vectX$. Se tiene que
  \[
  \vectY \sim N_p(C^{-1}\mu, C^{-1}(CC^T)(C^{-1})^T) \equiv N_p(C^{-1}\mu, I_p)
  \]
  por lo que
  \[
  \vectX^T B \vectX = (C\vectY)^TB(C\vectY) = \vectY^T C^T B C \vectY
  \]
  Aplicando un teorema anterior, tenemos que $\vectX^T B \vectX$ tiene una distribución $\chi^2_k$ no centrada si y solo si la matriz $C^T B C$ es idempotente. La prueba se reduce a probar que $B\Sigma$ es idempotente si y solo si $C^T B C$ es idempotente. Veamoslo:\\

  \boxed{\implies} Supongamos que $B\Sigma$ es idempotente, es decir, $B\Sigma B = B$. Entonces:
  \[
  (C^TBC)(C^TBC) = C^TBCC^TBC = C^TB\Sigma BC = C^TBC \implies idempotente
  \]
  \boxed{\impliedby} Supongamos ahora que $C^TBC$ es idempotente. Entonces:
  \[
  C^TBC = (C^TBC)(C^TBC) = C^TB\Sigma B C
  \]
  Multiplicamos a la izquierda por $(C^T)^{-1}$ y a la derecha por $C^{-1}$
  \[
  (C^T)^{-1}C^TBCC^{-1} = B = B\Sigma B = (C^T)^{-1}(C^T B \Sigma B C)C^{-1} \implies B \Sigma \ \ idempotente
  \]

  En cuanto a $k$ y $\delta$, por el mismo teorema anterior tenemos que
  \[
  k = \operatorname{rango}(C^T B C) = \operatorname{tr}(C^TBC) = \operatorname{tr}(BCC^T) = \operatorname{tr}(B\Sigma) = \operatorname{rango}(B\Sigma) = \operatorname{rango}(B)
  \], donde en las dos últimas igualdades hemos usado que $B\Sigma$ es idempotente y $\Sigma$ es no singular. Veamos $\delta$
  \[
  \delta = (C^{-1} \mu)(C^T B C)(C^{-1}\mu) = \mu(C^T)^{-1}C^T B CC^{-1}\mu = \mu^T B \mu^T
  \]
\end{proof}

%% FALTAN MUCHOS APUNTES

\section{Inferencia en la Distribución Normal Multivariante}
\subsection{Introducción}

Consideremos una población que sigue una distribución normal multivariante $N_p(\mu,\Sigma)$ con $\Sigma > 0$. Para poder realizar la inferencia será necesario tomar una muestra aleatoria simple de dicha población, esto es, observaciones independientes idénticamente distribuidas. La denotaremos por
\[
\{\vectX_\alpha: \alpha= 1 ,\dots,N\},
\]
donde $N$ es el tamaño muestral y $\vectX_{\alpha_i}$, la componente i-ésima correspondiente al vector $\vectX_i$ de la observación $\alpha$-ésima.

Nuestro objetivo inicial será estimar $\mu$ y $\Sigma$. El enfoque que tomaremos será la maximización de la función de verosimilitud.

\begin{ndef}[Media muestral]
  Llamamos \textit{media muestral} al vector de $\R^p$ construido de la forma:
  \[
  \overline{\vectX} = \frac{1}{N} \sum_{\alpha = 1 }^N \vectX_\alpha 
  = \begin{pmatrix}
      \frac{1}{N} \sum_{\alpha = 1 }^N \vectX_{\alpha_1} \\ 
      \vdots \\ 
      \frac{1}{N} \sum_{\alpha = 1 }^N \vectX_{\alpha_p}
    \end{pmatrix} 
  = \begin{pmatrix}
      \overline{\vectX}_1 \\ 
      \vdots \\ 
      \overline{\vectX}_p 
    \end{pmatrix}
  \]
\end{ndef}

\begin{ndef}[Matriz de dispersiones muestral]
  Llamamos \textit{matriz de dispersiones muestral} con respecto a $\overline{\vectX}$ a la matriz
  \[
    A = \sum_{\alpha = 1}^N (\vectX_\alpha - \overline{\vectX})(\vectX_\alpha - \overline{\vectX})^T.
  \]
\end{ndef}
A partir de la definición de matriz de dispersiones muestral podemos realizar las siguientes definiciones.
\begin{ndef}
  Definimos:
  \begin{itemize}
    \item La \textit{matriz de covarianzas muestral}, $S_N = A/N$.
    \item La \textit{matriz de quasi-covarianzas nuestral}, $S_N = A/(N-1)$.
    \item La \textit{matriz de correlaciones muestral}, 
    \[
      R =D^{-1/2}S_N D^{-1/2},
    \] donde 
    \[
      D = \operatorname{diag}(S_{11},\dots, S_{pp}) 
      \quad \text{y} \quad 
      D^{-1/2} = \operatorname{diag}\left(
        \frac{1}{\sqrt{S_{11}}},\dots,\frac{1}{\sqrt{S_{pp}}}
      \right).
    \]
    \end{itemize}
\end{ndef}

El siguiente resultado nos será últil en desarrollos posteriores y es independiente de la distribución multivariante que se esté considerando.

\begin{lema}
  Sea $\{\vectX_\alpha: \alpha = 1,\dots , N\}$ una muestra de una población \(p\)-dimensional y sea $\overline{\vectX}$ su media muestral. Entonces, para cualquier vector $b\in \mathbb R^p$ se verifica que
  \[
    \sum_{\alpha = 1}^N(\vectX_\alpha - b)(\vectX_\alpha - b)^T 
    = \sum_{\alpha = 1}^N(\vectX_\alpha - \overline{\vectX})(\vectX_\alpha - \overline{\vectX})^T + N(\overline{\vectX} - b)(\overline{\vectX} - b)^T.
  \]
  En particular, si tomamos
  \begin{itemize}
    \item el vector $b = \mu$ (si existe), entonces
    \[
      \sum_{\alpha = 1}^N(\vectX_\alpha - \mu)(\vectX_\alpha - \mu)^T 
      = \sum_{\alpha = 1}^N(\vectX_\alpha - \overline{\vectX})(\vectX_\alpha - \overline{\vectX})^T + N(\overline{\vectX} - \mu)(\overline{\vectX} - \mu)^T.
    \]
    \item el vector $b = 0$, entonces
    \[
      \sum_{\alpha = 1}^N(\vectX_\alpha - \mu)(\vectX_\alpha - \mu)^T 
      = \sum_{\alpha = 1}^N(\vectX_\alpha - \overline{\vectX})(\vectX_\alpha - \overline{\vectX})^T + N\overline{\vectX}\overline{\vectX}^T.
    \]
  \end{itemize}
\end{lema}

% FIXME: repetido? incompleto?
% \subsection{EMV de máxima verosimilitud de \texorpdfstring{$\boldsymbol \mu$}{mu} y \texorpdfstring{$\boldsymbol \Sigma$}{sigma} en una DNM}

% \begin{ndef}[Función de verosimilitud]
%   La función de verosimilitud se expresa como la función de $\mu$ y $\Sigma$ dada por:
%   \[
% L(\mu,\Sigma;x_1,\dots,x_\alpha) = f_{(\mu,\Sigma)}(x_1,\dots,x_N)
%   \]
% \end{ndef}

% FIXME: Aqui faltan apuntes. Fotos de sofía 3-7. cuidado con cosas repetidas.

\subsection{Estimadores de máxima verosimilitud de \texorpdfstring{$\boldsymbol \mu$}{mu} y \texorpdfstring{$\boldsymbol \Sigma$}{sigma} en una DNM}

\begin{ndef}[Función de verosimilitud]
  Sea el vector aleatorio $\vectX \sim N_p(\mu,\Sigma)$, con $\Sigma > 0$ y sea $\{X_\alpha : \alpha =1,\dots,N \}$ una muestra aleatoria simple.
  Para una realización muestral $\{ x_\alpha : \alpha = 1, \dots, N\}$, definimos la \textit{función de verosimilitud} como
  \[
  L(\mu,\Sigma;x_1,\dots,x_p) = f_{(\mu,\Sigma)}(x_1,\dots,x_N) = \prod_{\alpha = 1}^N f_{(\mu,\Sigma)}(x_\alpha).
  \]

\end{ndef}

\begin{nprop}[Estadísticos máximo versosímiles del vector de medias y de la matriz de covarianzas]
  \label{nprop:emv-medias-covarianzas}
  Sea el vector aleatorio $\vectX \sim N_p(\mu,\Sigma)$, con $\Sigma> 0$ y sea $\{ X_\alpha : \alpha = 1,\dots, N\}$ una muestra aleatoria simple de dicha población. 
  Entonces, los estadísticos máximo verosímiles del vector de medias $\mu$ y de la matriz de covarianzas $\Sigma$ son, respectivamente,
  \[
  \hat{\mu} = \vectX \quad \text{y} \quad \widehat{\Sigma} = \frac{A}{N} = S_N.
  \]
\end{nprop}

Para poder demostrar esta proposición necesitamos primero el siguiente lema.

\begin{lema}[Lema de Watson]
  \label{lema:watson}
  Sea $f(G) = -N \log(\det(G)) - \operatorname{tr}(G^{-1}D)$ con $G$ el argumento y $D$, ambas matrices simétricas definidas positivas. Entonces, existe el máximo de $f$ respecto $G$ y se alcanza en $G = D/N$.
\end{lema}

\begin{proof}
  Se deja como ejercicio para el lector. Como pista, utilizar el teorema de Dykstra.
\end{proof}

Ya estamos en condiciones de demostrar la proposición \ref{nprop:emv-medias-covarianzas}.

\begin{proof}[Demostración (Proposición \ref{nprop:emv-medias-covarianzas})]
  Consideremos primero el logaritmo de la función de verosimilitud, dado por
  \[
    \log L(\mu,\Sigma) = -\frac{pN}{2}\log(2\pi) - \frac{N}{2}\log\left(\det(\Sigma)\right) - \frac{1}{2}\operatorname{tr}\left(\Sigma^{-1}A\right) - \frac{N}{2}(\overline{\vectX} - \mu)^T \Sigma^{-1}(\overline{\vectX} - \mu).
  \]
  Ahora vayamos por partes. Al maximizar dicha función en \(\mu\), se maximizará donde se minimice la forma cuadrática
  \[
    (\overline{\vectX} - \mu)^T \Sigma^{-1}(\overline{\vectX} - \mu).
  \]
  Como $\Sigma > 0$, se tiene que $\Sigma^{-1} > 0$ así que $(\vectX - \mu)^T \Sigma^{-1}(\vectX - \mu) \geq 0$, que de hecho es igual a cero si y solo si $\mu = \vectX$. 
  Luego el estadístico máximo verosímil de $\mu$ es $\vectX$, ya que el valor que maximiza a $L(\mu,\Sigma)$ es $\mu = \vectX$.

  Por otro lado, para maximizar $\log L(\mu,\Sigma)$ en $\Sigma$ hemos de maximizar la función
  \[
    -\frac{pN}{2}\log(2\pi) + \frac{1}{2}f(\Sigma), \quad \text{con} \quad f(\Sigma) = -N\log(\operatorname{det}(\Sigma)) - \operatorname{tr(\Sigma^{-1}A)}.
  \]
  Tomando \(G = \Sigma\) y \(D = A\) en el lema de Watson (\ref{lema:watson}), el máximo se alcanza en \(A/N = S_N\), que es la matriz de covarianzas muestral.
\end{proof}

\subsection{Distribuciones de la media muestral y de la matriz de dispersiones}

A continuación vamos a obtener tanto la distribución exacta del vector media muestral, como de la matriz de dispersiones \(A = NS\), bajo la hipótesis de normalidad.
El resultado clave para hallar dichas distribuciones es el Teorema de Fisher, que nos proporcionará ambas distribuciones, además de la independencia entre ambas. 

Conocer la forma en que se distribuye la matriz de dispersiones es fundamental para demostrar el carácter definido positivo de la misma. 
Esto, junto con las condiciones bajo las que se verifica estarán recogidas en el Teorema de Dykstra.

\begin{nth}[Teorema de Fisher multivariante]
  Dada una muestra aleatoria $\{\vectX_\alpha: \alpha = 1,\dots,N\}$ de una población $N_p(\mu,\Sigma)$, el vector de medias $\overline{\vectX}$ se distribuye según una distribución $N_p\left(\mu,\Sigma/N\right)$.
  La matriz de dispersiones $A$ (EMV de $\Sigma$) se distribuye como lo haga
  \[
    \sum_{\alpha =1}^{N-1} Z_\alpha Z_\alpha^T,
  \]
  siendo los vectores $Z_\alpha$ independientes e idénticamente distribuidos según $N_p(0,\Sigma)$.
  Ambas distribuciones, $A$ y $\overline{\vectX}$, son independientes.
\end{nth}

\begin{nth}[Teorema de Dykstra]
  Sea el vector aleatorio $\vectX \sim N_p(\mu,\Sigma)$, con $\Sigma > 0$, y sea $\{X_\alpha : \alpha = 1,\dots,N\}$ una muestra aleatoria extraída de la población. La matriz de dispersiones muestral
  \[
    A = \sum_{\alpha = 1}^N (\vectX_\alpha - \overline{\vectX})(\vectX_\alpha - \overline{\vectX})^T
  \]
  es definida positiva con probabilidad 1 si y solo si $p<N$.
\end{nth}

\subsection{Distribuciones asintóticas de \texorpdfstring{$\boldsymbol A_{\boldsymbol n}$}{An} y \texorpdfstring{$\boldsymbol X_{\boldsymbol n}$}{Xn} para una población cualquiera}

\begin{nprop}
  Sea \(\{X_{\alpha}\}_{\alpha \geq 1}\) una sucesión de vectores \(p\)-dimensionales independientes e idéncitcamente distribuidos con media \(\boldsymbol \mu\) desconocida y matriz de covarianzas \(\Sigma\). Sea \[
    \overline{\vectX}_N = \frac{1}{N}\sum_{\alpha = 1}^{N}\vectX_{\alpha} \quad \text{para todo } N \geq 1.
  \]
  Entonces, cuando \(N \to \infty\), \[
    N^{\sfrac{1}{2}}(\overline{\vectX}_N - \boldsymbol \mu) = N^{-\sfrac{1}{2}}\sum_{\alpha = 1}^{N}(\overline{\vectX}_{\alpha} - \boldsymbol \mu) \sim N_p(\boldsymbol 0, \Sigma).
  \]
\end{nprop}

\begin{proof}
  La demostración se basa en el teorema central del límite.
  Sea \[
  \vectY_N = N^{-\sfrac{1}{2}} \sum_{\alpha = 1}^N (\vectX_{\alpha} - \boldsymbol \mu).
  \]
  Por el teorema de continuidad de las funciones características es suficiente demostrar que la función característica \(\Psi_{\vectY_N}(t)\) converge a \(\exp\{-\sfrac{1}{2}\boldsymbol t^T\Sigma\boldsymbol t\}\).
  Sabemos que para todo \(\boldsymbol t \in \mathbb R \) se tiene que \(\Psi_{\vectY_N}(\boldsymbol t) = \Psi_{\boldsymbol t^T\vectY_N}(1)\) y tenemos que ver que cómo es \(\Psi_{\boldsymbol t^T \vectY_N}(s)\) para \(s \in \mathbb R\).
  Tenemos \[
    \boldsymbol t^T\vectY_N = N^{-\sfrac{1}{2}}\sum_{\alpha = 1}^{N}\left(\boldsymbol t^T\vectX_{\alpha} - \boldsymbol t^T\boldsymbol\mu\right),
  \]
  donde sabemos que \(\boldsymbol t^T \vectX_{\alpha} - \boldsymbol t^T \boldsymbol \mu \sim (\boldsymbol 0, \boldsymbol t^T\Sigma \boldsymbol t)\) para todo \(\alpha = 1, \dots, N\).
  Aplicando el teorema central del límite tenemos \[
    \frac{\sum_{\alpha = 1}^{N}\left(\boldsymbol t^T\vectX_{\alpha} - \boldsymbol t^T\boldsymbol\mu\right)}{N^{\sfrac{1}{2}}\sqrt{\boldsymbol t^T\Sigma \boldsymbol t}} 
    = \frac{N^{-\sfrac{1}{2}}\sum_{\alpha = 1}^{N}\left(\boldsymbol t^T\vectX_{\alpha} - \boldsymbol t^T\boldsymbol\mu\right)}{\sqrt{\boldsymbol t^T\Sigma \boldsymbol t}}
    = \frac{\boldsymbol t^T \vectY_N}{\sqrt{\boldsymbol t^T\Sigma \boldsymbol t}}
    \sim N_p(0, 1).
  \]
  Luego \(\boldsymbol t^T\vectY_N \sim N_p(\boldsymbol 0, \boldsymbol t^T \Sigma \boldsymbol t)\). 
  Por el teorema de continuidad de las funciones características se tiene que para todo \(s \in \mathbb R\), cuando \(N\) tiende a infinito, \(\Psi_{\boldsymbol t^T\vectY_N}(s)\) tiende a \(\exp\{-\sfrac{1}{2}\,s^2(\boldsymbol t^T\Sigma \boldsymbol t)\}\).
\end{proof}

\begin{nprop}[Distribución asintótica de la matriz de dispersión]
  Sea \(\{X_{\alpha}\}_{\alpha \geq 1}\) una sucesión de vectores aleatorios independientes e idénticamente distribuidos, con vector de medias \(\boldsymbol \mu\) y matriz de covarianzas \(\Sigma\).
  Además suponemos que los vectores \(\vectX_{\alpha}\) tienen momentos de cuarto orden finitos.
  Consideremos, para cada \(N\) fijo,
  \[
    \overline{\vectX}_N = \frac{1}{N}\sum_{\alpha = 1}^N \vectX_{\alpha}
    \quad
    \text{y}
    \quad
    A_N = \sum_{\alpha = 1}^N (\vectX_{\alpha} - \overline{\vectX}_N)(\vectX_{\alpha} - \overline{\vectX}_N)^T.
  \]
  Entonces, \[
    N^{-\sfrac{1}{2}}(A_N - N\Sigma) \sim N_p(\boldsymbol 0, V),
  \] cuando \(N \to \infty\); en el sentido de que \[
    N^{-\sfrac{1}{2}}\left(\operatorname{Vec}(A_N) - N\operatorname{Vec}(\Sigma)\right) \sim N_p(\boldsymbol 0, V),
  \] de nuevo cuando \(N \to \infty\) y con \[
    V = \operatorname{Cov}\left(\operatorname{Vec}\left((\vectX_{\alpha - \boldsymbol \mu})(\vectX_{\alpha - \boldsymbol \mu})^T\right)\right).
  \]
\end{nprop}

\begin{proof}
  Por la fórmula de los momentos, \[
    A_N = \sum_{\alpha = 1}^N(\vectX_{\alpha} - \boldsymbol \mu)(\vectX_{\alpha} - \boldsymbol \mu)^T - N(\overline{\vectX}_N - \boldsymbol \mu)(\overline{\vectX}_N - \boldsymbol \mu)^T.
  \]
  Llamamos \[
    Z_{\alpha} = (\vectX_{\alpha} - \boldsymbol \mu)(\vectX_{\alpha} - \boldsymbol \mu)^T
    \quad
    \text{y}
    \quad
    B_N = (\overline{\vectX}_N - \boldsymbol \mu)(\overline{\vectX}_N - \boldsymbol \mu)^T,
  \] de forma que \[
    A = \sum_{\alpha = 1}^NZ_{\alpha} - NB_N.
  \]
  Vectorizando tenemos \[
    \operatorname{Vec}(A) = \sum_{\alpha = 1}^N \operatorname{Vec}(Z_{\alpha}) - N\operatorname{Vec}(B_N).
  \]
  Entonces, \[
    N^{-\sfrac{1}{2}}\left(\operatorname{Vec}(A) - N\operatorname{Vec}(\Sigma)\right)
    = N^{-\sfrac{1}{2}}\left(\sum_{\alpha = 1}^N\left(\operatorname{Vec}(Z_{\alpha}) - \operatorname{Vec}(\Sigma)\right)\right) - N^{\sfrac{1}{2}} \operatorname{Vec}(B_N).
  \]
  Vamos a estudiar el comportamiento asintótico de los dos sumandos de la última igualdad. 
  Observamos primero que \[
    \operatorname{E}[Z_{\alpha}] = \operatorname{E}[(\vectX_{\alpha} - \boldsymbol \mu)(\vectX_{\alpha} - \boldsymbol \mu)^T] = \Sigma,
  \] es decir, \(\operatorname{E}[\operatorname{Vec}(Z_{\alpha})] = \operatorname{Vec}(\Sigma)\).
  Para el primer sumando, los vectores \(\operatorname{Vec}(Z_{\alpha})\), con \(\alpha = 1, \dots, N\), constituyen una sucesión de vectores aleatorios independientes e indénticmente distribuidos (de dimensión \(p^2 \times 1\)) con vector de medias \(\operatorname{Vec}(\Sigma)\) y matriz de covarianzas \(V\).
  Aplicando el resultado anterior vemos que \[
    N^{-\sfrac{1}{2}}\left(\sum_{\alpha = 1}^N\left(\operatorname{Vec}(Z_{\alpha}) - \operatorname{Vec}(\Sigma)\right)\right)
  \] sigue una distribución \(N_{p^2}(\boldsymbol 0, V)\) cuando \(N \to \infty\).
  Para el segundo sumando, podemos escribir \begin{align*}
    N^{\sfrac{1}{2}} \operatorname{Vec}(B_N) 
    &= N^{\sfrac{1}{2}} \operatorname{Vec}\left((\overline{\vectX}_N - \boldsymbol \mu)(\overline{\vectX}_N - \boldsymbol \mu)^T\right)\\
    &= \operatorname{Vec}\left(\frac{1}{N^{\sfrac{1}{4}}}\left(N^{-\sfrac{1}{2}}(\vectX_N - \boldsymbol \mu)\right) \frac{1}{N^{\sfrac{1}{4}}}\left(N^{-\sfrac{1}{2}}(\vectX_N - \boldsymbol \mu)\right)^T\right).
  \end{align*}
  Aplicando el resultado sobre el comportamiento asintótico de \(\overline{\vectX}_N\), se tiene que \(N^{\sfrac{1}{2}}(\overline{\vectX}_N - \boldsymbol \mu) \sim N_p(\boldsymbol 0, \Sigma)\) cuando \(N \to \infty\).
  Tenemos entonces que tanto \[
    \frac{1}{N^{\sfrac{1}{4}}}\left(N^{-\sfrac{1}{2}}(\vectX_N - \boldsymbol \mu)\right)
    \quad
    \text{como}
    \quad
    \frac{1}{N^{\sfrac{1}{4}}}\left(N^{-\sfrac{1}{2}}(\vectX_N - \boldsymbol \mu)^T\right)
  \] 
  convergen en probabilidad a \(\boldsymbol 0\).
  % Ejercicio: demostrar esto último con la desigualdad de Markov: \[
  % \operatorname{P}[|X| \geq a] \leq \frac{\operatorname{E}|X|}{a}.
  % \]
  % Ejercicio: demostrar también que es convergencia en distribución usando la desigualdad de Chebyshev.
  Luego \[
    \operatorname{Vec}\left(\frac{1}{N^{\sfrac{1}{4}}}\left(N^{-\sfrac{1}{2}}(\vectX_N - \boldsymbol \mu)\right) \frac{1}{N^{\sfrac{1}{4}}}\left(N^{-\sfrac{1}{2}}(\vectX_N - \boldsymbol \mu)\right)^T\right)
  \] 
  converge en probabilidad al vector \(\boldsymbol 0\).
\end{proof}

\begin{nprop}
  Sea \(\{X_{\alpha}\}_{\alpha \in \mathbb N}\) una sucesión de variables aleatorias tal que cuando \(\alpha\) tiende a infinito, \(X_{\alpha}\) converge a \(X\) en distribución. 
  Sea \(\{Y_{\alpha}\}_{\alpha \in \mathbb N}\) una sucesión de variables aleatorias tal que
  \[
    Y_{\alpha} \xrightarrow[\alpha \to \infty]{d}\boldsymbol 0 \; \left(\iff Y_{\alpha} \xrightarrow[\alpha \to \infty]{P} \boldsymbol 0 \right).
  \]
  Entonces, \(X_{\alpha} + Y_{\alpha} \xrightarrow[\alpha \to \infty]{d} X\).
\end{nprop}

% Utilizamos esto en lo último de la demostración de la proposición anterior

\subsection{Distribución de Wishart}

Wishart, 1928.

Motivación. Según el Teorema de Fisher (multivariante) para una muestra aleatoria simple \(\vectX_1, \dots, \vectX_n\) de una distribución normal multivariante \(N_p(\boldsymbol \mu, \Sigma)\) con \(\Sigma > 0\) se tiene que \(A = \sum_{\alpha = 1}^{N}(\vectX_{\alpha} - \overline{\vectX})(\vectX_{\alpha} - \overline{\vectX})^T\) se distribuye como lo hace \(\sum_{\alpha = 1}^N \vectZ_{\alpha}\vectZ_{\alpha}^T\), con \(\vectZ_{\alpha}\) variables aleatorias independientes e idénticamente distribuidas según una distribución \(N_p(0, \Sigma)\).

% FIXME: indicar que las Z_alpha estas son X_alpha - \overline{X_alpha}.

\begin{ndef}
  Sean \(\vectZ_1, \dots, \vectZ_n\) vectores aleatorios independientes e idénticamente distribuidos según una distribución \(N_p(\boldsymbol 0, \Sigma)\) con \(\Sigma > 0\) y con \(n \geq P\).
  Se define la \textit{distribución de Wishart centrada con \(n\) grados de libertad} como la distribución de la matriz aleatoria \(\sum_{\alpha = 1}^N \vectZ_{\alpha}\vectZ_{\alpha}^T\).
  Se denota por \(W_p(n, \Sigma)\).
\end{ndef}

Como consecuencia, dada una muestra aleatoria simple de una población que sigue una distribución \(N_p(\boldsymbol 0, \Sigma)\) con \(\Sigma > 0\) tenemos que
\[
  A = NS_N = (N-1)S_{N-1} \sim W_p(n, \Sigma).
\]
En el caso \(\Sigma > 0\), la distribución de Wishart tiene la función de densidad:
\[
  f(A) = \frac{
    |A|^{\sfrac{n-p-1}{2}}\exp\{-\frac{1}{2}\operatorname{tr}(\Sigma^{-1}A)\}
  }{
    2^{\sfrac{np}{2}}|\Sigma|^{\sfrac{n}{2}}\pi^{\sfrac{p(p-1)}{4}}\prod_{i=1}^p\Gamma(\frac{n+1-i}{2})
  }.
\]
Para \(A\) matriz simétrica definida positiva , tener \(n \geq P\) nos garantiza que \(A > 0\).
Usando la función gamma multivariante,
\[
 \Gamma_m(z) = \pi^{\textstyle \frac{m(m-1)}{4}}\prod_{i=1}^m \Gamma\left(z - \frac{i-1}{z}\right) 
\]
para \(z \in \mathbb C\) con \(\operatorname{Re}(z) \geq 0\),
\[
  f(A) = \frac{
    |A|^{\textstyle\sfrac{n-p-1}{2}}\exp\left\{-\frac{1}{2}\operatorname{tr}(\Sigma^{-1}A)\right\}
  }{
    2^{\textstyle\sfrac{np}{2}}|\Sigma|^{\textstyle\sfrac{n}{2}}\Gamma_p\left(\frac{n}{2}\right)
  }.
\]

\begin{nprop}[Ley de Wishart]
  \label{nprop:ley-wishart}
  Si \(a \sim W_1(n, \sigma^2)\) entonces \(\frac{a}{\sigma^2} \sim \chi^2_n\).
\end{nprop}

\begin{ejer}
  Demostrar  la proposición \ref{nprop:ley-wishart}. Para ello hay que hacer un cambio de variable en \(a\).
\end{ejer}

\begin{nprop}[Función característica de una matriz aleatoria]
  La función característica de una matriz aleatoria \(Y_{r \times s}\) se define como
  \[
    \Psi_Y(\Theta) = \operatorname{E}\left[\exp\left\{i \operatorname{tr}\left(\Theta^TY\right)\right\}\right]
  \]
  para toda \(\Theta\) en el espacio de matrices de orden \(r \times s\).
\end{nprop}

\begin{proof}
  Vemos inmediatamente que
  \begin{align*}
    \Psi_{\operatorname{vec}(Y)}\left(\operatorname{vec}(\Theta)\right)
    &= \operatorname{E}\left[
      \exp\left\{
        i[\operatorname{vec}(\Theta)^T]\operatorname{vec}(Y)
        \right\}
      \right]\\
    &= \operatorname{E}\left[
      \exp\left\{
        i\sum_{k=1}^s
          \left(
            \sum_{l=1}^r \Theta_{lk}Y_{lk}
          \right)
        \right\}
      \right]\\
    &= \operatorname{E}\left[\exp\left\{i \operatorname{tr}\left(\Theta^T Y\right)\right\}\right].
  \end{align*}
\end{proof}

% TODO: no se qué de una función característica restringida con una Y tilde

Veamos a continuación algunas propiedades de la Ley de Wishart.

\begin{nprop}[Momentos de primer y segundo orden]
  Sea \(A \sim W_p(n, \Sigma)\). Entonces, \begin{itemize}
    \item \(\operatorname{E}[A] = A\Sigma\).
    \item \(\operatorname{Cov}(a_{ij}, a_{kl}) = n(\sigma_{ik}\sigma_{jl}+ \sigma_{il}\sigma_{jk})\).
  \end{itemize}
\end{nprop}

\begin{nprop}[Reproductividad respecto de los grados de libertad]
  Sean \(A_1, \dots, A_q\) matrices aleatorias independientes, con \(A \sim W_p(n_{j}, \Sigma)\), con \(j = 1, \dots, q\).
  Entonces,
  \[
    A = \sum_{j=1}^q A_j \sim W_p\left(\sum_{j=1}^q n_j, \Sigma\right).
  \]
\end{nprop}

\begin{nprop}[Transformaciones lineales <<rectangulares>>]
  Sea \(A\) una matriz aleatoria tal que \(A \sim W_q(n, \Sigma)\) y sea \(M_{k\times p}\) una matriz no aleatoria de rango \(k \leq p\). 
  Entonces, 
  \[
    MAM^T \sim W_k(n, M\Sigma M^T).
  \]
  Análogamente,
  \[
    MA^{-1}M^T \sim W_k\left(n-m+k, \left(M\Sigma^{-1} M^T\right)^{-1}\right),
  \]
  donde \(m\) es... 
  %TODO: ¿qué es m? El profesor no lo sabe
\end{nprop}

\begin{proof}
  Por definición \(A = \sum_{\alpha = 1}^n \vectZ_{\alpha}\vectZ_{\alpha}^T\), con \(\vectZ_{\alpha} \sim N_p(\boldsymbol 0, \Sigma)\), independientes para cada \(\alpha\).
  Consideremos las transformaciones lineales
  \[
    \vectY_{\alpha} = M\vectZ_{\alpha} \quad \text{para}\quad \alpha = 1, \dots, n
  \]
  de tal forma que \(\vectY_{\alpha} \sim N_k(\boldsymbol 0, M\Sigma M^T)\).
  Entonces, por definición, \begin{align*}
    B = \sum_{\alpha = 1}^n \vectY_{\alpha}\vectY_{\alpha}^T = \dots = MAM^T \sim W_k(n, M\Sigma M^T).
  \end{align*}
\end{proof}

\begin{nprop}[Distribución de la matriz de cuasi-covarianzas muestral, \(\boldsymbol S_{\boldsymbol N \boldsymbol - \boldsymbol 1}\)]
  Sea \(S_n = A/n\) (con \(n = N - 1\)) la matrz de cuasi-covarianzas muestral que procede de la muestra aleatoria simple con población \(N_p(\boldsymbol \mu, \Sigma)\).
  Entonces, \(S_n \sim W_p(n, \Sigma/n)\) y además, \(S_N \sim W_p(N-1, \Sigma/N)\), con \(S_N = A/N\).
\end{nprop}