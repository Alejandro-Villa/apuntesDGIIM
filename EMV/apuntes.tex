\section{Fundamentación probabilística de vectores aleatorios}

En esta sección definimos algunos conceptos bien conocidos de la teoría de la probabilidad,
a modo de breve repaso y para fijar notación.

% TODO: insertar referencia a apuntes de Análisis Matemático II y Probabilidad.

\begin{ndef}[Vector y variable aleatorios]
    Sea $(\Omega, \mathscr{A}, P)$ un espacio de probabilidad. Un \emph{vector aleatorio}, $\boldsymbol X = (X_1, \dots, X_p)$ es una función medible
    \begin{align*}
      \boldsymbol X:(\Omega, \mathscr{A}) &\rightarrow (\mathbb{R}^p, \mathscr{B}^p)\,.
    \end{align*}
     Cuando $p=1$, lo notamos $X$ y decimos que es una \emph{variable aleatoria}.
    La condición de medibilidad es: \(\boldsymbol X^{-1}(B) \in \mathscr{A}\) para cada \( B \in \mathscr{B}^p\).
\end{ndef}



\begin{ndef}[Probabilidad inducida]
    La \emph{probabilidad inducida por} o \emph{distribución de} un vector aleatorio $\boldsymbol X = (X_1, \dots, X_p)$ se define como
    \[
    P_{\boldsymbol X}[B] := P[\boldsymbol X^{-1}(B)]\,.\] para todo \(B \in \mathbb{B}^p\). Es decir, $P_X = P \circ \restr{\boldsymbol X^{-1}}{\mathscr{B}}\,.$
\end{ndef}

\begin{notacion}
  Cuando escribamos probabilidades de sucesos asociados a un vector aleatorio $\boldsymbol X$, usaremos la siguiente notación:
  \begin{itemize}
  \item $P[\boldsymbol X \in A] = P_X[A]$,
  \item $P[X_1 \le x1, \dots, X_p \le x_p] = P_X[(-\infty, x_1]\times \dots \times (-\infty, x_p]]$,
  \end{itemize}
  y algunas formas análogas.
\end{notacion}

\begin{ndef}[Función de distribución]
    Se define la \emph{función de distribución} asociada a $P_{\boldsymbol X}$ como
    \begin{align*}
    F_{\boldsymbol X}:\mathbb{R}^p &\rightarrow [0,1] \\
    (x_1,\dots,x_p) &\mapsto P[X_1 \leq x_1, \dots, X_p \leq x_p]\,.
    \end{align*}
\end{ndef}

\begin{ndef}[Función de densidad]
    Si existe una función $f_{\boldsymbol X} : \R^p \to \R$ que sea integrable en el sentido de Lebesgue y tal que
    \[
    F_{\boldsymbol X}(\boldsymbol x) = \int^{x_1}_{-\infty} \dots \int^{x_p}_{-\infty} f_{\boldsymbol X}(u_1, \dots,  u_p) \mathop{}\!\mathrm{d}u_1 \dots \mathop{}\!\mathrm{d}u_p \text{ para todo } \boldsymbol  x \in \mathbb{R}^p
    ,\]
    diremos que $f_{\boldsymbol X}$ es una \emph{función de densidad} asociada a  $F_{\boldsymbol X}$. En los puntos de continuidad de $f_{\boldsymbol X}$, se puede escribir como:
    \[
    % TODO Añadir lo de las derivadas
    f_{\boldsymbol X}(\boldsymbol x) = \frac{\partial^p}{\partial x_1 \dots \partial x_p} F_{\boldsymbol X}(\boldsymbol x) \quad \text{para todo } \boldsymbol x \in \mathbb{R}^p
    .\]
\end{ndef}

\subsection{Independencia}

Veamos ahora algunas nociones relativas a la independencia de variables aleatorias.

\begin{ndef}[Conjunto rectangular]
  Se dice que $B\in \mathscr{B}^p$ es un \emph{conjunto rectangular} si es un producto de $p$ conjuntos de $\mathscr{B}$. Es decir, si
  
\[
    B = B_1 \times \dots \times B_p,\quad B_j \in \mathscr{B}, \quad j = 1, \dots, p
.\]
\end{ndef}

En general, dado un conjunto rectangular $B \in \mathscr{B}^p$ se tiene que
\[
P_{\boldsymbol X}[B] \neq P_{X_1}[B_1] \dots  P_{X_p}[B_p].
\]

\begin{ndef}
    Si en las condiciones anteriores se da la igualdad para todo conjunto producto en \(\mathbb{B}^p\), se dice que \(X_1, \dots, X_p\) son \emph{independientes}.
\end{ndef}

\begin{nprop}[Caracterizaciones de independencia]
  \(X_1, \dots, X_p\) son independientes si, y solo si se da alguna de las siguientes condiciones:
  \begin{nlist}
  \item $F_{\boldsymbol X} (\boldsymbol x) = F_{X_1} (x_1) \cdots F_{X_p}(x_p)\,$ para todo \(\boldsymbol x \in \mathbb{R}^p\).
   \item $\boldsymbol X$ tiene una función de densidad y \(f_{\boldsymbol X} (\boldsymbol x) = f_{X_1}(x_1) \cdots f_{X_p}(x_p)\,\) para todo \(\boldsymbol x \in \mathbb{R}^p\), salvo, a lo sumo, en un conjunto de medida de Lebesgue nula.
  \end{nlist}
\end{nprop}

\subsection{Función característica}

\begin{ndef}[Función característica]
    La \emph{función característica} de un vector aleatorio \(\boldsymbol X = (X_1,\dots,X_p)\) se define como \[\psi_{\boldsymbol X}(\boldsymbol t)=E\left[e^{i\boldsymbol t^T\boldsymbol X}\right], \quad \boldsymbol t\in \mathbb{R}^p.\]
\end{ndef}

\begin{nth}[Unicidad]
  La función característica de un vector aleatorio determina de forma única su distribución.
\end{nth}

\begin{nprop}
  Las componentes de \(\boldsymbol X=(X_1,\dots,X_p)^T\) son independientes si, y solo si \[\psi_{\boldsymbol X}(\boldsymbol t)=E\left[e^{i\boldsymbol t^T\boldsymbol X}\right] = \prod_{k=1}^p\psi_{X_k}(t_k)\,.\]
\end{nprop}

\begin{nprop}
  Si las componentes de \(\boldsymbol X=(X_1,\dots, X_p)\) son independientes, entonces la función característica de la variable \(Y=\sum_{k=1}^p X_k\) es \[\psi_{Y}(t)=E\left[e^{itY}\right] = \prod_{k=1}^p\psi_{X_k}(t).\]
\end{nprop}

\subsubsection{Transformaciones lineales}

Sea $\boldsymbol X = (X_1, \dots X_p)^T$ un vector aleatorio $p-$dimensional y sea $\boldsymbol Y = (Y_1,\dots,Y_q)^T$ otro vector aleatorio $q-$dimensional definido como
\[
  \boldsymbol Y = B\boldsymbol X + \boldsymbol b\,,
\]
con $B \in \mathscr{M}_{q\times p}(\R)$ una matriz constante y $\boldsymbol b\in \mathbb R ^n$.

\begin{nprop}
  La función característica de $\boldsymbol Y$ se obtiene como:
  \[
  \psi_{\boldsymbol Y}(\boldsymbol t) = E\left[e^{i\boldsymbol t^T \boldsymbol Y}\right] = \dots = e^{i\boldsymbol t^T \boldsymbol b} \psi_{\boldsymbol X}(B^T \boldsymbol t)\,, \quad \boldsymbol t = (t_1,\dots,t_q)^T \in \mathbb R^q\,.
  \]
\end{nprop}
%#TODO: escribir la forma matricial
%#TODO: ¿Está amsmath incluido? Usar bmatrix de amsmath
  En particular, si \[\boldsymbol X= \left( X_{(1)} | X_{(2)}\right)^T := (X_{(1),1}, \dots, X_{(1),k}, X_{(2),1}, \dots, X_{(2),p-k}),\] con $X_{(1)}$ de dimensión $(k\times1)$ y $X_{(2)}$ de dimensión $(p-k) \times 1$, se tendría que
  \[
X_{(1)} = \left(\begin{array}{c | c}
    I_k & 0 \\ \hline
    0 & 0
        \end{array}\right)X + 0_{k\times 1}\,,
\]
por lo que, para $\boldsymbol t_{(1)} = (t_1,\dots,t_k)^T \in \mathbb R^k$
\[
\psi_{X_{(1)}}\left(\boldsymbol t_{(1)}\right) = \underbrace{e^{i\boldsymbol t_{(1)}^T 0}}_{=1} \psi_X\left(\left(\begin{array}{c | c}
    I_k & 0 \\ \hline
    0 & 0
        \end{array}\right)\boldsymbol t_{(1)}\right) = \psi_X\left(\frac{\boldsymbol t_{(1)}}{0_{(p-k)}}\right)\,.
\]

\subsubsection{Relación con la función de densidad}

La función de densidad (en el caso continuo) de un vector aleatorio $\boldsymbol X = (X_1,\dots,X_p)^T$ y la correspondiente función característica constituyen un par de \emph{transformadas de Fourier}, de forma que
\[
\psi_{\boldsymbol X}(\boldsymbol t) = \int_{\mathbb R ^p} e^{i \boldsymbol t^T \boldsymbol x} f_X(\boldsymbol x) \mathop{}\!\mathrm{d}\boldsymbol x\,,
\]
y por otro lado
\[
f_{\boldsymbol X}(\boldsymbol x) =  \dfrac{1}{(2\pi)^p} \int_{\mathbb R^p} e^{- i \boldsymbol t^T x} \psi_{\boldsymbol X}(t) \mathop{}\!\mathrm{d}\boldsymbol t\,.
\]


\subsection{Aspectos generales sobre vectores aleatorios}
\subsubsection{Esperanza y covarianza}
\begin{ndef}
  Sea $\boldsymbol X=(X_1,\dots,X_n)^T$ un vector aleatorio. Se define el \emph{vector de medias} de $\boldsymbol X$ como:
  \[
  \boldsymbol \mu_{\boldsymbol X} = E[\boldsymbol X] = \begin{pmatrix}  E[X_1] \\ \dots \\ E[X_p] \end{pmatrix} = \begin{bmatrix} \mu_1 \\ \dots \\ \mu_p \end{bmatrix}
  \]
  siempre que existan \emph{todas} las esperanzas unidimensionales.
  \end{ndef}

\begin{nprop}[Propiedad de linealidad]
  Sea $\boldsymbol Y = B\boldsymbol X + \boldsymbol b$ con $B \in M_{q\times p}$ constante y $\boldsymbol b \in \mathbb R^q$ constante. Entonces,
  \[
    \boldsymbol \mu_{\boldsymbol Y} = E[\boldsymbol Y] = BE[\boldsymbol X] + \boldsymbol b = B\boldsymbol \mu_{\boldsymbol X} + \boldsymbol b\,.
\]
\end{nprop}
%#TODO: Demostración(ejercicio)

Podemos extender esta propiedad al caso de matrices aleatorias. Sea $X\in \mathcal M_{p\times q}$ es una matriz aleatoria, y $B\in \mathcal M_{m\times p}$ , $C \in \mathcal M_{q\times n}$ , $D \in \mathcal M_{m \times n}$ matrices constantes. Entonces, para $W = BXC + D$, una matriz aleatoria de dimensión $m\times n$, la matriz de medias viene dada por
\[
E[W] = \left(E[W_{kl}]\right)_{(kl)} = B \cdot E[X] \cdot C + D\,, \quad \text{con } E[X] = \left(E[X_{ij}]\right)_{(ij)}\,.
\]

\begin{ndef}
  Se define la \emph{matriz de covarianzas} del vector $\boldsymbol X = (X_1,\dots,X_p)^T$ como:
  \[
\Sigma = \operatorname{Cov}(\boldsymbol X) = E\left[(\boldsymbol X-\boldsymbol \mu)(\boldsymbol X-\boldsymbol \mu)^T\right] = \begin{bmatrix} \sigma_{11} & \dots & \sigma_{1p} \\ \vdots& \ddots & \vdots \\ \sigma_{p1} &  \dots & \sigma_{pp}\end{bmatrix}\,,
\]
con $\sigma_{ij} = E\left[(X_i - \mu_i)(X_j - \mu_j)\right] = \sigma_{ji}$. Solo puede definirse si existen todas las esperanzas. 
\end{ndef}

\begin{nota}
  Los elementos de la diagonal de la matriz de covarianzas se pueden calcular como \[\sigma_{ii}=E\left[(X_i - \mu_i)^2\right] = \operatorname{Var}(X_i) = \sigma_i^2\,,\]siendo $\sigma_i$ la \emph{desviación típica}.
\end{nota}

\begin{nprop}[Propiedades elementales]
  La matriz de covarianzas verifica las siguientes propiedades elementales:
  \begin{enumerate}
    \item Es simétrica.
    \item Los elementos de su diagonal son no negativos.
    \item La clase de matrices de covarianzas en dimensión $p\times p$ coincide con la clase de matrices simétricas definidas no negativas.
    \end{enumerate}
  \end{nprop}
%#TODO: demostración (Ejercicio)
\begin{proof}
  Vamos a ver únicamente la demostración de la propiedad 3.
  \begin{nlist}
    \item[3.] $\boxed{\implies}\,$ Supongamos que $\Sigma$ es la matriz de covarianza de un vector aleatorio $\boldsymbol X$, y $\boldsymbol \mu_{\boldsymbol X}$ es su vector de medias. Sea $\boldsymbol \alpha \in \mathbb R^p$. Tenemos que probar que $\boldsymbol \alpha^T \Sigma \boldsymbol \alpha^T \geq 0$. Consideramos la variable aleatoria $\boldsymbol \alpha^T \boldsymbol X$. \begin{align*}
      0 \leq \operatorname{Var}(\alpha^T X) = E\left[\|\alpha^T X - E[\alpha^T X]\|^2\right] &= E\left[\|\alpha^T X - \alpha^T \mu_X\|^2\right] = E\left[\|\alpha^T(X-\mu)\|^2\right]\\ 
        &=^{(1)} E\left[\alpha^T(X-\mu)(X-\mu)^T \alpha\right] \\
        &= \alpha^T E\left[(X-\mu)(X-\mu)^T\right] \alpha \\
        &= \alpha^T \Sigma \alpha
    \end{align*} Donde en $(1)$ hemos separado el cuadrado y hemos traspuesto uno de los términos (el resultado es el mismo, un escalar).

    $\boxed{\impliedby}\,$ Sea $A$ una matriz simétrica, definida no negativa. Entonces, se puede encontrar una factorización $A = B B^T$. donde $C \in \mathscr{M}_{p}(\R)$.
    Consideramos un vector aleatorio $\vectX$ cuyas entradas son $p$ variables independientes e idénticamente distribuidas con distribución normal estándar. Entonces, $\Sigma_{\vectX} = I_p$. Esto se puede comprobar usando que $E[XY] = E[X]E[Y]$ si $X$ e $Y$ son variables aleatorias independientes. Consideramos también $\vectY = B\vectX$, entonces:

    \[
       \Sigma_{\vectY} = B\Sigma_{\vectX} B^T = B B^T = A
    .\]

%#TODO: Terminar demostración
  \end{nlist}
\end{proof}

%#TODO: revisar el estilo de esta parte, quizá añadir una subsección?
Vamos a ver ahora diferentes \textbf{casos} que se pueden dar respecto a la matriz $\Sigma$.

\paragraph{Si $\Sigma$ es definida positiva} Se nota como $\Sigma > 0$. En este caso $\Sigma$ es \emph{no singular} pues $(|\Sigma| > 0)$, y por tanto existe su inversa, $\Sigma^{-1}$. Esto es interesante pues nos permite normalizar el vector aleatorio de manera que tenga un vector de medias cero y una matriz de covarianzas que sea la identidad.

\begin{nota}
  Usaremos la notación $X\sim(\mu,\Sigma)$ para denotar que la media y la matriz de covarianzas están bien definidas.
\end{nota}

\begin{ndef}[Normalización]
    Dado un vector aleatorio $\boldsymbol X\sim(\mu,\Sigma)$, con $\Sigma>0$, se define su \emph{normalización} en «origen» y «escala» como
    \[
  Z = C^{-1}(X - \mu)\,,
\]
para cualquier matriz $C\in M_{p\times p}$ tal que $E = CC^T$. Esta matriz $C$ no es única, pues existen infinitas descomposiciones de $\Sigma$ de esta forma.

  \end{ndef}

Veamos a continuación algunas de las propiedades de esta variable $Z$.
  \begin{nlist}
    \item
  $
  E[Z] = E[C^{-1}(X-\mu)] = C^{-1}E[(X-\mu)] = C^{-1}(E[X] - \mu) = C^{-1}(\mu - \mu ) = 0
  $

\item $\operatorname{Cov}(Z) = E[ZZ ^T] = E[C^{-1}(X-\mu)(X-\mu)^T(C^{-1})^T]$ y, por la propiedad de las matrices que nos dice que $(C^{-1})^T = (C^T)^{-1}$, se tiene en la igualdad anterior : $\operatorname{Cov}(Z) = C^{-1}E[(X-\mu)(X-\mu)^T](C^T)^{-1} = C^{-1} \Sigma (C^T)^{-1} = C^{-1}CC^T(C^T)^{-1} = I \cdot I = I \implies Z \sim (0, I_{p\times p}).$

  \end{nlist}

  \begin{ndef}
    Se define la \emph{distancia de Mahalanobis} de $\vectX\sim(\mu,\Sigma)$ con $\Sigma > 0$ con respecto a su vector de medias como
    \[
    \Delta(\vectX,\mu) = \left\{ (\vectX-\mu)^T \Sigma (\vectX-\mu) \right\} ^{\frac{1}{2}},
    \]
    que es una variable aleatoria.
  \end{ndef}

  Vamos a interpretar esta definición. Sea $\vectZ$ una normalización de $\vectX$:
  \[
  \Delta(\vectX,\mu) = \left\{ (\vectX-\mu)^T (CC^T)^{-1}(\vectX-\mu)\right\}^{\frac{1}{2}} = \left\{ (\vectX-\mu)^T (C^T)^{-1}C^{-1}(\vectX-\mu)\right\}^{\frac{1}{2}} = \left\{\vectZ^T \vectZ\right\}^{\frac{1}{2}} = \Vert \vectZ\Vert\,,
  \]
  con norma la euclídea en $\mathbb R^p$. Así, esta distancia es la norma de cualquier normalización de $\vectX$. Recordemos que $\Vert \vectZ\Vert$ es una variable aleatoria, pues es una aplicación de una función a un vector aleatorio.

  \begin{nprop}\hfill
    \begin{nlist}
    \item $E[\Delta^2 (X,\mu)] = p$,
    \item la ecuación $\Delta(X,\mu) = k$ pcon $k\geq 0$ constante, define la \textbf{hipervariedad de contorno} correspondiente a aquellos puntos $x \in \mathbb R^p$ taes que, en el espacio transformado $\mathbb R^p$ por:
      \[
      x \mapsto z = C^{-1}(x-\mu)
      \]
      se sitúan en la esfera (euclídea, $p-$dimensional) de radio $k$ y centro en el origen.
    \end{nlist}

  \end{nprop}

\paragraph{$\Sigma$ es \textbf{semidefinida positiva}} (esto es, $\Sigma \geq 0$ con $|\Sigma| = 0$).  En este caso $\Sigma$ es singular ($\nexists \Sigma ^{-1} $ ). Por tanto, se tiene que $rango(\Sigma) = r < p$ y $\Sigma = C C^T$ con $C \in M_{p\times r}$ y cuyo rango es $r$.\\

  \begin{ncor}
    Con probabilidad $1$, las componentes del vector $X = (X_1, \dots, X_n)$ cumplirán una relación de dependencia lineal del tipo:
    \[
    \alpha^T X = k, \quad \quad \alpha \in \mathbb R^p, \quad \alpha \ne 0 , \quad k \in \mathbb R
    \]
    
  Por tanto, toda la variabilidad de $X$ se sitúa en un hiperplano afín (casi seguramente con respecto a la medida $P$)
\end{ncor}

\begin{proof}
  Se sigue del hecho de que la covarianza es un producto en el espacio cociente de las variables aleatorias con varianza finita que son iguales salvo constante aditiva,
  y de que la matriz de Gram de un conjunto finito de vectores en un espacio prehilbertiano es de rango máximo si y solo si los vectores son linealmente independientes.
\end{proof}

Dicho esto, existen varias \textbf{medidas de variación global}.Podemos distinguir, dado $\Sigma$, y $\lambda_j$ con $j = 1,\dots,p$ los autovalores de $\Sigma$:
\begin{nlist}
\item $\det(\Sigma) =  \prod_{j=1}^p \lambda_j$,
  \item $\traza(\Sigma) = \sum_{j = 1}^p \sigma_j^2 = \sum_{j=1}^p \lambda_j$
\end{nlist}


\subsection{Caracterización de la distribución de un v.a. en términos de las distribuciones univariantes de c.l. de sus componentes}

\begin{nth}
  Sea $\vectX = (X_1,\dots, X_n)^T$ un vector aleatorio. Se tiene que la distribución de $\vectX$ queda unívocamente determinada por las distribuciones univariantes de la forma
  \[
\alpha ^T \vectX, \quad \alpha \in \mathbb R ^p\,.
  \]
\end{nth}
\begin{proof}
  Sea $Y_\alpha = \alpha^T \vectX$ (variable aleatoria unidimensional), para cada $\alpha \in \mathbb R^p$. La función característica de $Y_\alpha$ viene dada por
  \[
  \psi_{Y_\alpha}(t) = E\left[e^{itY_\alpha}\right] = E \left[e^{it(\alpha^T \vectX)}\right] \quad \text{para todo } t \in \mathbb R\,.
  \]
  En particular, si $t=1$
  \[
  \psi_{Y_\alpha}(1) = E\left[e^{i\alpha^T \vectX}\right] = \psi_{\vectX} (\alpha)
  \]
  Es decir:
  \[
  \psi_{\vectX}(t) = \psi_{Y_t}(1), \quad \forall t \in \mathbb R ^p
  \]
\end{proof}

\subsection{Momentos y cumulantes}

Sea $\vectX= (X_1,\dots,X_p)^T$ un vector aleatorio con función característica $\phi_{\vectX}$.
\begin{ndef}
  Se define el \textbf{momento} (no centrado) $p-$dimensional de orden $(r_1, \dots, r_p)$ de $X$ como:
  \[
\mu_{r_1,\dots,r_p}^{1,\dots,p} = E \left[ X_1^{r_1} \dots X_p^{r_p}\right]\,.
  \]
\end{ndef}

Los momentos se pueden obtener a partir de la función característica derivándose de su expansión de Taylor(respecto al origen).

\[
\phi_X(t) = E\left[e^{it^T X}\right] = E\left[\sum_{r=0}^\infty \frac{1}{r!}(i t^T X)^r\right] = \sum_{r= 0}^\infty \sum _{r_1+\dots + r_p = r} \mu_{r_1 \dots r_p}^{1 \dots p} \frac{(it_1)^{r_1} \dots (it_p)^{r_p}}{r_1 ! \dots r_p !}
\]
En particular, podemos enunciar este teorema:
\begin{nth}
  Si $E=\left[|X_1|^{m_1} \dots |X_p|^{m_p}\right] < \infty$, entonces la función característica de $X$ es $(m_1, \dots, m_p)$ veces diferenciable y , si $m = m_1 + \dots + m_p$,
  \[
\frac{\partial ^m}{\partial t_1^{m_1} \dots \partial t_p^{m_p}} \phi_X(t)| _{t = 0} = i^m \mu_{m_1}^1 \dots \mu_{m_p}^p
  .\]
\end{nth}

Consideremos el logaritmo de la función característica:
\[
\log \phi(t).
\]
\begin{ndef}
  Se definen los \emph{cumulantes} ($p-$dimensionales) de orden $(r_1 , \dots, r_p)$ como los coeficientes de la correspondiente expansión:
  \[
  \log \phi_X(t) =  \sum_{r= 0} \sum_{r_1+\dots + r_p = r} \mathcal K_{r_1 \dots r_P}^{1\dots p} \frac{(it_1)^{r_1} \dots (it_p)^{r_p}}{r_1 ! \dots r_p !}
  .\]
\end{ndef}


\subsubsection{Cambio de variables}

\begin{nth}
  Sea $\vectX = (X_1,\dots,X_n)^T$ un vector aleatorio con función de densidad $f_X(x)$ positiva sobre $S \subseteq \mathbb R^d$ y continua. Sea $Y = (Y_1,\dots,Y_p)^T$ un vector aleatorio con:
  \[
  Y = g(X) = (g_1(X),\dots ,g_p(X))^T
  \]
  con $g = (g_1,\dots,g_p): S \to \mathcal T$ biyectiva, por lo que $\exists g^{-1}$, y denotamos $g^{-1} = h = (h_1,\dots, h_p)^T : \mathcal T \to S$.

  Supongamos que existen las derivadas parciales:
  \[
    \frac{\partial h_i(y)}{\partial y_j} \quad (i,j = 1,\dots,p)
  \]
  y que son continuas sobre $\mathcal T$.

  Entonces, la función de densidad $f_Y(y)$ del vector $Y=(Y_1,\dots,Y_p)^T = y(X)$ viene dada por:
  \[
  f_Y(y) = f_X(g^{-1}(y))\det(J_{g^{-1}}(y))
  .\]
  %#TODO: Añadir determinante de jacobiana
\end{nth}

\begin{ncor}[Caso lineal]
  Sea $Y = BX + b$ con $B\in \mathscr{M}_{p}(\R)$ y no singular. En este caso,
  \[
  J_{g^{-1}}(g(x)) = \left[ J_{g}(x) \right]^{-1} = \det(B)^{-1} = \det\left(B^{-1}\right)
  \]
  y se tiene:
  \[
  f_Y(y) = f_X\left(B^{-1}(y-b)\right)\left|\det(B)^{-1}\right|
  .\]

\end{ncor}
\begin{nota}
  Al factorizar una matriz definida positiva, siempre existe una factorización $CC^T$ con $\det(C) > 0$.
  \end{nota}

\subsection{Distribución normal multivariante (en el caso $\Sigma > 0$)}
  En esta sección estudiamos la distribución normal multivariante, refiriéndonos siempre al caso en el que tiene matriz de covarianzas definida positiva.
Algunos aspectos que justifican el estudio de esta distribución en particular:
\begin{itemize}
\item es la distribución multivariante más estudiada, y existen infinidad de resultados acerca de la misma,
\item DNM es la base de muchas técnicas del análisis multivariante confirmatorio o inferencial,
\item DNM es una extensión \emph{natural} de la DNM univariante al caso multivariante,
\item las distribuciones marginales de cualquier orden en una DNM son también normales,
\item las distribuciones condicionadas (internamente) en una DNM también son normales y se obtienen de forma sencilla,
\item la familia de DNM es cerrada bajo transformaciones lineales,
\item la familia de DNM es cerrada bajo combinaciones lineales de vectores (mutuamente) independientes,
\item la DNM viene determinada por los momentos de primer y segundo orden (media y matriz de covarianzas),
  \item si dos subvectores de un vector aleatorio con DNM tienen correlaciones cruzadas nulas, entonces dichos subvectores son (mutuamente) independientes.

\end{itemize}


\begin{ndef}
  Sea $X = (X_1,\dots, X_p)^T$ un vector aleatorio. Se dice que $X$ tiene una distribución dormal $p-$variante si su densidad es de la forma:
  \[
f_X(x) = \frac{1}{(2\pi)^{p/2}\det(\Sigma)^{\frac{1}{2}}} \exp\left\{- \dfrac{1}{2}(x-\mu)^T \Sigma^{-1}(x-\mu)\right\}
\]
Siendo $\mu = (\mu_1, \dots, \mu_p)^T \in \mathbb R^p$ y $\Sigma$ una matriz escalar $p\times p$ simétrica y definida positiva.
\end{ndef}

Veamos que, en efecto, $f_X$ es una función de densidad
\begin{enumerate}
\item $f_X(x) \geq 0, \ \forall x \in \mathbb R$,
\item $\int_{\mathbb R^p} f_X(x) \mathrm{d}x = 1$. En efecto, como $\Sigma$ es una matriz simétrica y definida positiva, podemos descomponerla de la forma $\Sigma = C C^T$ con $C \in \mathcal \mathscr{M}_{p}(\R), \det(C) > 0$. La forma de la densidad sugiere el cambio de variable $Z = C^{-1}(x-\mu)$.  El determinante del jacobiano de la transformación es $\det\left(C^{-1}\right)$. Por otro lado:

\[
  (x-\mu)^T \Sigma^{-1}(x-\mu) = zz^T
\]

Por tanto, podemos escribir:

  \[
    \begin{split}
      \int_{\mathbb R^p} \frac{1}{(2\pi)^{p/2}\det(\Sigma)^{\frac{1}{2}}} \exp\left\{- \dfrac{1}{2}(x-\mu)^T \Sigma^{-1}(x-\mu)\right\} \mathrm{d}x \\
      =_{c.v.} \int _{\mathbb R^p} \frac{1}{(2\pi)^{\frac{p}{2}}\det(C)^{\frac{1}{2}} \det(C^T)^{\frac{1}{2}}} \exp\left\{ -\frac{1}{2}zz^T\right\} \det(C) \mathrm{d}z\\ = \int_{\mathbb R^p} \frac{1}{\left(\sqrt{2\pi}\right)^p}\exp\left\{-\frac{1}{2}\sum_{j=1}^p z_j^2\right\} \mathrm{d}z_1 \dots \mathrm{d}z_p = \prod_{j=1}^p \int_{\mathbb R} \frac{1}{\sqrt{2\pi}} \exp\left\{- \frac{1}{2} z^2\right\}\mathrm{d}z_j =^{(1)} \prod_{j=1}^p 1 = 1 
  \end{split}
  ,\]
  donde en $(1)$ hemos usado que cada integral es la densidad de una distribución normal univariante estándar ($N(0,1)$).
\end{enumerate}


\subsubsection{Vector de medias y matriz de covarianzas}
Del cálculo anterior se desprende que el vector aleatorio $$Z = C^{-1}(X-\mu)$$ se distribuye con función de densidad
\[
f_Z(z) = \prod_{j = 1}^p \frac{1}{\sqrt{2\pi}}exp\{-\frac{1}{2}z_i^2\} = \prod_{j=1}^p f_{Z_j}(z_j)
\]
Es decir, para cada $j = 1,\dots, p$
\[
Z_j \sim N(0,1)
\]
siendo $Z_1,\dots,Z_p$ independientes( en particular, incorreladas) $Z \sim N_p(0,I_p)$

A partir de esto, probamos que $\mu$ y $\Sigma$ representan, respectivamente, el vector de medias y la matriz de covarianzas del vector aleatorio $X \sim N_p(\mu,\Sigma)$

\subsubsection{Propiedades}
\begin{enumerate}
\item Sobre cambio de variable Lineal. Sea $X \sim N_p(\mu,\Sigma)$, con $\Sigma > 0$ e $Y = BX+b$ con $B$ matriz escalar $p\times p$ no singular y $b$ un vector aleatorio escalar $p\times 1$. Entonces, se tiene que:
  \[
Y \sim N_p(B\mu+b, B \Sigma B^T)
  \]
\end{enumerate}
