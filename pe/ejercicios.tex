\section{Tema 1}

\subsection{Ejercicios mandados}

\begin{ejer}
  Definir la distribución de probabilidad, función de distribución y función característica de una variable aleatoria y expresar dichas funciones en términos de la función masa de probabilidad o la función de densidad, según que la variable sea de tipo discreto o continuo, respectivamente.
\end{ejer}

\begin{sol}
  La \emph{distribución de probabilidad} de una variable aleatoria es una función de probabilidad definida en el espacio de Borel:
  \begin{align*}
    P_X : \B & \to [0, 1] \\
    B & \mapsto P_X(B) = P(X^{-1}(B)) = P[X \in B]
  \end{align*}

  La \emph{función de distribución} es la función de puntos definida como:
  \begin{align*}
    F: \R & \to [0, 1] \\
    x & \mapsto  F_X(x) = P_X((-\infty, x]) = P[X \leq x]
  \end{align*}

  La \emph{función característica} es la función definida como
  \begin{align*}
    \varphi_X: \R & \to \mathbb{C} \\
    t & \mapsto \varphi_X(t) = E[e^{itX}]
  \end{align*}

  \begin{itemize}
    \item Caso discreto:
    $$\forall B \in \B, \; P_X(B) = \sum \limits_{x \in B \cap E_X} P[X = x] = \sum \limits_{x \in B \cap E_X} f(x) $$

    $$\forall x \in \R, \; F_X(x) = P_X[(-\infty, x]] = \sum \limits_{x \in E_X, x \leq y} P[X = x] = \sum \limits_{x \in E_X, x \leq y} f(x)$$

    $$\forall t \in \R, \; \varphi_X(t) = E[e^{itX}] = \sum \limits_{x \in E_X} e^{itx}P[X = x] = \sum \limits_{x \in E_X} e^{itx} f(x)$$

    \item Caso continuo:
    $$\forall B \in \B, \; P_X(B) = \int_\B f(x) dx$$

    $$\forall x \in \R, F_X(x) = P_X[(-\infty,x]] = P[X \in (-\infty, x]] = P[X \leq x] = \int^x_{-\infty} f(y)dy$$

    $$\forall t \in \R, \; \varphi_X(t) = E[e^{itX}] = \int_\R e^{itx} f(x) dx$$
  \end{itemize}
  Donde $f(x)$ representa la función masa de probabilidad en el caso discreto, y la función densidad en el caso continuo.
\end{sol}

\begin{ejer}\label{ej:distributionproof}
  Demostrar que la distribución de probabilidad de una variable aleatoria es una medida de probabilidad definida sobre la $\sigma$-álgebra de Borel.
\end{ejer}

\begin{sol}
  Veamos si
  \begin{align*}
    P_X : \B & \to \R \\
    B & \mapsto P_X(B) = P(X^{-1}(B)) = P[X \in B]
  \end{align*}
  cumple con los tres Axiomas de Kolmogorov, usando que P es una probabilidad:

  \begin{nlist}
    \item $P_X(B) = P[X \in B] \geq 0$, $\forall B \in \B$.
    \item $\forall \{B_n\}_{n \in \N} \subset \B$ disjuntos $$ P_X(\bigcup \limits^\infty_{n = 1} B_n) = P[X \in \bigcup \limits^\infty_{n = 1} B_n] = P[\bigcup \limits^\infty_{n = 1} (X \in B_n)] = \sum \limits^\infty_{n = 1} P[X \in B_n] = \sum \limits^\infty_{n = 1} P_X(B_n)$$
    \item $P_X(\R) = P[X^{-1}(\R)] = P[\W] = 1$
  \end{nlist}
\end{sol}

\begin{ejer}
  Demostrar la caracterización de vectores aleatorios mediante variables aleatorias.
\end{ejer}

\begin{sol}
  Sea $X = (X_1, \ldots, X_n) : (\W, \A, P) \to (\R^n, \B^n)$, entonces $X$ es vector aleatorio $\iff$ $X_1, \ldots, X_n$ son variables aleatorias.

  \begin{multline*}
    X \text{ v.a} \iff \forall x = (x_1, \ldots, x_n) \in \R^n, \; X^{-1}((-\infty, x]) = [X \leq x] = \\ =  [X_1 \leq x_1, \ldots, X_n \leq x_n] = \bigcap \limits^n_{i = 1} [X_i \leq x_i] \subset \A \iff \forall i \in \{1, \ldots, n\}, \; \forall x_i \in \R \\ \A \supset [X_i \leq x_i] = X^{-1}((-\infty, x_i]) \iff X_1, \ldots, X_n \text{ v.a }
  \end{multline*}

  Vemos la penúltima implicación tomando $x_j = \infty \forall j \neq i$ en $\bigcap \limits^n_{i = 1} [X_i \leq x_i] \subset \A \implies [X_i \leq x_i] \subset \A$, y al revés sabiendo que $\bigcap \limits^n_{i = 1} [X_i \leq x_i] \subset [X_i \leq x_i] \subset \A$.
\end{sol}

\begin{ejer}
  Sean $X$ e $Y$ variables aleatorias independientes tales que $E[X] = 0$, $Var(X) = 1$ y la variable $Y$ tiene distribución uniforme en $[-\pi, \pi]$. Consideremos el proceso estocástico $\{X_t\}_{t \geq 0}$ definido por

  $$X_t = X \cos(t + Y)$$

  Calcular las funciones media y covarianza y decir si el proceso es débilmente estacionario.
\end{ejer}

\begin{sol}
  Primero veamos que efectivamente $\{X_t\}_{t \geq 0}$ es un proceso estocástico. Para $t \geq 0$, tenemos que $X_t : (\W, \A, P) \to (\R, \B)$, luego falta ver que es medible, pero funciones Borel de variables aleatorias son medibles y el producto de funciones medibles es medible.

  Función media: $\mu(t) = E[X_t] = E[X \cos(t + Y)]$, como $\cos(t + Y)$ es función medible por composición de funciones medibles junto a que $X$ y $Y$ son independientes, entonces $X$ y $\cos(t + Y)$ son independientes; luego $E[X \cos(t + Y)] = E[X] E[\cos(t + Y)] = 0$. Luego $\mu(t) = 0$, $\forall t \geq 0$.

  Función covarianza: $C(s,t) = R(s,t) - \mu_t \mu_s = R(s,t) = E[X_t X_s] = E[X^2 \cos(t + Y) \cos(s + Y)]$, por la independencia otra vez, $E[X^2 \cos(t + Y) \cos(s + Y)] = E[X^2] E[\cos(t + Y) \cos(s + Y)] = Var(X) E[\cos(t + Y) \cos(s + Y)] = E[\cos(t + Y) \cos(s + Y)]$, resolvemos la integral $\frac{1}{2\pi}\int^\pi_{-\pi} \cos(t + y) \cos(s + y) dy$ y tenemos que $C(s,t) = \frac{1}{2} \cos(s - t)$.


  Este pe. es débilmente estacionario ya que su función media es constante, $\mu(t) = 0$, $\forall t \geq 0$; y la función covarianza cumple $C(s,t) = \frac{1}{2} \cos(s - t) = \frac{1}{2} \cos(s + h - t - h) = C(s + h, t + h) = C(0, t - s)$, $\forall t,s \geq 0$.
\end{sol}

\begin{ejer}
  Ejemplos de procesos estocásticos atendiendo a la clasificación según el espacio de estados y espacio paramétrico.
\end{ejer}

\begin{sol}
  Ejemplos:

  \begin{itemize}
    \item PDTD: Nº de productos fabricados al día $\equiv X_n$

    $T \equiv \N$(días), $E = \N \cup \{0\}$(nº productos).
    \item PCTD: Nota media por curso $\equiv X_n$

    $T = \{1,2,3,4,5\}$(curso), $E = [0, 10]$(nota media).
    \item PDTC: Nº de lanzamientos hechos en el instante $t \equiv X_t$

    $T = \R^+_0$(tiempo), $E = \N \cap \{0\}$(lanzamientos).
    \item PCTC: Temperatura en una ciudad en el instant $t \equiv X_t$

    $T = \R^+_0$(tiempo), $E = \R^+_0$(Kelvin).
  \end{itemize}
\end{sol}

\begin{ejer}
  Demostrar que si las componentes del vector aleatorio $X = (X_1, \ldots, X_n)^T$ son independientes, entonces la funcion característica del vector $X$ es igual al producto de las funciones características de las variables $X_k$, $k = 1, \ldots, n$.
\end{ejer}

\begin{sol}
  Sea un vector aleatorio $X = (X_1, \ldots, X_n)$, la función característica del vector $X$ es $\varphi_X(t) = E[e^{it^TX}] = E[e^{i(t_1 X_1 + \ldots + t_n X_n)}] = E[e^{i t_1 X_1 + \ldots + i t_n X_n}] = E[\prod \limits^n_{k = 1} e^{i t_k x_k}] = \prod \limits^n_{k = 1} E[e^{i t_k x_k}] = \prod \limits^n_{k = 1} \varphi_{X_k} (t_k)$. Podemos separar las esperanzas ya que $X_1, \ldots, X_n$ son independientes y $e^{itx}$ es función Borel de variable aleatoria; y por tanto $e^{itX_k}, \; k = 1, \ldots, n$, siguen siendo independientes.
\end{sol}

\begin{ejer}
  Demostrar que para un proceso con incrementos independientes las distribuciones finito dimensionales del proceso (esto es, las distribuciones de los vectores ($X_{t_1}, X_{t_2}, \ldots, X_{t_n})^T$, $\forall t_1 < t_2 < \ldots < t_n$) están determinadas por las distribuciones marginales unidimensionales ($X_{t_1}$) y por las distribuciones de los incrementos ($X_{t_i} - X_{t_1 - 1}$, $i = 2, \ldots, n$).
\end{ejer}

\begin{sol}
  Tenemos un $\{X_t\}_{t \in T}$ p.e con incrementos independientes, entonces $\forall t_1 < \ldots < t_n \in T$, $X_{t_1}, X_{t_2} - x_{t_1}, \ldots, X_{t_n} - X_{t_{n-1}}$ son v.a independientes.

  Definimos $S_1 = X_{t_1}$, $S_2 = X_{t_2} - X_{t_1} \implies X_{t_2} = S_1 + S_2$

  $S_3 = X_{t_3} - X{t_2} \implies X_{t_3} = S_1 + S_2 + S_3$, por tanto $X_{t_k} = \sum \limits^k_{j = 1} S_j$.

  $\phi_{(X_{t_1}, \ldots, X_{t_n})}(u_1, \ldots, u_n) = \phi_{(s_1, \ldots, s_n)}(v_1, \ldots, v_n)$

\end{sol}

\begin{ejer}
  Demostrar que la clase de rectángulos medibles $\C^\N$ es una semi-álgebra.
\end{ejer}

\begin{sol}
  Tenemos que $$\C^\N = \{\prod \limits^\infty_{i = 1} A_i : n \in \N, \;A_i \in \B, \; A_{j} = \R, \; 1 \leq i \leq n, \; j > n\}$$.

  Veamos que $\C^\N$ es una semi-álgebra.
  \begin{nlist}
    \item $\R^\N \in \C^\N$.

    Como $\R \in \B$ ($\B$ es $\sigma$-álgebra) $\implies \R^\N \in \C^\N$.

    \item $\forall A, B \in \C^\N \implies A \cap B \in \C^\N$.

    Sean $A, B \in \C^\N$, $A \cap B = (\prod \limits^\infty_{n = 1} A_n) \cap (\prod \limits^\infty_{n = 1} B_n) = \prod \limits^\infty_{n = 1} (A_n \cap B_n) \in \C^\N$, ya que $A_i, B_i \in \B \implies A_i \cap B_i \in \B$ (por ser $\B$ $\sigma$-álgebra).

    \item $\forall A, B \in \C^\N$, $\exists \{C_i\}^n_{i = 1} \subset \C^\N$ disjuntos dos a dos tal que $A \setminus B = \bigcup \limits^n_{i = 1} C_i$.

    Sean $A, B \in \C^\N$, $A \setminus B = (\prod \limits^\infty_{n = 1} A_n) \setminus(\prod \limits^\infty_{n = 1} B_n) = \prod \limits^\infty_{n = 1} (A_n \setminus B_n) = C \in \C^\N$, ya que $A_i \setminus B_i = A_i \cap \overline{B_i} \in \B$ ($\B$ es $\sigma$-álgebra).
  \end{nlist}
\end{sol}

\begin{ejer}
  Demostrar el Teorema de medibilidad para procesos estocásticos en tiempo discreto.
\end{ejer}

\begin{sol}
  Sea la función \begin{align*}
    \chi : (\W, \A) & \to (\R^\N, \B^\N) \\
    \w & \mapsto X(\w) = \{X_n(\w)\}_{n \in \N}
  \end{align*}

  Entonces $\chi$ es medible $\iff \{X_n\}_{n \in \N}$ son medibles.

  $\boxed \implies$


  $\boxed \impliedby$
\end{sol}


\subsection{Ejercicios en clase}

\begin{ejer}
  Sea el espacio de probabilidad $([-1, 1], \B_{[-1, 1]}, U[-1, 1])$ y

  $$\forall n \in \N, \; X_n(\w) =
    \begin{cases}
      -1, \w \in [-1, \frac{-1}{n}] \\
      0, \w \in [\frac{-1}{n}, \frac{1}{n}] \\
      1, \w \in [\frac{1}{n}, 1]
    \end{cases}$$

  Y las trayectorias:

  \begin{equation*}
    \w \in [-1, 0) \rightarrow \chi(\w) =
  \begin{cases}
        0, & n \leq \frac{-1}{\w} \\
        -1, & n \geq \frac{-1}{w}
  \end{cases}
  \end{equation*}

  \begin{equation*}
    \w \in [0, 1] \rightarrow \chi(\w) =
  \begin{cases}
        0, & n < \frac{1}{\w} \\
        1, & n \geq \frac{1}{w}
  \end{cases}
  \end{equation*}
\end{ejer}

\begin{sol}
  Los pasos a seguir son:
  \begin{nlist}
    \item Demostrar que tenemos un PETD (p.e)
    \item Obtener las trayectorias
    \item Obtener la distribución basandonos en la trayectoria
    \item Ver que la distribución queda dada por distribuciones finito dimensionales
  \end{nlist}

  Veamos la distribución:

  $P[X_1 = 0] = P((-1, 1)) = 1$

  $P[X_1 = 0, X_2 = 0] = P[(-1, 1) \cap (\frac{-1}{2}, \frac{1}{2})] = P[(\frac{-1}{2}, \frac{1}{2})] = \frac{1}{2}$

  $P[X_1 = 0, X_2 = 1] = P[(-1, 1) \cap [-1, \frac{-1}{2})] = P[(-1, \frac{-1}{2}]] = \frac{1}{4}$

  $P[X_1 = 0, X_2 = 2] = P[(-1, 1) \cap (\frac{1}{2}, 1]] = P[[\frac{1}{2}, 1]] = \frac{1}{4}$


  $P[X_1 = 0, \ldots, X_n = 0] = P[(-1, 1) \cap (\frac{-1}{2}, \frac{1}{2}) \cap \ldots \cap (\frac{-1}{n}, \frac{1}{n})] = P((\frac{-1}{n}, \frac{1}{n})) = \frac{1}{n}$.

  $P[X_1 = 0, \ldots, X_{k-1} = 0, X_k = -1, \ldots, X_n = -1] = P((-1, 1) \cap \ldots \cap (\frac{-1}{k -1}, \frac{1}{k-1}) \cap [-1, \frac{-1}{k}] \cap \ldots \cap [-1, \frac{-1}{n}]) = P((\frac{-1}{k-1}, \frac{1}{k-1}) \cap [-1, \frac{-1}{k}]) = \frac{1}{2k(k-1)}$

  $P[X_1 = 0, \ldots, X_{k-1} = 0, X_k = 1, \ldots, X_n = 1] = P((-1, 1) \cap \ldots \cap [\frac{1}{k-1}, 1]  \cap \ldots \cap [\frac{1}{n}, 1]) = P((\frac{-1}{k-1}, \frac{1}{k-1}) \cap [\frac{1}{k}, 1]) = \frac{1}{2k(k-1)}$

  Comprobamos que es una probabilidad (viendo que suma):

  $$\frac{1}{n} + \sum \limits^n_{k = 2} \frac{1}{2k(k-1)} + \sum \limits^n_{k = 2} \frac{1}{2k(k-1)} = \frac{1}{n} + \sum \limits^n_{k = 2} \frac{1}{k(k-1)}, \; k = 2, 3, \ldots, n$$

  Por ejemplo por $n = 3, 4$:

  $n = 3 \rightarrow \frac{1}{3} + \frac{1}{2} + \frac{1}{6} = 1$.

  $n = 4 \rightarrow \frac{1}{4} + \frac{1}{2} + \frac{1}{6} + \frac{1}{12} = 1$.
\end{sol}

\subsection{Ejercicios PETC}

\begin{ejer}
  Sea $(\W, \A, P)$ un espacio probabilístico, con $\W = [0,1]$, $\A = \B_{[0,1]}$, y $P$ la distribución uniforme en $[0,1]$. Definimos $\{X_n\}_{n>4}$ sobre $(\W, \A, P)$ por

  \begin{equation*}
    \forall n > 4, \; X_n(\w) = \begin{cases}
      0, \text{ si  } 0 \leq \w < \frac{1}{4} - \frac{1}{n} \\
      1, \text{ si  } \frac{1}{4} - \frac{1}{n} \leq \w < \frac{1}{2} - \frac{1}{n} \\
      2, \text{ si  } \frac{1}{2} - \frac{1}{n} \leq \w < \frac{3}{4} - \frac{1}{n} \\
      3, \text{ si  } \frac{3}{4} - \frac{1}{n} \leq \w \leq 1
    \end{cases}
  \end{equation*}

  Demostrar que $\{X_n\}_{n>4}$ es un proceso estocástico, calcular sus trayectorias y su distribución.
\end{ejer}

\begin{sol}
  Veamos primero que es un proceso estocástico (las $X_n$ son medibles):

  Como $X_n = 1_{[\frac{1}{4} - \frac{1}{n}, \frac{1}{2} - \frac{1}{n}]} + 2_{[\frac{1}{2} - \frac{1}{n}, \frac{3}{4} - \frac{1}{n}]} + 3_{[\frac{3}{4} - \frac{1}{n}, 1]}$, es decir, es suma de funciones indicadoras de conjuntos medibles, por tanto medibles.

  Definimos las trayectorias:
  \begin{align*}
    \chi : \W & \to \R^\N \\
    \w & \mapsto \{X_n(\w)\}_{n>4}
  \end{align*}
  Tenemos que:
  \begin{itemize}
    \item $\chi(0) = \{0, 0, \ldots, 0, \ldots \}$
    \item $\chi(\frac{1}{4}) = \{1, 1, \ldots, 1, \ldots\}$
    \item $\chi(\frac{1}{2}) = \{2, 2, \ldots, 2, \ldots\}$
    \item $\w \in [\frac{3}{4}, 1] \implies \chi(\w) = \{3\}_{n>4}$
    \item $\w \in (0, \frac{1}{4}) = (0, \frac{1}{4} - \frac{1}{n}) \cup [\frac{1}{4} - \frac{1}{n}, \frac{1}{4}]$

      \begin{itemize}
        \item $\w < \frac{1}{4} - \frac{1}{n} \iff \frac{1}{n} < \frac{1}{4} - \w \iff n > \frac{4}{1 - 4\w} \implies X_n(\w) = 0$
        \item $\w \geq \frac{1}{4} - \frac{1}{n} \iff n \leq \frac{4}{1-4\w} \implies X_n(\w) = 1$
      \end{itemize}
    Por tanto

    \item $\w \in (\frac{1}{4}, \frac{1}{2}) = (\frac{1}{4}, \frac{1}{2}-\frac{1}{n}) \cup [\frac{1}{2} - \frac{1}{n}, \frac{1}{2})$
    \begin{itemize}
      \item $\w < \frac{1}{2} - \frac{1}{n} \iff n > \frac{2}{1 - 2\w} \implies X_n(\w) = 1$
      \item $\w \geq \frac{1}{2} - \frac{1}{n} \iff n \geq \frac{2}{1-2\w} \implies X_n(\w) = 2$
    \end{itemize}
    Por tanto $\forall \w \in (\frac{1}{4}, \frac{1}{2}), \; \chi(\w) = $
  \end{itemize}

  Ahora obtendremos la distribución, que está determinada por las distribuciones finito dimensionales:

  \begin{itemize}
    \item $P[X_5 = 0, X_6 = 0, \ldots, X_n = 0] = P[(0, \frac{1}{4} - \frac{1}{5} \cap \ldots \cap (0, \frac{1}{4} - \frac{1}{n})] = P[(0, \frac{1}{4} - \frac{1}{5})] = \frac{1}{4} - \frac{1}{5} = \frac{1}{20}$
    \item $P[X_5 = 1, X_6 = 1, \ldots, X_n = 1] = P[(\frac{1}{4} - \frac{1}{5}, \frac{1}{2} - \frac{1}{5}) \cap \ldots \cap (\frac{1}{4} -\frac{1}{n}, \frac{1}{2} - \frac{1}{n})] = P[(\frac{1}{4} - \frac{1}{n}, \frac{1}{2} - \frac{1}{5})] = \frac{1}{n} + \frac{1}{20}$
    \item $P[X_5 = 2, \ldots, X_n = 2] = P[C_5 \cap \ldots \cap C_n] = P[(\frac{1}{2} - \frac{1}{n}, \frac{3}{4} - \frac{1}{5})] = \frac{1}{20} + \frac{1}{n}$
    \item $P[X_5 = 3, \ldots, X_n = 3] = P[D_5 \cap \ldots D_n] = P[(\frac{3}{4} - \frac{1}{n}, 1)] = \frac{1}{4} + \frac{1}{n}$
    \item $P[X_5 = 1, X_6 = 1, \ldots, X_{k-1} = 1, X_k = 0, \ldots, X_n = 0] = P[B_5 \cap B_6 \cap \ldots \cap B_{k-1} \cap A_k \cap \ldots \cap A_n] = P[(\frac{1}{4} - \frac{1}{k-1}, \frac{1}{4} - \frac{1}{k})] = \frac{1}{(k-1)k}$
    \item $P[X_5 = 2, X_6 = 2, \ldots, X_{k-1} = 2, X_k = 1, \ldots, X_n = 1] = P[C_5 \cap \ldots \cap C_{k-1} \cap B_k \cap \ldots \cap B_n] = P[(\frac{1}{2} - \frac{1}{k-1}, \frac{3}{4} - \frac{1}{5}) \cap (\frac{1}{4} - \frac{1}{n}, \frac{1}{2} - \frac{1}{k})] = P[(\frac{1}{2} - \frac{1}{k-1}, \frac{1}{2} - \frac{1}{k})] = \frac{1}{(k-1)k}$
    \item $P[X_5 = 3, \ldots, X_{k-1} = 3, X_k = 2, \ldots, X_n = 2] = P[D_5 \cap \ldots \cap D_{k-1} \cap C_k \cap \ldots \cap C_n] = P[(\frac{3}{4} - \frac{1}{k-1}, \frac{3}{4} - \frac{1}{k})] = \frac{1}{(k-1)k}$
  \end{itemize}

  Comprobamos que las probabilidades suman uno:

  \begin{multline*}
    \frac{1}{20} + (\frac{1}{n} + \frac{1}{2}) + (\frac{1}{n} + \frac{1}{20}) + (\frac{1}{n} + \frac{1}{20}) + (\frac{1}{4} + \frac{1}{n}) + 3 \sum \limits^n_{k = 6} \frac{1}{(k-1)k} = \\ = \frac{3}{n} + \frac{2}{5} + 3 \sum \limits^n_{k = 6} \frac{1}{(k-1)k}, \; n > 4
  \end{multline*}

  Tomamos valores de $n$ y lo comprobamos fácilmente, por ej $n = 5, 6, \ldots, 10, 50$.
\end{sol}
