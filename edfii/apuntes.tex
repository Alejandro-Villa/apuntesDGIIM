\section{Ecuaciones lineales: teoremas de existencia y unicidad.}


\begin{ndef}[Ecuación diferencial lineal]
Sea $(\alpha,\beta)\subseteq \R$ un intervalo abierto y sean $A:(\alpha,\beta)\to \M_d(\R)$ y $b:(\alpha,\beta)\to \R^d$ funciones continuas. Entonces una ecuación diferencial lineal es de la forma:

\begin{equation}
x'=A(t)x+b(t) \tag{C} \label{lin:completa}
,\end{equation}

y se dice \emph{completa}, o bien

\begin{equation}
x'=A(t)x \tag{H} \label{lin:homogenea}
,\end{equation}

y en este caso se dice \emph{homogénea}.
\end{ndef}

\begin{nth}[Teorema de existencia y unicidad de la solución]
Dados $t_0\in(\alpha,\beta)$ y $x_0 \in \R^d$ y consideramos el \emph{problema de valores intermedios} (PVI):

\begin{equation}
\left\{\begin{array}{rl}
x' &=A(t)x+b(t), \\
x(t_0)&=x_0.
\end{array}\right.
\tag{PVI}\label{lin:PVI}
\end{equation}

Entonces, existe una única solución $\varphi : (\alpha,\beta)\to\R^d$, con $\varphi \in \C ^ 1(\R)$, que verifica

\[
\varphi'(t) = A(t)\varphi(t) + b(t) \qquad \forall t \in (\alpha,\beta)
\]

y que además cumple $\varphi(t_0)=x_0$, es decir, una única solución de \ref{lin:PVI}.
\end{nth}

Aplicando el teorema al PVI

\[
\begin{cases}
  x' = Ax, \\
  x(t_0) = e_j.
\end{cases}
\tag{PVI$_j$} \label{lin:PVIj}
\]

para cada $j=1,\dots,d$, obtenemos soluciones $\varphi_j$. Sea $x_0\in \R^d$, y consideramos $\varphi = \sum_{j=1}^d \langle x_0,e_j \rangle\varphi_j$.
Es inmediato comprobar que $\varphi$ es solución de \eqref{lin:PVI} con $b=0$. Por tanto, hemos probado:

\begin{ncor}
  Las soluciones de \eqref{lin:homogenea} son un espacio vectorial de dimensión $d$.
\end{ncor}

Ahora, consideremos $\varphi$ la solución de \eqref{lin:PVI}, para $x_0\in\R^d$ arbitrario, es decir, una solución cualquiera de \eqref{lin:completa}.
Sean $\varphi_c$ la solución de

\begin{equation}
\left\{\begin{array}{rl}
x' &=A(t)x+b(t), \\
x(t_0)&=x_0',
\end{array}\right.
\end{equation}

con $x_0'\in\R^d$ arbitrario, y $\varphi_h$ la de

\begin{equation}
\left\{\begin{array}{rl}
x' &=A(t)x, \\
x(t_0)&=x_0 - x_0',
\end{array}\right.
\end{equation}

y ahora es rutinario comprobar que $\varphi = \varphi_c + \varphi_h$. Además, para cualquier otra solución $\psi_{h}$ de \eqref{lin:homogenea}, $\varphi_c + \psi_h$
es solución de \eqref{lin:completa}.
En esta ocasión, hemos probado:

\begin{ncor}
  El conjunto $S_c$ de soluciones de \eqref{lin:completa}, dada cualquier solución $\varphi$ de la misma, es el espacio afín

  \[
    S_c = \varphi + S_h,
    \]

    con $S_h$ el conjunto de soluciones de \eqref{lin:homogenea}.
\end{ncor}

Otra consecuencia inmediata de la aplicación del teorema a \ref{lin:PVIj} es la siguiente:

\begin{ncor}[Existencia y unicidad de m.f.p.]
  Existe una única función de clase 1 $\Phi: (\alpha, \beta) \to \M_d(\R)$ tal que $\Phi' = A\Phi$, y
  además $\Phi(t_0) = I_d$.
\end{ncor}

\section{Estabilidad en el sentido de Lyapunov.}

En esta sección, con arreglo a definir los conceptos básicos de la teoría de estabilidad
de Lyapunov, consideramos un problema de valores iniciales más general:

\begin{equation}
\left\{\begin{array}{rl}
x' &=f(t,x), \\
x(t_0)&=x_0,
\end{array}\right.
\tag{P} \label{est:PVI}
\end{equation}

con $D \subseteq \R \times \R^d$, $f: D \to \R^d$ continua y tal que haya unicidad de soluciones en \ref{est:PVI}.
Notamos, para cada $x_0\in \R^d$ tal que $(t_0,x_0) \in D$, $\varphi_{x_0} : (\alpha, +\infty) \to \R^d$ a la solución maximal de \ref{est:PVI}.

\begin{ndef}
  $\varphi_{x_0}$ es \emph{estable} si, para cada $\varepsilon > 0$, existe $\delta > 0$ tal que

  \[
    (t_0, \bar{x_0}) \in D \text{ y } \|x_0 - \bar{x_0}\| < \delta \implies 
  \begin{cases}
    \text{ $\varphi_{\bar{x_0}}$ está definida en $(t_0, +\infty)$}, \\
    \|\varphi_{x_0}(t) - \varphi_{\bar{x_0}}(t)\| < \varepsilon \text{ para cada $t \ge t_0$}.
  \end{cases}
  \]

  $\varphi_{x_0}[t_0, +\infty)+(-\varepsilon, \varepsilon)$ se llama \emph{entorno tubular} de $\varphi_{x_0}$.
\end{ndef}

\begin{ndef}
  Una solución se dice \emph{inestable} si no es estable.
\end{ndef}

%%% TODO: poner algunos ejemplos de PVI con soluciones estables e inestables.

Negando la definición de estabilidad, obtenemos:

\begin{nprop}[Caracterización de soluciones inestables]
  $\varphi_{x_0}$ es inestable si, y solo si, existen $\varepsilon_0 > 0, \xn \subseteq\R^d$ con
  $(t_0,x_n)\in D$, $x_n \to x_0$ y existe, para cada $n\in\N$, $t_n \ge t_0$ tal que

  \[
    \| \varphi_{x_n}(t_n) - \varphi_{x_0}(t_n) \| \ge \varepsilon_0
  .\]
\end{nprop}

%% CLASE LUNES 17

\begin{nprop}
Las siguientes afirmaciones son equivalentes:
\begin{nlist}
\item Todas las soluciones de \eqref{lin:completa} son atractores.
\item Existe una solución de \eqref{lin:completa} que es un atractor.
\item La solución trivial $y = 0$ de \eqref{lin:homogenea} es un atractor.
\item Todas las soluciones de \eqref{lin:homogenea} convergen hacia el vector 0 cuando $t \to +\infty$.
\item La matriz fundamental de \eqref{lin:homogenea} principal en $t_0$ converge hacia la matriz 0 cuando $t \to +\infty$.
\end{nlist}
\end{nprop}

\begin{ncor}
Los atractores de la ecuación \eqref{lin:homogenea} son asintóticamente estables.
\end{ncor}

\begin{ndef}
\begin{itemize}
\item Se dice que la ecuación \ eqref{completa} es estable si todas su soluciones son estables
\item Se dice que la ecuación \ eqref{completa} es asintóticamente estable si todas sus soluciones son asintóticamente estables.
\end{itemize}
\end{ndef}

No siempre podemos calcular la matriz fundamental, pero nosotros nos centraremos en el caso escalar donde sí podemos calcularla.

\subsection{Estabilidad de ecuaciones lineales escalares}
Sean $a,b:(\alpha,+\infty)\to \R$ continuas. Se considera la ecuación diferencial lineal escalar \eqref{completa} y sea $t_0\in(\alpha,+\infty)$.
La matriz fundamental principal en $t_0$ en $\phi(t)=exp(\int^t_{t_0} a(s)ds)$ y por tanto podemos caracterizar la estabilidad de \eqref{completa} controlando una primitiva del coeficiente $a(t)$.
\begin{nth}{Proposicion 3}
\begin{itemize}
\item La ecuación de \ eqref{completa} es estable sii la función $a(t)$ tiene una primitiva acotada superiormente en $[t_0,+\infty]$.
\item La ecuación \ eqref{completa} es a.e. sii la función $a(t)$ tiene una primitiva que converge hacia $-\infty$ [....]
\end{itemize}
\end{nth}
Ejemplo 1:
- Inestable
- A.E.
- estable
-
- Inestable
- estable
- estable
-


Estable $-> \lambda <= 0$
A.E. $-> \lambda < 0$
Inestable $-> \lambda > 0$

\subsection{Estabilidad de ecuaciones lineales con coeficientes constantes}
Sea $A \in \M_d(\R)$ una matriz cuadrada.  Usaremos la siguiente notación:
NOTACIÓN:
\begin{ndef}{Espectro de $A$}
El espectro de $A$ es el conjunto de valores propios de $A$, tanto reales como complejos, $\sigma(A)={\lambda_1,...,\lambda_d}$, contados con su multiplicidad en nuestro caso.
\end{ndef}
\begin{ndef}{Multiplicidad del valor propio $\lambda_j$}
$m(\lambda_j) \forall \lambda_j \in \sigma{A}$
\end{ndef}
\begin{ndef}{Dimensión de cada subespacio propio $E_{\lambda_j}$}
$dim E_{\lambda_j} = dim ker(A-\lambda_{jI})= d-rango(A-\lambda I)$
\end{ndef}
\begin{ndef}
Los valores propios de A cuya parte real es 0 son:
$\sigma_0(A)={\lambda \in \sigma(A) \ Re(\lambda)=0}$
\end{ndef}

Consideramos la EDO lineal homogénea y autónoma $x'=Ax x\in\R^d$. El principal indicador para determinar la estabilidad de * es el máximo de las partes reales de los valores propios de A:
$\mu(A)=máx{Re(\lambda : \lambda\in\sigma(A))}$

\begin{nth}
\begin{enumerate}
\item Si $\mu(A)<0$ entonces la EDO lineal * es A.E.
\item Si $\mu(A)=0$ y $m(\lambda)=dimE_\lambda \forall \lambda \in \sigma_0(A)$, entonces la EDO lineal * es estable (pero no A.E.)
\item Si $\mu(A)=0$ y $\exists \lambda \in \sigma_0(A)$ tal que $m(\lambda) != dim E_lambda$, entonces la EDO lineal * es inestable.
\item Si $\mu(A)>0$ entonces la EDO lineal * es inestable.
\end{enumerate}
\end{nth}

A continuación, haremos una serie de ejemplos y proseguiremos con un poquito de teoría para poder producir la demostración de este enunciado.
Ejemplo 1:
{
\begin{equation}
\left\{\begin{array}{lcl}
x_1'=2x_1+3x_2\\
x_2'=5x_1-x_2
\end{array}\right.
\end{equation}
()=()()
\begin{equation}
\left\{\begin{array}{lcl}
x_1' = -2 3   x_1'
x_2'= 5 -1    x_2'
\end{array}\right.
\end{equation}

EC. $\lambda^2-traza(A)\lambda + det(A=0)$
$\lambda+3\lambda-13=0$
$\sigma(A)={\frac{-3+\sqrt{61}}{2},\frac{-3-\sqrt{61}}{2}}={2.4...,5.4...}$
$\mu(A)>2.4 >0$
La ecuación es inestable.


%% MARTES 18

Recordemos la forma canónica de Jordan.

Sea $A \in \M_d(\R) \exists P, J \in \M_d(\C) tales que:$
\begin{itemize}
\item $P$ es regular (invertible)
\item $J$ es diagonal por bloques:
\begin{equation}
J=diag(J_1,...,J_k) J_j matriz diagonal \forall j=1,...,k
\end{equation}
\item $A=P\cdot J\cdot P^{-1}$
Para calcular $J$ se usan:
\item $\sigma(A)={\lambda_1,...\lambda_d}$
\item $m(\lambda_j)$
\item $dim E_{\lambda_j}$
\item $\nu(\lambda_j)=min{k\in\mathbb{Z}^+: BORRA}$
Los bloques de $J$ pueden ser:
\item de orden 1: $J_k=(\lambda_k) \lambda_k \in \sigma(A)$
\item de orden mayor que 1: $J_k=diag(\lambda_k,...\lambda_k) (1...1 encima de la diag) con \lambda_k \in \sigma(A)$
\item el orden $(J_k) \leq \nu(\lambda_k)$
\item la suma de los órdenes de todos los bloques asociados aun mismo valor propio es igual a la multiplicidad de ese valor propio
\end{itemize}

INSERTE EJEMPLO DE CAJA DE JORDAN

Con estos datos, podemos hacer el siguiente cálculo de la exponencial:

$$A=P\cdot J \cdot P^{-1} \implies A^n=P \cdot J^n \cdot P^{-1} \implies e^A=P \cdot e^J \cdot P^{-1}$$

Y de manera análoga:
\begin{itemize}
\item $e^{At}=P \cdot e^{Jt} \cdot P^{-1}$
\item $e^{Jt}=diag(e^{J_1t},...,e^{J_rt} )$
\end{itemize}

%% Inserte ejemplo

\begin{lema}
$$|||e^{Jt}|||_1=max{|||e^{J_1t}|||_1,|||e^J_rt|||_1}$$
\end{lema}

\begin{lema}
$\exists T \geq 0 $ tal que :
$$|||e^{Jt}|||_1=(1+...+\frac{t^n}{n!})\cdot e^{\mu t} \qquad \forall t \geq T$$
donde:
$$\mu=max{Re(\lambda_j):\lambda_j\in \sigma(A)}$$
$$n=max{\nu(\lambda_j)-1 : \lambda_j\in \sigma(A) y Re(\lambda_j)=\mu}$$
\end{lema}

Usando el lema anterior:
%%\begin{equation}
%%|||e^{Jt}|||_1=max
%%\left\{\(\begin{array}{lcl}
%%1+...+\frac{t^{\nu(\lambda_j)-1}}{(\nu (\lambda_j)-1)!}
%%\end{array}\right\)
%%\cdot e^{Re(\lambda_j)t} : \lambda_j \in \sigma(A) \right\}
%%\tag{cosarara}\label{cosarara}
%%\end{equation}
%%$\exists T \geq 0$ tal que:
%%\begin{equation}
%%(1+...+\frac{t^{\nu(\lambda_j)-1}}{\nu(\lambda_j - 1})\cdot e^{Re(\lambda_j)t} \leq (1+...1+\frac{t^n}{n!}e^{\mu t})
%%\end{equation}

Finalmente, podemos demostrar el teorema anterior:

\begin{proof}
$\exists P, J \in \M_d(\C)$ tales que $P$ es inestable, $J$ es diagonal por bloques y $A=P\cdot J \cdot P^{-1}$. Por tanto $$e^{At}=P\cdot e^{Jt} \cdot P^{-1}$$.
Defino en $\M_d(\C)$ la norma matricial:
$$|||B|||=|||P\cdot J \cdot P^{-1}|||_1$$.
Así, tenemos que:
\begin{enumerate}
\item Si $\mu < 0 \implies \lim_{t\to\infty}|||e^{At}|||=0\implies x'=Ax$ es A.E.
\item Si $\mu = 0 y dimE_j=m(\lambda)' \forall \lambda \in \sigma_0(A)\implies n=0 \implies |||e^{At}|||=1\implies|||e^{At}||| es estable pero no A.E.$
\item Si $\mu = 0 y \exists \lambda in \sigma_0(A) \nu(A)>1 \implies n\geq 1\implies |||e^{At}|||\geq 1+t$ no es acotada en $[0,+\infty)$
\item Si $\mu > 0 \qquad |||e^{At}|||\geq e^{\mu t}\to \infty$ no es acotada en $[0,+\infty)$
\end{enumerate}
\end{proof}
